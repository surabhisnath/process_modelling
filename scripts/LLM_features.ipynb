{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from brokenaxes import brokenaxes\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "# keys = json.load(open(\"keys.json\"))\n",
    "# os.environ[\"OPENAI_API_KEY\"]=keys[\"OPENAI_API_KEY\"]\n",
    "# openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# import together\n",
    "# os.environ[\"TOGETHER_API_KEY\"]=keys[\"TOGETHER_API_KEY\"]\n",
    "# together.api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "from scipy.stats import chisquare, sem\n",
    "plt.rcParams.update({\n",
    "    \"axes.facecolor\": \"white\",                      # background stays white\n",
    "    \"axes.edgecolor\": \"black\",                      # keep axis edges\n",
    "    \"patch.facecolor\": \"lightcoral\",\n",
    "    \"text.usetex\": False,                           # render text with LaTeX\n",
    "    \"font.family\": \"sans-serif\",                    # use serif fonts\n",
    "    \"axes.spines.top\": False,                       # remove top border\n",
    "    \"axes.spines.right\": False,                     # remove right border\n",
    "    \"axes.labelsize\": 16,                           # bigger axis labels\n",
    "    \"xtick.labelsize\": 14,                          # bigger x-tick labels\n",
    "    \"ytick.labelsize\": 14,                          # bigger y-tick labels\n",
    "    \"axes.titlesize\": 18,                           # bigger title\n",
    "    \"figure.dpi\": 100,                              # higher resolution\n",
    "})\n",
    "from mpl_toolkits.mplot3d import Axes3D  # activates 3D projection\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.8510708093162753, pvalue=6.407844692624839e-79)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "res = stats.spearmanr([\n",
    "  1.1580507, 0.066552, 0.15002869, 0.12491651, 0.50153756, 0.4646486,\n",
    "  0.03276786, 0.6010311, 0.79835117, 0.17289853, -0.15135707, 0.61203575,\n",
    "  0.7781848, 0.02536051, -0.07856986, 0.0549242, 0.02384605, -0.02910639,\n",
    "  0.03493351, 0.19123204, -0.05259304, 0.08858683, 0.11527813, 0.14396,\n",
    "  0.07731297, -0.66851664, -0.07858647, 0.04621653, 0.22485556, 0.15002869,\n",
    "  -0.11783456, 0.1686158, -0.02257443, 0.22341302, -0.02752743, 0.2625989,\n",
    "  1.0301206, 0.00537847, -0.13365363, 0.185491, 0.17862962, 0.12753233,\n",
    "  -0.16732693, -0.00957566, -0.14719708, 0.1356208, 0.11857465, 0.04137856,\n",
    "  0.23021367, 0.37945804, 0.63501996, -0.02282515, 0.26294622, 0.2020318,\n",
    "  0.14207482, -0.03154175, 0.0965078, 0.04651363, -0.01554856, 0.07699603,\n",
    "  -0.25729716, 0.22635578, 0.20745762, -0.15078925, 0.27794722, 0.22197317,\n",
    "  0.0220029, 0.21577607, -0.01638547, -0.06082735, 0.11514991, -0.02682037,\n",
    "  0.24092402, 0.0509807, 0.29014394, 0.3170514, 0.1474342, 0.25680888,\n",
    "  -0.09915292, 0.10866363, 0.38521418, 0.6746649, -0.04486789, 0.09256448,\n",
    "  0.09188308, 0.04121339, -0.00351273, -0.00684924, -0.09741485, 0.00654446,\n",
    "  0.15581496, -0.03598781, 0.16000925, 0.0302618, 0.07773, 0.13252752,\n",
    "  0.18527493, 0.16073628, 0.3530335, -0.02182009, 0.08069973, 0.13580552,\n",
    "  0.08299359, -0.06750766, -0.07201986, 0.5718461, 0.09996863, -0.3960657,\n",
    "  0.05393778, 0.34754175, 0.06921431, 0.07340286, 0.02609423, -0.15512504,\n",
    "  -0.01863477, 0.0195228, 0.19711065, 0.05177296, 0.06562126, 0.0283671,\n",
    "  0.07755759, 0.10482535, 0.08231503, 0.04983292, 0.02383711, 0.19636773,\n",
    "  0.26506984, 0.11020374, 0.00301669, -0.03850859, 0.13092853, -0.04770505,\n",
    "  0.07051668, -0.05927828, 0.1477192, -0.03026943, 0.17311388, 0.17623009,\n",
    "  -0.03218031, 0.33844858, -0.07766677, -0.02166699, 0.28007072, 0.37346584,\n",
    "  -0.00587232, 0.67575157, 0.46452212, -0.2557572, -0.54157025, 0.24099968,\n",
    "  0.30411935, 0.3873901, 0.2799937, 0.219869, 0.15630972, 0.26409063,\n",
    "  -0.37027508, -0.23646036, -0.25293064, 0.07645313, -0.01491474, -0.21320367,\n",
    "  0.37175664, -0.6685171, -0.6774641, -0.4304775, 0.14318395, -0.07766677,\n",
    "  0.44245473, 0.02848186, 0.34545654, -0.04045193, 0.43116635, -0.5424605,\n",
    "  0.51363033, 0.42576542, 0.02953316, -0.14596188, 0.691795, 0.31653142,\n",
    "  0.7904402, 0.3214874, -0.6007354, 0.5383026, -0.12882365, 0.15980609,\n",
    "  0.14227541, 0.02892765, -0.10804482, 0.18001588, -0.03116298, 0.30848604,\n",
    "  -0.15298577, -0.22948225, -0.00501704, 0.4223263, 0.21181339, -0.26086813,\n",
    "  -0.05800282, 0.11817957, 0.94055045, 0.25821707, 0.29894063, 0.3418922,\n",
    "  0.05526873, -0.29937133, -0.2576614, 0.30290988, -0.34574243, 0.21302362,\n",
    "  -0.39774117, -0.70628226, -0.21065642, 0.22367957, 0.27424657, -0.09583478,\n",
    "  0.21845773, 0.35433283, -0.20297682, 0.44000757, -0.08162612, -0.18823823,\n",
    "  0.19776697, 0.11883362, -0.04506757, 0.29694948, 0.17870554, -0.35416117,\n",
    "  -0.31016517, 0.05436027, 1.0310941, 0.16865107, -0.903157, 0.4812503,\n",
    "  -0.19584571, 0.17977235, -0.09362502, 0.03269472, -0.07100697, -0.21684028,\n",
    "  -0.01430975, -0.07024553, 0.04779949, 0.71499205, -0.16509095, 0.39607492,\n",
    "  -0.07400755, 0.88061047, 0.30820328, 0.00145134, 0.08355403, -0.3034132,\n",
    "  -0.3066935, 0.6282013, -0.09907807, -0.01995344, -0.22855847, 0.2761869,\n",
    "  -0.17401811, 0.26420203, -0.7447636, 0.55518275, 0.11356897, 0.15456748,\n",
    "  0.35075888, -0.1795154, -0.04036535, 0.02133798, 0.2914487, 0.12546548,\n",
    "  -0.11387607, -0.01981103, -0.31187755, -0.13924871, -0.14790551, 0.6950596,\n",
    "  -0.318596], [ 1.2177e+00,  5.2574e-02,  2.0691e-01,  7.7909e-02,  5.8386e-01,\n",
    "         7.5381e-01,  1.2244e-01,  6.5246e-01,  8.7809e-01,  5.9232e-03,\n",
    "        -1.8068e-02,  6.5317e-01,  8.7080e-01,  2.0773e-02, -1.1789e-01,\n",
    "         1.1105e-01,  1.0918e-02, -6.9095e-02,  4.7445e-02,  1.8493e-01,\n",
    "         3.0293e-02, -7.8805e-03,  7.1707e-02,  1.6109e-01,  7.3488e-01,\n",
    "        -2.5816e+00, -1.1290e-01,  7.9168e-02,  2.0109e-01,  2.0691e-01,\n",
    "        -1.0174e-01, -3.9501e-02, -6.3240e-02,  2.0763e-01,  1.3497e-02,\n",
    "         2.5860e-01,  1.0808e+00,  1.7532e-01, -2.9210e-01,  2.4286e-01,\n",
    "         1.6367e-01,  1.4450e-01, -1.5695e-01,  5.7041e-02, -1.4699e-01,\n",
    "         1.8240e-01,  1.0194e-01,  3.8402e-02,  2.0835e-01,  6.5108e-01,\n",
    "         9.8582e-01,  1.8808e-02,  4.1869e-01,  2.7516e-01,  1.3322e-01,\n",
    "        -3.9467e-02,  9.4672e-02,  9.0696e-02, -3.1064e-02,  1.1669e-01,\n",
    "        -2.1390e-01,  2.1139e-01,  2.2220e-01, -1.1258e-01,  2.6631e-01,\n",
    "         2.1709e-01,  4.6156e-02,  2.3443e-01,  1.0973e-02, -5.0720e-02,\n",
    "         1.4442e-01, -8.4203e-03,  2.4668e-01,  8.1504e-02,  2.7228e-01,\n",
    "         3.3234e-01,  1.3382e-01,  2.2149e-01, -1.4794e-01,  1.6788e-01,\n",
    "         4.4645e-01,  6.6848e-01, -9.7567e-03,  1.1359e-01,  1.1761e-01,\n",
    "         6.0601e-02,  1.8856e-02,  9.4752e-03, -6.3905e-02,  9.4476e-02,\n",
    "         1.7318e-01,  9.2187e-03,  2.7042e-01,  2.8692e-02,  1.0154e-01,\n",
    "         1.2040e-01,  1.9320e-01,  1.4716e-01,  4.1904e-01,  6.2614e-02,\n",
    "         9.9927e-02,  1.0769e-01,  6.9961e-02, -2.7831e-01, -1.5791e-01,\n",
    "         9.2957e-01,  3.7885e-02,  2.3824e-01,  2.8561e-03,  3.6763e-01,\n",
    "         6.0534e-02,  7.3076e-02,  2.9031e-02, -2.0133e-01, -7.9010e-01,\n",
    "         6.7771e-02,  2.6188e-01,  6.1086e-02,  6.3032e-02,  3.0310e-02,\n",
    "         9.6690e-02,  9.2967e-02,  6.8943e-02,  9.0438e-03, -1.8403e-02,\n",
    "         1.8849e-01,  2.0906e-01,  7.3553e-02,  1.8591e-02,  1.7406e-02,\n",
    "         1.6709e-01, -2.7976e-02,  6.4591e-02, -6.4864e-02,  1.3947e-01,\n",
    "        -1.7043e-02,  1.7007e-01,  2.0168e-01, -8.4742e-02,  2.9896e+00,\n",
    "         7.6335e-01, -1.7611e+00,  3.0184e+00,  3.6346e+00,  2.9983e+00,\n",
    "         9.6362e-01,  1.2018e+00, -1.3062e+00, -1.3769e+00,  6.3828e-01,\n",
    "         1.9903e-01,  5.5710e-01,  4.3116e-01,  3.6188e-01,  2.2746e-01,\n",
    "         1.6634e-01, -4.9325e-01, -2.4451e-01, -2.0677e-01,  7.5131e-01,\n",
    "        -1.7167e-01, -3.7987e-01,  2.2863e+00, -2.5829e+00, -6.9011e-01,\n",
    "        -4.7591e-01,  2.8120e-01,  7.6335e-01,  6.4544e-01,  1.1360e-01,\n",
    "         5.8384e-01, -2.3332e-01,  5.8058e-01, -2.3776e-01,  6.0398e-01,\n",
    "         4.8580e-01,  8.9983e-01, -3.2498e-01,  1.2267e+00,  1.3414e+00,\n",
    "         1.6912e+00,  6.1602e-01, -6.8303e-01,  4.9978e-01,  5.2183e-02,\n",
    "         4.7443e-01,  2.1893e-01,  4.1458e-02,  6.4964e-01,  2.4672e-01,\n",
    "        -3.8888e-01,  2.8057e-01, -1.9691e-01, -1.3606e-01, -3.9673e-02,\n",
    "         3.3402e-01,  6.3802e-01, -3.0997e-01, -3.5222e-01,  4.3983e-02,\n",
    "         9.3503e-01,  7.3512e-02,  5.8664e-01,  3.9915e-01,  2.1116e-02,\n",
    "        -3.2053e-01, -1.9446e-01,  4.3154e-01, -4.5954e-01,  5.1017e-01,\n",
    "        -5.1193e-01, -8.5005e-01, -2.1842e-01,  1.7095e-01,  2.0430e-01,\n",
    "         3.6688e-02, -2.2384e-01,  1.0872e-01, -2.5344e-01,  6.3886e-01,\n",
    "        -7.7928e-02,  4.8087e-02,  2.4898e-01,  3.0565e-02, -1.2778e-01,\n",
    "         3.2667e-03,  1.6276e-01, -5.0161e-01, -1.8908e-01,  6.3838e-02,\n",
    "         1.5808e+00,  2.8004e-01, -9.4456e-01,  4.5628e-01, -9.1751e-02,\n",
    "         6.2631e-02, -8.1940e-02, -1.8854e-01, -2.0044e-01, -3.3221e-01,\n",
    "        -1.6646e-01, -1.8494e+00,  9.5229e-01,  1.5170e+00,  1.5935e-01,\n",
    "         1.7627e+00, -8.7813e-01,  8.0516e-01,  3.8076e-01,  4.6237e-02,\n",
    "         2.3521e-01, -6.7211e-01, -1.3051e+00,  7.5715e-01,  3.1887e-02,\n",
    "        -1.2255e-01, -3.7634e-01,  2.1352e-01, -2.4031e-01,  7.2117e-02,\n",
    "        -5.6895e-01,  6.1831e-01,  1.3669e-02,  1.4805e-02,  5.0719e-01,\n",
    "        -2.2781e-01,  4.1483e-02, -2.6503e-01,  3.4719e-01,  6.4745e-01,\n",
    "        -6.9564e-01,  1.2456e-01, -1.8082e-01, -3.3121e-01, -1.2321e-01,\n",
    "         8.5364e-01, -1.5340e-01]\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"../csvs/hills.csv\")\n",
    "data3 = pd.read_csv(\"../csvs/noconstraints.csv\")\n",
    "data4 = pd.read_csv(\"../csvs/similar.csv\")\n",
    "data5 = pd.read_csv(\"../csvs/divergent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0748178775349477"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[\"fpatchitem\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop(columns=[\"fpatchnum\", \"fpatchitem\", \"fitemsfromend\", \"flastitem\",  \"meanirt\", \"catitem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv(\"../csvs/hills_removecols.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"previous_response\"] = data2.groupby(\"pid\")[\"response\"].shift(1)\n",
    "data2[\"order\"] = data2.groupby(\"pid\").cumcount() + 1\n",
    "data2 = data2[~data2[\"response\"].isin([\"mammal\", \"bacterium\", \"unicorn\", \"woollymammoth\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.616122759313407\n",
      "0.08617167626181474 0.5386771751507037\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "LLM = [-0.9255,  0.5204, -1.6303,  0.2473,  1.8285,  0.4606,  1.2766,\n",
    "         0.9898,  0.4488, -0.4499,  0.9888,  0.1349,  0.2244,  0.2634,  0.0105,\n",
    "        -0.0853,  0.0293,  0.0956,  0.1151, -1.7363,  1.4970, -0.1477,  0.2745,\n",
    "         1.0074,  1.0074, -0.2799,  0.0395,  0.7450,  0.5204, -0.2911, -1.5390,\n",
    "        -0.4192,  0.3429,  0.0236,  0.1685,  1.2109,  0.0479,  0.6305, -0.3033,\n",
    "         0.4523, -0.1821,  2.4305, -0.1727, -0.0441,  0.4466, -0.6810, -0.0246,\n",
    "         0.8417,  1.5777,  2.0010, -0.6070,  0.1164,  0.3162, -0.1183, -0.1200,\n",
    "        -0.3759, -0.0985,  2.3146,  0.3572,  0.3372, -1.1479, -0.4176,  0.6292,\n",
    "         0.2064,  0.7692,  0.0851,  1.0894,  0.5365,  0.3604,  0.4378,  0.4105,\n",
    "         0.3604,  0.2523,  1.4086,  0.5277, -0.0730, -0.1788,  0.7516,  0.6652,\n",
    "         0.9830,  1.8034,  0.0078, -0.2328,  0.2395, -0.0550,  0.5279,  0.2868,\n",
    "         1.8565, -0.4671,  0.5954, -0.2804,  0.0979, -0.0621,  0.0189,  0.0356,\n",
    "         0.3746,  0.7372,  0.0051,  0.1424, -0.0785,  0.3206, -0.0058,  0.9527,\n",
    "        -2.1665, -0.4393,  0.5472, -1.8019,  0.7446,  0.2837,  0.4872,  0.3876,\n",
    "         0.0418, -0.4765, -0.6500,  0.4242,  0.1756,  0.1551, -0.2118,  0.0531,\n",
    "        -0.0662,  0.2928,  0.2596,  0.2032,  0.5794,  0.0813, -0.6023, -0.0997,\n",
    "        -0.0694, -0.0637,  0.0864,  0.8803, -1.9425,  0.1074,  0.2726,  0.5413,\n",
    "         0.0674,  0.7408, -0.4525]\n",
    "human = [-0.0138,  0.2288,  0.0713,  0.2251,  0.4985, -0.0537,  0.3645,\n",
    "         0.8561,  0.0238, -0.2880,  0.4157,  0.8518,  0.1569, -0.0027,  0.0396,\n",
    "         0.0630, -0.1624,  0.0649,  0.1641,  0.0206,  0.0017,  0.0838, -0.0555,\n",
    "         0.5559, -0.7895, -0.0076,  0.0017,  0.2628,  0.2288, -0.0541,  0.1251,\n",
    "        -0.1686,  0.2309,  0.0779,  0.5883,  1.0658, -0.1096, -0.3396,  0.1288,\n",
    "         0.0763,  0.1391, -0.2139, -0.0767,  0.1378, -0.2801,  0.1341,  0.0270,\n",
    "         0.0579,  0.5069,  0.8559,  0.0077,  0.4070,  0.4237,  0.2192, -0.1268,\n",
    "         0.1215,  0.0483, -0.1079,  0.0792, -0.6001,  0.0649,  0.0857, -0.1114,\n",
    "         0.3703,  0.1707,  0.0384,  0.2559, -0.0324, -0.0618,  0.1994, -0.0444,\n",
    "         0.3105,  0.0272,  0.2003,  0.3637,  0.0885,  0.2255, -0.2400,  0.3229,\n",
    "         0.3821,  0.6627, -0.0209,  0.1115,  0.0705,  0.0733,  0.0584, -0.2636,\n",
    "         0.2422, -0.0185,  0.3026, -0.0936, -0.0978, -0.0480,  0.1836,  0.2017,\n",
    "         0.2223,  0.3732,  0.3546,  0.0788,  0.2831,  0.1905,  0.0489, -0.2282,\n",
    "         0.0266,  0.4614,  0.0873, -1.4220,  0.2816,  0.2110,  0.0814,  0.1245,\n",
    "         0.0292, -0.4524, -0.5090,  0.2836, -0.0463,  0.0966,  0.1732,  0.0613,\n",
    "        -0.0064, -0.1436,  0.1286,  0.6397,  0.1939,  0.1482,  0.1542,  0.0967,\n",
    "         0.0611, -0.1571,  0.5137, -0.3009,  0.0859, -0.0212,  0.0628,  0.0014,\n",
    "         0.1840,  0.2051,  0.0435]\n",
    "arr = np.array(LLM)  # uses natural log (nats)\n",
    "p = np.exp(arr - arr.max())  # stable softmax\n",
    "p = p / p.sum()\n",
    "H = entropy(p)  # natural log\n",
    "print(H)\n",
    "\n",
    "print(np.var(human), np.var(LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.,  2.,  2., 12., 45., 43., 18.,  5.,  4.,  3.]),\n",
       " array([-2.1665, -1.7068, -1.2471, -0.7874, -0.3277,  0.132 ,  0.5917,\n",
       "         1.0514,  1.5111,  1.9708,  2.4305]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(human)\n",
    "plt.hist(LLM, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.15024111402599574, pvalue=0.012459193007020558)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = stats.spearmanr([-2.9148e-01,  2.4424e-01, -8.1303e+00,  6.9032e-01,\n",
    "         3.6365e+00, -1.3209e-01,  7.3872e-01,  1.1487e+00,  2.1078e-01,\n",
    "         5.6577e-02,  1.1454e+00,  2.1946e-01,  1.5408e-01,  2.1686e-01,\n",
    "        -1.8729e-02, -5.5054e-02,  8.5510e-03,  9.8847e-02,  1.7395e-01,\n",
    "        -2.2743e+00,  1.8172e+00,  1.5081e-02,  5.0679e-01,  9.9867e-01,\n",
    "         9.9867e-01, -1.7719e-03,  1.2513e-02,  2.5924e-01,  2.4424e-01,\n",
    "        -4.7581e-01, -1.6835e-01,  2.5619e-02,  2.8401e-01, -1.7529e-01,\n",
    "         1.1855e-01,  1.9490e+00, -9.7991e-01,  6.1161e-01,  6.6150e-01,\n",
    "         6.7946e-01, -4.2616e-01,  1.4136e+00, -2.7602e-01,  2.5224e-01,\n",
    "         3.7179e-01, -1.6605e-01,  3.7538e-01,  9.0639e-01,  3.0909e-01,\n",
    "         2.0503e+00, -9.5320e-02,  4.3919e-01,  2.1470e-01, -1.0550e-01,\n",
    "        -3.7236e-02, -2.8967e-01, -1.9098e-01,  2.1405e+00,  4.0118e-01,\n",
    "         1.5765e+00, -2.7593e-01, -1.2068e-02,  6.3690e-01, -1.4508e-01,\n",
    "         6.9140e-01,  1.1684e-01,  1.0625e+00,  6.0810e-01,  2.3132e-01,\n",
    "         2.0299e-01,  4.4889e-01,  2.7036e-01,  2.6660e-01,  8.7229e-01,\n",
    "         5.7886e-01,  1.2424e-01, -1.2302e-01,  1.4509e+00, -6.7067e-02,\n",
    "         1.0750e+00,  1.2604e+00,  3.7675e-01, -2.0247e-01,  2.1191e-01,\n",
    "        -2.7192e-01,  1.2195e-02,  4.3884e-01,  1.2436e+00, -1.4545e-01,\n",
    "         5.9411e-01, -3.0505e-01,  7.9493e-01, -8.1727e-03,  8.0294e-02,\n",
    "         1.8077e-01,  8.5342e-02,  1.1285e+00,  7.3788e-01, -1.9714e-01,\n",
    "        -1.6419e-01,  2.5797e-01,  8.1629e-01,  1.8285e+00, -1.1435e+00,\n",
    "         1.1652e+00,  7.6730e-01, -1.2892e+00,  1.3580e-02, -3.5733e-04,\n",
    "         4.0742e-01,  4.4426e-01,  1.1893e-01, -7.4854e-01, -3.2908e+00,\n",
    "        -3.1833e-01, -1.8794e-01,  1.8743e-01, -6.2901e-01, -2.2365e-01,\n",
    "         1.4992e-01,  3.4905e-01,  2.0200e-01, -1.9455e+00,  5.5240e-01,\n",
    "         6.4667e-02, -1.1275e-01, -1.0322e-01, -1.5200e-02, -7.2125e-01,\n",
    "         2.4674e-01,  6.0785e-01, -1.6238e+00,  2.4725e-01,  3.0446e-01,\n",
    "        -1.8557e-01, -3.8663e-01,  6.3399e-01, -8.8990e-01,  2.1706e+00,\n",
    "         9.3075e-02, -1.0056e+00, -7.1388e+00, -2.0379e+00, -3.6679e+00,\n",
    "        -2.5143e+00,  2.4238e+00, -2.6246e+00, -2.8871e+00, -3.4656e+00,\n",
    "        -1.6324e+00,  6.7901e-01, -6.1342e-01,  2.1201e-01, -9.1141e-02,\n",
    "         2.0529e+00, -1.2509e-01, -9.7123e-01, -1.7653e+00,  7.5115e+00,\n",
    "         6.1623e-01, -5.3890e-01,  1.0000e+00,  1.0018e+00, -9.4627e-01,\n",
    "        -7.5666e-03,  8.4654e-02,  9.3075e-02, -1.1409e+00, -3.3363e+00,\n",
    "         4.9693e-01, -7.7543e-01,  1.2678e+00, -4.1524e+00, -1.7546e+00,\n",
    "        -2.8846e+00,  3.1121e+00,  1.3347e-01,  2.2567e+00, -1.8985e+00,\n",
    "         7.9853e+00, -3.9650e-01,  1.2713e+00, -8.7645e-02,  1.2980e+00,\n",
    "        -3.6895e+00,  1.8473e+00, -2.9226e+00, -5.3246e+00, -9.7876e-01,\n",
    "        -2.2369e+00, -7.8690e-01,  1.0412e+00, -5.2993e-01, -1.3671e+00,\n",
    "         1.0942e+00, -1.5030e+01,  4.0659e-01,  8.9422e-01, -1.3451e+00,\n",
    "         8.7263e-01,  1.9400e+00,  5.9422e-01,  7.1105e-01,  1.0719e+00,\n",
    "         1.3063e+00, -2.0613e-01,  9.4034e-01,  6.0888e-01, -1.2416e+00,\n",
    "         1.3315e+00, -5.5893e-01, -9.6736e-01,  2.9846e-01,  9.7265e-01,\n",
    "        -8.1761e-01,  8.4811e-01, -1.7651e+00, -5.4309e+00, -1.0776e+00,\n",
    "         1.4252e+00, -1.5209e+00, -3.0753e-01, -1.7989e+00, -2.8663e+00,\n",
    "        -6.6442e-02,  4.7052e+00, -1.0575e+00, -2.0733e-01,  9.8709e-01,\n",
    "         4.2006e+00, -7.5948e-01, -1.6393e+00, -3.6260e-01, -1.5854e+00,\n",
    "         1.3462e+00, -5.0698e-01, -4.7515e-02,  4.1168e-02, -1.6100e+00,\n",
    "        -1.3112e+00,  7.1733e+00,  1.6045e+00, -3.2416e+00, -2.6234e+00,\n",
    "        -1.3672e-01, -3.3525e+00,  2.3354e+00, -6.0989e-01,  1.8492e+00,\n",
    "         1.6289e+00, -5.5372e-01, -2.6225e+00,  1.6587e+00, -2.9327e+00,\n",
    "         1.4169e+00, -1.8154e+00,  2.9959e+00,  1.7145e+00,  2.0041e+00,\n",
    "        -3.4359e+00, -9.0674e-01,  9.0206e-02, -1.7712e+00, -9.4995e-01,\n",
    "         3.8847e-01, -1.3289e-01,  1.3356e+00, -1.1158e+00, -1.8713e+00,\n",
    "         1.0795e+01, -4.7507e-01, -1.2621e+00, -1.2111e+00,  5.8322e-01,\n",
    "         4.5307e-02, -4.2930e+00], [5.2574e-02,  2.0691e-01,  7.7909e-02,  5.8386e-01,\n",
    "         7.5381e-01,  1.2244e-01,  6.5246e-01,  8.7809e-01,  5.9232e-03,\n",
    "        -1.8068e-02,  6.5317e-01,  8.7080e-01,  2.0773e-02, -1.1789e-01,\n",
    "         1.1105e-01,  1.0918e-02, -6.9095e-02,  4.7445e-02,  1.8493e-01,\n",
    "         3.0293e-02, -7.8805e-03,  7.1707e-02,  1.6109e-01,  7.3488e-01,\n",
    "        -2.5816e+00, -1.1290e-01,  7.9168e-02,  2.0109e-01,  2.0691e-01,\n",
    "        -1.0174e-01, -3.9501e-02, -6.3240e-02,  2.0763e-01,  1.3497e-02,\n",
    "         2.5860e-01,  1.0808e+00,  1.7532e-01, -2.9210e-01,  2.4286e-01,\n",
    "         1.6367e-01,  1.4450e-01, -1.5695e-01,  5.7041e-02, -1.4699e-01,\n",
    "         1.8240e-01,  1.0194e-01,  3.8402e-02,  2.0835e-01,  6.5108e-01,\n",
    "         9.8582e-01,  1.8808e-02,  4.1869e-01,  2.7516e-01,  1.3322e-01,\n",
    "        -3.9467e-02,  9.4672e-02,  9.0696e-02, -3.1064e-02,  1.1669e-01,\n",
    "        -2.1390e-01,  2.1139e-01,  2.2220e-01, -1.1258e-01,  2.6631e-01,\n",
    "         2.1709e-01,  4.6156e-02,  2.3443e-01,  1.0973e-02, -5.0720e-02,\n",
    "         1.4442e-01, -8.4203e-03,  2.4668e-01,  8.1504e-02,  2.7228e-01,\n",
    "         3.3234e-01,  1.3382e-01,  2.2149e-01, -1.4794e-01,  1.6788e-01,\n",
    "         4.4645e-01,  6.6848e-01, -9.7567e-03,  1.1359e-01,  1.1761e-01,\n",
    "         6.0601e-02,  1.8856e-02,  9.4752e-03, -6.3905e-02,  9.4476e-02,\n",
    "         1.7318e-01,  9.2187e-03,  2.7042e-01,  2.8692e-02,  1.0154e-01,\n",
    "         1.2040e-01,  1.9320e-01,  1.4716e-01,  4.1904e-01,  6.2614e-02,\n",
    "         9.9927e-02,  1.0769e-01,  6.9961e-02, -2.7831e-01, -1.5791e-01,\n",
    "         9.2957e-01,  3.7885e-02,  2.3824e-01,  2.8561e-03,  3.6763e-01,\n",
    "         6.0534e-02,  7.3076e-02,  2.9031e-02, -2.0133e-01, -7.9010e-01,\n",
    "         6.7771e-02,  2.6188e-01,  6.1086e-02,  6.3032e-02,  3.0310e-02,\n",
    "         9.6690e-02,  9.2967e-02,  6.8943e-02,  9.0438e-03, -1.8403e-02,\n",
    "         1.8849e-01,  2.0906e-01,  7.3553e-02,  1.8591e-02,  1.7406e-02,\n",
    "         1.6709e-01, -2.7976e-02,  6.4591e-02, -6.4864e-02,  1.3947e-01,\n",
    "        -1.7043e-02,  1.7007e-01,  2.0168e-01, -8.4742e-02,  2.9896e+00,\n",
    "         7.6335e-01, -1.7611e+00,  3.0184e+00,  3.6346e+00,  2.9983e+00,\n",
    "         9.6362e-01,  1.2018e+00, -1.3062e+00, -1.3769e+00,  6.3828e-01,\n",
    "         1.9903e-01,  5.5710e-01,  4.3116e-01,  3.6188e-01,  2.2746e-01,\n",
    "         1.6634e-01, -4.9325e-01, -2.4451e-01, -2.0677e-01,  7.5131e-01,\n",
    "        -1.7167e-01, -3.7987e-01,  2.2863e+00, -2.5829e+00, -6.9011e-01,\n",
    "        -4.7591e-01,  2.8120e-01,  7.6335e-01,  6.4544e-01,  1.1360e-01,\n",
    "         5.8384e-01, -2.3332e-01,  5.8058e-01, -2.3776e-01,  6.0398e-01,\n",
    "         4.8580e-01,  8.9983e-01, -3.2498e-01,  1.2267e+00,  1.3414e+00,\n",
    "         1.6912e+00,  6.1602e-01, -6.8303e-01,  4.9978e-01,  5.2183e-02,\n",
    "         4.7443e-01,  2.1893e-01,  4.1458e-02,  6.4964e-01,  2.4672e-01,\n",
    "        -3.8888e-01,  2.8057e-01, -1.9691e-01, -1.3606e-01, -3.9673e-02,\n",
    "         3.3402e-01,  6.3802e-01, -3.0997e-01, -3.5222e-01,  4.3983e-02,\n",
    "         9.3503e-01,  7.3512e-02,  5.8664e-01,  3.9915e-01,  2.1116e-02,\n",
    "        -3.2053e-01, -1.9446e-01,  4.3154e-01, -4.5954e-01,  5.1017e-01,\n",
    "        -5.1193e-01, -8.5005e-01, -2.1842e-01,  1.7095e-01,  2.0430e-01,\n",
    "         3.6688e-02, -2.2384e-01,  1.0872e-01, -2.5344e-01,  6.3886e-01,\n",
    "        -7.7928e-02,  4.8087e-02,  2.4898e-01,  3.0565e-02, -1.2778e-01,\n",
    "         3.2667e-03,  1.6276e-01, -5.0161e-01, -1.8908e-01,  6.3838e-02,\n",
    "         1.5808e+00,  2.8004e-01, -9.4456e-01,  4.5628e-01, -9.1751e-02,\n",
    "         6.2631e-02, -8.1940e-02, -1.8854e-01, -2.0044e-01, -3.3221e-01,\n",
    "        -1.6646e-01, -1.8494e+00,  9.5229e-01,  1.5170e+00,  1.5935e-01,\n",
    "         1.7627e+00, -8.7813e-01,  8.0516e-01,  3.8076e-01,  4.6237e-02,\n",
    "         2.3521e-01, -6.7211e-01, -1.3051e+00,  7.5715e-01,  3.1887e-02,\n",
    "        -1.2255e-01, -3.7634e-01,  2.1352e-01, -2.4031e-01,  7.2117e-02,\n",
    "        -5.6895e-01,  6.1831e-01,  1.3669e-02,  1.4805e-02,  5.0719e-01,\n",
    "        -2.2781e-01,  4.1483e-02, -2.6503e-01,  3.4719e-01,  6.4745e-01,\n",
    "        -6.9564e-01,  1.2456e-01, -1.8082e-01, -3.3121e-01, -1.2321e-01,\n",
    "         8.5364e-01, -1.5340e-01])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_983149/2198514859.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n"
     ]
    }
   ],
   "source": [
    "def get_featuredf():\n",
    "    featuredict = pk.load(open(f\"../files/features_gpt41.pk\", \"rb\"))\n",
    "    featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "    featuredf = featuredf.replace({True: 1, False: 0, 'True': 1, 'True.': 1, 'TRUE': 1, 'true': 1, 'False': 0, 'False.': 0, 'false': 0})\n",
    "    featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n",
    "    return featuredf, featuredf.columns.tolist()\n",
    "\n",
    "vf_featuredf, vf_featurecols = get_featuredf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_983149/1952150731.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_Gives birth</th>\n",
       "      <th>feature_Is monotreme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>platypus</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date mammal</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>echidna</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature_Gives birth feature_Is monotreme\n",
       "platypus                      0                    1\n",
       "date mammal                   1                    1\n",
       "echidna                       0                    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hills = pd.read_csv(\"../csvs/hills.csv\")\n",
    "hills = hills.drop(columns=[\"fpatchnum\", \"fpatchitem\", \"fitemsfromend\", \"flastitem\",  \"meanirt\", \"catitem\"])\n",
    "hills[\"previous_response\"] = hills.groupby(\"pid\")[\"response\"].shift(1)\n",
    "hills[\"order\"] = hills.groupby(\"pid\").cumcount() + 1\n",
    "hills = hills[~hills[\"response\"].isin([\"mammal\", \"bacterium\", \"unicorn\", \"woollymammoth\"])]\n",
    "\n",
    "def get_featuredf():\n",
    "    featuredict = pk.load(open(f\"../files/features_gpt41.pk\", \"rb\"))\n",
    "    featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "    featuredf = featuredf.replace({True: 1, False: 0, 'True': 1, 'True.': 1, 'TRUE': 1, 'true': 1, 'False': 0, 'False.': 0, 'false': 0})\n",
    "    featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n",
    "    return featuredf, featuredf.columns.tolist()\n",
    "vf_featuredf, vf_featurecols = get_featuredf()\n",
    "\n",
    "valid_animals = set(hills[\"response\"].unique())\n",
    "X_df = vf_featuredf.loc[vf_featuredf.index.intersection(valid_animals)].sort_index()\n",
    "X_df = vf_featuredf\n",
    "X_df[X_df[\"feature_Is monotreme\"] == 1][[\"feature_Gives birth\", \"feature_Is monotreme\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_Is mammal</th>\n",
       "      <th>feature_Is bird</th>\n",
       "      <th>feature_Is insect</th>\n",
       "      <th>feature_Is reptile</th>\n",
       "      <th>feature_Is amphibian</th>\n",
       "      <th>feature_Is fish</th>\n",
       "      <th>feature_Is rodent</th>\n",
       "      <th>feature_Is primate</th>\n",
       "      <th>feature_Is jungle animal</th>\n",
       "      <th>feature_Is non-jungle animal</th>\n",
       "      <th>feature_Is feline</th>\n",
       "      <th>feature_Is canine</th>\n",
       "      <th>feature_Is subspecies of an animal</th>\n",
       "      <th>feature_Is carnivore</th>\n",
       "      <th>feature_Is herbivore</th>\n",
       "      <th>feature_Is omnivore</th>\n",
       "      <th>feature_Is larger in size compared to other animals</th>\n",
       "      <th>feature_Is smaller in size compared to other animals</th>\n",
       "      <th>feature_Is average size compared to other animals</th>\n",
       "      <th>feature_Is warm-blooded</th>\n",
       "      <th>feature_Is cold-blooded</th>\n",
       "      <th>feature_Is a predator</th>\n",
       "      <th>feature_Is prey for larger animals</th>\n",
       "      <th>feature_Is a parasite</th>\n",
       "      <th>feature_Is a host for parasites</th>\n",
       "      <th>feature_Is nocturnal</th>\n",
       "      <th>feature_Is diurnal</th>\n",
       "      <th>feature_Has fur</th>\n",
       "      <th>feature_Has feathers</th>\n",
       "      <th>feature_Has scales</th>\n",
       "      <th>feature_Has exoskeleton</th>\n",
       "      <th>feature_Has beak</th>\n",
       "      <th>feature_Has claws</th>\n",
       "      <th>feature_Has whiskers</th>\n",
       "      <th>feature_Has horns</th>\n",
       "      <th>feature_Has antlers</th>\n",
       "      <th>feature_Has tusks</th>\n",
       "      <th>feature_Has wings</th>\n",
       "      <th>feature_Has tail</th>\n",
       "      <th>feature_Has less than four limbs</th>\n",
       "      <th>feature_Has exactly four limbs</th>\n",
       "      <th>feature_Has more than four limbs</th>\n",
       "      <th>feature_Has stripes</th>\n",
       "      <th>feature_Has spots</th>\n",
       "      <th>feature_Has mane</th>\n",
       "      <th>feature_Has crest</th>\n",
       "      <th>feature_Has gills</th>\n",
       "      <th>feature_Has flippers</th>\n",
       "      <th>feature_Has compound eyes</th>\n",
       "      <th>feature_Has segmented body</th>\n",
       "      <th>feature_Has a long neck</th>\n",
       "      <th>feature_Can fly</th>\n",
       "      <th>feature_Can swim</th>\n",
       "      <th>feature_Can climb</th>\n",
       "      <th>feature_Can dig</th>\n",
       "      <th>feature_Can jump</th>\n",
       "      <th>feature_Can camouflage</th>\n",
       "      <th>feature_Can hibernate</th>\n",
       "      <th>feature_Can be trained or tamed by humans</th>\n",
       "      <th>feature_Is found in zoos</th>\n",
       "      <th>feature_Lives in water</th>\n",
       "      <th>feature_Lives in trees</th>\n",
       "      <th>feature_Lives underground</th>\n",
       "      <th>feature_Lives on land</th>\n",
       "      <th>feature_Is native to Africa</th>\n",
       "      <th>feature_Is native to Asia</th>\n",
       "      <th>feature_Is native to North America</th>\n",
       "      <th>feature_Is native to South America</th>\n",
       "      <th>feature_Is native to Australia</th>\n",
       "      <th>feature_Is native to Europe</th>\n",
       "      <th>feature_Lives in Arctic/far North</th>\n",
       "      <th>feature_Is found in deserts</th>\n",
       "      <th>feature_Is found in forests</th>\n",
       "      <th>feature_Is found in oceans</th>\n",
       "      <th>feature_Is found in grasslands</th>\n",
       "      <th>feature_Is found in mountains</th>\n",
       "      <th>feature_Lives in burrows</th>\n",
       "      <th>feature_Lays eggs</th>\n",
       "      <th>feature_Gives birth</th>\n",
       "      <th>feature_Is venomous</th>\n",
       "      <th>feature_Is domesticated</th>\n",
       "      <th>feature_Lives in groups</th>\n",
       "      <th>feature_Is solitary</th>\n",
       "      <th>feature_Builds nests</th>\n",
       "      <th>feature_Is migratory</th>\n",
       "      <th>feature_Has social hierarchy</th>\n",
       "      <th>feature_Uses tools</th>\n",
       "      <th>feature_Shows intelligence</th>\n",
       "      <th>feature_Communicates vocally</th>\n",
       "      <th>feature_Can change color</th>\n",
       "      <th>feature_Is capable of mimicry</th>\n",
       "      <th>feature_Has echolocation</th>\n",
       "      <th>feature_Is known for speed</th>\n",
       "      <th>feature_Is known for strength</th>\n",
       "      <th>feature_Is kept as a pet</th>\n",
       "      <th>feature_Is used in farming</th>\n",
       "      <th>feature_Is hunted by humans</th>\n",
       "      <th>feature_Is used for food by humans</th>\n",
       "      <th>feature_Is used for transportation</th>\n",
       "      <th>feature_Is used in scientific research</th>\n",
       "      <th>feature_Has a long lifespan</th>\n",
       "      <th>feature_Has regenerative ability</th>\n",
       "      <th>feature_Is vertebrate</th>\n",
       "      <th>feature_Is invertebrate</th>\n",
       "      <th>feature_Is marsupial</th>\n",
       "      <th>feature_Is placental</th>\n",
       "      <th>feature_Is monotreme</th>\n",
       "      <th>feature_Is flightless</th>\n",
       "      <th>feature_Has webbed feet</th>\n",
       "      <th>feature_Is known for intelligence</th>\n",
       "      <th>feature_Is a scavenger</th>\n",
       "      <th>feature_Is territorial</th>\n",
       "      <th>feature_Is endangered</th>\n",
       "      <th>feature_Is bioluminescent</th>\n",
       "      <th>feature_Is capable of parental care</th>\n",
       "      <th>feature_Is a pollinator</th>\n",
       "      <th>feature_Can tolerate extreme temperatures</th>\n",
       "      <th>feature_Exhibits seasonal color changes</th>\n",
       "      <th>feature_Is active during dawn or dusk (crepuscular)</th>\n",
       "      <th>feature_Produces pheromones for communication</th>\n",
       "      <th>feature_Lives symbiotically with other species</th>\n",
       "      <th>feature_Is bi-parental (both parents care for offspring)</th>\n",
       "      <th>feature_Displays mating rituals</th>\n",
       "      <th>feature_Has specialized courtship behavior</th>\n",
       "      <th>feature_Exhibits territorial marking</th>\n",
       "      <th>feature_Is associated with mythology or folklore</th>\n",
       "      <th>feature_Exhibits altruistic behavior</th>\n",
       "      <th>feature_Is a keystone species in its ecosystem</th>\n",
       "      <th>feature_Can regenerate body parts</th>\n",
       "      <th>feature_Is raised in captivity or farms</th>\n",
       "      <th>feature_Has unique reproductive strategies (e.g., asexual reproduction)</th>\n",
       "      <th>feature_Hibernates during winter</th>\n",
       "      <th>feature_Has a role in biological pest control</th>\n",
       "      <th>feature_Has distinct seasonal breeding cycles</th>\n",
       "      <th>feature_Forms a symbiotic relationship with plants (e.g., pollination)</th>\n",
       "      <th>feature_Uses specific vocalizations to communicate</th>\n",
       "      <th>feature_Is a flagship species (conservation symbol)</th>\n",
       "      <th>feature_Displays warning coloration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>perch</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_Is mammal feature_Is bird feature_Is insect feature_Is reptile  \\\n",
       "perch                 0               1                 0                  0   \n",
       "\n",
       "      feature_Is amphibian feature_Is fish feature_Is rodent  \\\n",
       "perch                    0               1                 0   \n",
       "\n",
       "      feature_Is primate feature_Is jungle animal  \\\n",
       "perch                  0                        0   \n",
       "\n",
       "      feature_Is non-jungle animal feature_Is feline feature_Is canine  \\\n",
       "perch                            1                 0                 0   \n",
       "\n",
       "      feature_Is subspecies of an animal feature_Is carnivore  \\\n",
       "perch                                  0                    1   \n",
       "\n",
       "      feature_Is herbivore feature_Is omnivore  \\\n",
       "perch                    0                   0   \n",
       "\n",
       "      feature_Is larger in size compared to other animals  \\\n",
       "perch                                                  0    \n",
       "\n",
       "      feature_Is smaller in size compared to other animals  \\\n",
       "perch                                                  1     \n",
       "\n",
       "      feature_Is average size compared to other animals  \\\n",
       "perch                                                 1   \n",
       "\n",
       "      feature_Is warm-blooded feature_Is cold-blooded feature_Is a predator  \\\n",
       "perch                       0                       1                     1   \n",
       "\n",
       "      feature_Is prey for larger animals feature_Is a parasite  \\\n",
       "perch                                  1                     0   \n",
       "\n",
       "      feature_Is a host for parasites feature_Is nocturnal feature_Is diurnal  \\\n",
       "perch                               1                    0                  1   \n",
       "\n",
       "      feature_Has fur feature_Has feathers feature_Has scales  \\\n",
       "perch               0                    0                  1   \n",
       "\n",
       "      feature_Has exoskeleton feature_Has beak feature_Has claws  \\\n",
       "perch                       0                0                 0   \n",
       "\n",
       "      feature_Has whiskers feature_Has horns feature_Has antlers  \\\n",
       "perch                    0                 0                   0   \n",
       "\n",
       "      feature_Has tusks feature_Has wings feature_Has tail  \\\n",
       "perch                 0                 0                1   \n",
       "\n",
       "      feature_Has less than four limbs feature_Has exactly four limbs  \\\n",
       "perch                                1                              0   \n",
       "\n",
       "      feature_Has more than four limbs feature_Has stripes feature_Has spots  \\\n",
       "perch                                0                   1                 1   \n",
       "\n",
       "      feature_Has mane feature_Has crest feature_Has gills  \\\n",
       "perch                0                 0                 1   \n",
       "\n",
       "      feature_Has flippers feature_Has compound eyes  \\\n",
       "perch                    0                         0   \n",
       "\n",
       "      feature_Has segmented body feature_Has a long neck feature_Can fly  \\\n",
       "perch                          0                       0               0   \n",
       "\n",
       "      feature_Can swim feature_Can climb feature_Can dig feature_Can jump  \\\n",
       "perch                1                 0               0                0   \n",
       "\n",
       "      feature_Can camouflage feature_Can hibernate  \\\n",
       "perch                      1                     0   \n",
       "\n",
       "      feature_Can be trained or tamed by humans feature_Is found in zoos  \\\n",
       "perch                                         0                        1   \n",
       "\n",
       "      feature_Lives in water feature_Lives in trees feature_Lives underground  \\\n",
       "perch                      1                      0                         0   \n",
       "\n",
       "      feature_Lives on land feature_Is native to Africa  \\\n",
       "perch                     0                           0   \n",
       "\n",
       "      feature_Is native to Asia feature_Is native to North America  \\\n",
       "perch                         0                                  1   \n",
       "\n",
       "      feature_Is native to South America feature_Is native to Australia  \\\n",
       "perch                                  0                              0   \n",
       "\n",
       "      feature_Is native to Europe feature_Lives in Arctic/far North  \\\n",
       "perch                           1                                 0   \n",
       "\n",
       "      feature_Is found in deserts feature_Is found in forests  \\\n",
       "perch                           0                           0   \n",
       "\n",
       "      feature_Is found in oceans feature_Is found in grasslands  \\\n",
       "perch                          0                              0   \n",
       "\n",
       "      feature_Is found in mountains feature_Lives in burrows  \\\n",
       "perch                             0                        0   \n",
       "\n",
       "      feature_Lays eggs feature_Gives birth feature_Is venomous  \\\n",
       "perch                 1                   0                   0   \n",
       "\n",
       "      feature_Is domesticated feature_Lives in groups feature_Is solitary  \\\n",
       "perch                       0                       1                   0   \n",
       "\n",
       "      feature_Builds nests feature_Is migratory feature_Has social hierarchy  \\\n",
       "perch                    0                    0                            0   \n",
       "\n",
       "      feature_Uses tools feature_Shows intelligence  \\\n",
       "perch                  0                          0   \n",
       "\n",
       "      feature_Communicates vocally feature_Can change color  \\\n",
       "perch                            0                        0   \n",
       "\n",
       "      feature_Is capable of mimicry feature_Has echolocation  \\\n",
       "perch                             0                        0   \n",
       "\n",
       "      feature_Is known for speed feature_Is known for strength  \\\n",
       "perch                          0                             0   \n",
       "\n",
       "      feature_Is kept as a pet feature_Is used in farming  \\\n",
       "perch                        0                          1   \n",
       "\n",
       "      feature_Is hunted by humans feature_Is used for food by humans  \\\n",
       "perch                           1                                  1   \n",
       "\n",
       "      feature_Is used for transportation  \\\n",
       "perch                                  0   \n",
       "\n",
       "      feature_Is used in scientific research feature_Has a long lifespan  \\\n",
       "perch                                      1                           0   \n",
       "\n",
       "      feature_Has regenerative ability feature_Is vertebrate  \\\n",
       "perch                                0                     1   \n",
       "\n",
       "      feature_Is invertebrate feature_Is marsupial feature_Is placental  \\\n",
       "perch                       0                    0                    0   \n",
       "\n",
       "      feature_Is monotreme feature_Is flightless feature_Has webbed feet  \\\n",
       "perch                    0                     1                       0   \n",
       "\n",
       "      feature_Is known for intelligence feature_Is a scavenger  \\\n",
       "perch                                 0                      0   \n",
       "\n",
       "      feature_Is territorial feature_Is endangered feature_Is bioluminescent  \\\n",
       "perch                      0                     0                         0   \n",
       "\n",
       "      feature_Is capable of parental care feature_Is a pollinator  \\\n",
       "perch                                   0                       0   \n",
       "\n",
       "      feature_Can tolerate extreme temperatures  \\\n",
       "perch                                         0   \n",
       "\n",
       "      feature_Exhibits seasonal color changes  \\\n",
       "perch                                       0   \n",
       "\n",
       "      feature_Is active during dawn or dusk (crepuscular)  \\\n",
       "perch                                                  0    \n",
       "\n",
       "      feature_Produces pheromones for communication  \\\n",
       "perch                                             0   \n",
       "\n",
       "      feature_Lives symbiotically with other species  \\\n",
       "perch                                              0   \n",
       "\n",
       "      feature_Is bi-parental (both parents care for offspring)  \\\n",
       "perch                                                  0         \n",
       "\n",
       "      feature_Displays mating rituals  \\\n",
       "perch                               1   \n",
       "\n",
       "      feature_Has specialized courtship behavior  \\\n",
       "perch                                          1   \n",
       "\n",
       "      feature_Exhibits territorial marking  \\\n",
       "perch                                    0   \n",
       "\n",
       "      feature_Is associated with mythology or folklore  \\\n",
       "perch                                                0   \n",
       "\n",
       "      feature_Exhibits altruistic behavior  \\\n",
       "perch                                    0   \n",
       "\n",
       "      feature_Is a keystone species in its ecosystem  \\\n",
       "perch                                              0   \n",
       "\n",
       "      feature_Can regenerate body parts  \\\n",
       "perch                                 0   \n",
       "\n",
       "      feature_Is raised in captivity or farms  \\\n",
       "perch                                       1   \n",
       "\n",
       "      feature_Has unique reproductive strategies (e.g., asexual reproduction)  \\\n",
       "perch                                                  0                        \n",
       "\n",
       "      feature_Hibernates during winter  \\\n",
       "perch                                0   \n",
       "\n",
       "      feature_Has a role in biological pest control  \\\n",
       "perch                                             1   \n",
       "\n",
       "      feature_Has distinct seasonal breeding cycles  \\\n",
       "perch                                             1   \n",
       "\n",
       "      feature_Forms a symbiotic relationship with plants (e.g., pollination)  \\\n",
       "perch                                                  0                       \n",
       "\n",
       "      feature_Uses specific vocalizations to communicate  \\\n",
       "perch                                                  0   \n",
       "\n",
       "      feature_Is a flagship species (conservation symbol)  \\\n",
       "perch                                                  0    \n",
       "\n",
       "      feature_Displays warning coloration  \n",
       "perch                                   0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = X_df[\"feature_Has feathers\"]  # adjust exact column name\n",
    "f = X_df[\"feature_Is bird\"]\n",
    "\n",
    "violations = X_df[(m == 0) & (f == 1)]\n",
    "violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(vf_featurecols)[np.argsort(LLM)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(vf_featurecols)[np.argsort(human)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf[vf_featuredf[\"feature_Is a flagship species (conservation symbol)\"] == 1][\"feature_Is a flagship species (conservation symbol)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    correlation_matrix = df.corr()\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape), k=1)  # Upper triangle mask\n",
    "    corr = correlation_matrix.where(mask == 0)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(corr, annot=False, cmap='RdBu', fmt=\".1f\", mask=np.triu(np.ones_like(corr, dtype=bool)), vmin=-1, vmax=1)\n",
    "    plt.show()\n",
    "    return correlation_matrix\n",
    "\n",
    "vf_featuredf_corr = correlation_matrix(vf_featuredf)\n",
    "\n",
    "print(np.mean(vf_featuredf_corr > 0.5))         # % of correlated features (greater than 0.5 corr)\n",
    "print(np.mean(vf_featuredf_corr < -0.5))        # % of correlated features (less than -0.5 corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf_corr[\"feature_Is insect\"].to_dict()[\"feature_Has segmented body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_columns(corr, threshold=0.65):\n",
    "    highly_correlated = {}\n",
    "    # can change the logic to only keep correlated columns that as least correlated with others\n",
    "    # if (np.sum(vf_featuredf_corr.loc[\"feature_Is mammal\"].values) - 1)/(len(vf_featuredf_corr.loc[\"feature_Is mammal\"].values) - 1)\n",
    "    for i, col in enumerate(corr.columns):\n",
    "        for prev_col in corr.columns[:i]:  # Check only previous columns\n",
    "            if abs(corr.loc[col, prev_col]) >= threshold:  # Check correlation\n",
    "                highly_correlated[col] = prev_col\n",
    "                print(col, prev_col, corr.loc[col, prev_col])\n",
    "                break\n",
    "    return highly_correlated\n",
    "\n",
    "high_corr_columns_vf = get_highly_correlated_columns(vf_featuredf_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_responsedf(df):\n",
    "    featuredict = vf_featuredf.to_dict(orient='index')\n",
    "    mapped_features = df['response'].map(featuredict)\n",
    "    mapped_features = mapped_features.apply(lambda x: x if isinstance(x, dict) else {})\n",
    "    fc = pd.DataFrame(mapped_features.tolist())\n",
    "    df = pd.concat([df, fc], axis=1)\n",
    "    df = df.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})\n",
    "    dropped_rows = df[df[vf_featurecols].isna().any(axis=1)]\n",
    "    df = df.dropna(subset=vf_featurecols)\n",
    "    for col in vf_featurecols:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df, dropped_rows\n",
    "\n",
    "data2, dropped_rows2 = add_features_to_responsedf(data2)\n",
    "\n",
    "feature_cols = [col for col in data2.columns if col.startswith('feature_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means(df, featuredf, featurecols):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6.5, 3))\n",
    "    ax[0].hist(featuredf.mean(axis=0).values, color=\"indianred\", alpha=0.6);\n",
    "    ax[0].set_xlabel(\"P(feat)=1\")\n",
    "    ax[0].set_ylabel(\"Number of features\")\n",
    "    ax[1].hist(df[featurecols].mean(axis=0).values, color=\"indianred\", alpha=0.6);\n",
    "    ax[1].set_xlabel(\"P(feat)=1 in responses\")\n",
    "plot_means(data2, vf_featuredf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[vf_featurecols].corr().loc[[\"feature_Is monotreme\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Correlated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_vf = list(high_corr_columns_vf.keys())\n",
    "# vf_featuredf = vf_featuredf.drop(columns = remove_columns_vf)\n",
    "# data2 = data2.drop(columns = remove_columns_vf)\n",
    "# vf_featurecols = [item for item in vf_featurecols if item not in remove_columns_vf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_active = vf_featuredf.mean() * 100\n",
    "top5 = percent_active.sort_values(ascending=False).head(10)\n",
    "bottom5 = percent_active.sort_values().head(10)\n",
    "combined = pd.concat([top5, bottom5])\n",
    "plt.figure(figsize=(10, 3.5))\n",
    "bars = plt.bar(combined.index, combined.values, color=\"#D8BFD8\")\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage Active\")\n",
    "plt.title(\"Top 5 and Bottom 5 Most Active Features -- UNIFORM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_active2 = data2[vf_featurecols].mean() * 100\n",
    "top5 = percent_active2.sort_values(ascending=False).head(10)\n",
    "bottom5 = percent_active2.sort_values().head(10)\n",
    "combined = pd.concat([top5, bottom5])\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(combined.index, combined.values, color=\"#D8BFD8\")\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage Active (1)\")\n",
    "plt.title(\"Top 5 and Bottom 5 Most Active Features -- RESPONSES\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_diff = percent_active2.subtract(percent_active)\n",
    "top5 = percent_diff.sort_values(ascending=False).head(10)\n",
    "bottom5 = percent_diff.sort_values().head(10)\n",
    "combined = pd.concat([top5, bottom5])\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(combined.index, combined.values, color=\"#D8BFD8\")\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage Active (1)\")\n",
    "plt.title(\"Top 5 and Bottom 5 Differences (Responses - Uniform)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Datasets - Figure 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 100\n",
    "\n",
    "# Uniform\n",
    "sum_activity_diff1 = []\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "unique_responses = data2[\"response\"].unique()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff1.append(sum_abs_mean_activity_diff)\n",
    "\n",
    "# Frequency-based\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)  # dict: response  log_freq\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff2 = []\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff2.append(sum_abs_mean_activity_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(5,3), gridspec_kw={'width_ratios': [0.5,0.5,0.3]})\n",
    "ax1.hist(sum_activity_diff1, alpha=0.5, label='Uniform sampling', color='lightcoral')\n",
    "ax1.set_xlim(1.5, 2.25)\n",
    "ax1.set_ylabel(\"Num. simulations\")\n",
    "ax2.hist(sum_activity_diff2, alpha=0.7, label='Frequency-based sampling', color='indianred')\n",
    "ax2.set_xlim(4.5, 5.5)\n",
    "ax2.set_xlabel(\"Total abs. feature activity diff.\")\n",
    "vline_value = np.sum(np.abs(vf_featuredf[feature_cols].mean() - data2[feature_cols].mean()))\n",
    "ax3.axvline(vline_value, color='red', linestyle='--', linewidth=2, label=f'Data = {vline_value:.2f}')\n",
    "ax3.set_xlim(vline_value - 0.5, vline_value + 0.5)\n",
    "\n",
    "for a, b in zip([ax1, ax2], [ax2, ax3]):\n",
    "    a.spines['right'].set_visible(False)\n",
    "    b.spines['left'].set_visible(False)\n",
    "\n",
    "d = 0.015  # size of break slant\n",
    "def add_break(ax_left, ax_right):\n",
    "    kwargs = dict(color='k', clip_on=False)\n",
    "    ax_left.plot((1 - d, 1 + d), (-d, +d), transform=ax_left.transAxes, **kwargs)\n",
    "    ax_right.plot((-d, +d), (-d, +d), transform=ax_right.transAxes, **kwargs)\n",
    "\n",
    "add_break(ax1, ax2)\n",
    "add_break(ax2, ax3)\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.58, 1.09), ncol=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    num_features_same = [np.nan]  # Initialize with nan for the first row\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same.append(np.nan)\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            # consecutive_1s = ((row1 == 1) & (row2 == 1))\n",
    "            num_features_same.append(consecutive_1s.sum())\n",
    "    \n",
    "    group['num_features_same'] = num_features_same\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "summary = (\n",
    "    data2\n",
    "    .groupby(\"order\")[\"num_features_same\"]\n",
    "    .agg(mean=\"mean\", sem=lambda x: x.std(ddof=1) / np.sqrt(len(x)))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(\n",
    "    summary[\"order\"][:30],\n",
    "    summary[\"mean\"][:30],\n",
    "    yerr=summary[\"sem\"][:30],\n",
    "    fmt=\"o-\",\n",
    "    capsize=3\n",
    ")\n",
    "plt.xlabel(\"Response Order\")\n",
    "plt.ylabel(\"Mean HS\")\n",
    "plt.title(\"Mean HS over Response Order\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "data2 = data2.reset_index(drop=True)\n",
    "\n",
    "model = smf.mixedlm(\n",
    "    \"RT ~ num_features_same * order\",\n",
    "    data=data2,\n",
    "    groups=data2[\"pid\"]\n",
    ").fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "data2[data2[\"num_features_same\"] > 122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data2[data2[\"num_features_same\"] > 122][\"RT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(\n",
    "    data2.loc[\n",
    "        (data2[\"num_features_same\"] > 117) & (data2[\"num_features_same\"] < 122),\n",
    "        \"RT\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(\n",
    "    data2.loc[\n",
    "        (data2[\"num_features_same\"] > 113) & (data2[\"num_features_same\"] < 117),\n",
    "        \"RT\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle within each pid - TAKES SUPER LONG, SERIALIZE\n",
    "try:\n",
    "    means = pk.load(open(\"../files/shuffled_data_meanHS.pk\", \"rb\"))\n",
    "except:\n",
    "    means = []\n",
    "    for _ in tqdm(range(100)):\n",
    "        shuffled_data2 = data2.groupby(\"pid\", group_keys=False).apply(lambda x: x.sample(frac=1).reset_index(drop=True))\n",
    "        shuffled_data2 = get_num_features_same(shuffled_data2, vf_featurecols)\n",
    "        means.append(np.mean(shuffled_data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.figure(figsize=(4, 3.5))\n",
    "data_mean = np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values)\n",
    "bax = brokenaxes(xlims=((98, 99), (data_mean - 0.1, data_mean + 0.1)),hspace=0.05)\n",
    "bax.hist(means, bins=6, color='lightcoral', label=\"Shuffled\")\n",
    "bax.axvline(data_mean, color='red', linestyle='--', linewidth=2, label=f'Data = {data_mean:.2f}')\n",
    "bax.set_xlabel(\"Mean num features same\", labelpad=25)\n",
    "bax.set_ylabel(\"Number of simulations\", labelpad=25)\n",
    "bax.axs[0].set_xticks([98.2, 98.5, 98.8])\n",
    "bax.axs[1].set_xticks([105, 106])  # or [round(data_mean)] if dynamic\n",
    "bax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['num_features_same_next'] = data2.groupby('pid')['num_features_same'].shift(-1)\n",
    "plot_df = data2.dropna(subset=['num_features_same', 'num_features_same_next'])\n",
    "mean_val = np.mean(data2['num_features_same'])\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.scatter(plot_df['num_features_same_next'], plot_df['num_features_same'], alpha=0.1, s=10, c=\"indianred\")\n",
    "plt.ylabel(r'$HS_i$')\n",
    "plt.xlabel(r'$HS_{i+1}$')\n",
    "plt.xlim(np.min(data2['num_features_same']), np.max(data2['num_features_same']))\n",
    "plt.ylim(np.min(data2['num_features_same']), np.max(data2['num_features_same']))\n",
    "plt.axhline(mean_val, color='black', linestyle='--', linewidth=1)\n",
    "plt.axvline(mean_val, color='black', linestyle='--', linewidth=1)\n",
    "# --- Quadrant counts ---\n",
    "x = plot_df['num_features_same_next']\n",
    "y = plot_df['num_features_same']\n",
    "q1 = np.sum((x > mean_val) & (y > mean_val))  # top-right\n",
    "q2 = np.sum((x < mean_val) & (y > mean_val))  # top-left\n",
    "q3 = np.sum((x < mean_val) & (y < mean_val))  # bottom-left\n",
    "q4 = np.sum((x > mean_val) & (y < mean_val))  # bottom-right\n",
    "# Place counts on the plot\n",
    "xlim = plt.xlim()\n",
    "ylim = plt.ylim()\n",
    "plt.text(xlim[1]*0.97, ylim[1]*0.97, f\"{q1}\", ha='right', va='top', fontsize=9, weight='bold')\n",
    "plt.text(xlim[0]*1.03, ylim[1]*0.97, f\"{q2}\", ha='left', va='top', fontsize=9, weight='bold')\n",
    "plt.text(xlim[0]*1.03, ylim[0]*1.03, f\"{q3}\", ha='left', va='bottom', fontsize=9, weight='bold')\n",
    "plt.text(xlim[1]*0.97, ylim[0]*1.03, f\"{q4}\", ha='right', va='bottom', fontsize=9, weight='bold')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['num_features_same_next'] = data2.groupby('pid')['num_features_same'].shift(-1)\n",
    "data2['num_features_same_next_next'] = data2.groupby('pid')['num_features_same'].shift(-2)\n",
    "plot_df = data2.dropna(subset=['num_features_same', 'num_features_same_next', 'num_features_same_next_next'])\n",
    "x = plot_df['num_features_same']\n",
    "y = plot_df['num_features_same_next']\n",
    "z = plot_df['num_features_same_next_next']\n",
    "mean_val = np.mean(data2['num_features_same'])\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, alpha=0.2, s=10, c=\"indianred\")\n",
    "ax.set_xlabel(r'$HS_i$')\n",
    "ax.set_ylabel(r'$HS_{i+1}$')\n",
    "ax.set_zlabel(r'$HS_{i+2}$')\n",
    "ax.set_xlim(np.min(data2['num_features_same']), np.max(data2['num_features_same']))\n",
    "ax.set_ylim(np.min(data2['num_features_same']), np.max(data2['num_features_same']))\n",
    "ax.plot([mean_val, mean_val], [ax.get_ylim()[0], ax.get_ylim()[1]], [ax.get_zlim()[0], ax.get_zlim()[1]],\n",
    "        color='black', linestyle='--', linewidth=1)\n",
    "ax.plot([ax.get_xlim()[0], ax.get_xlim()[1]], [mean_val, mean_val], [ax.get_zlim()[0], ax.get_zlim()[1]],\n",
    "        color='black', linestyle='--', linewidth=1)\n",
    "ax.plot([ax.get_xlim()[0], ax.get_xlim()[1]], [ax.get_ylim()[0], ax.get_ylim()[1]], [mean_val, mean_val],\n",
    "        color='black', linestyle='--', linewidth=1)\n",
    "octants = {\n",
    "    '+ + +': np.sum((x > mean_val) & (y > mean_val) & (z > mean_val)),\n",
    "    '- + +': np.sum((x < mean_val) & (y > mean_val) & (z > mean_val)),\n",
    "    '+ - +': np.sum((x > mean_val) & (y < mean_val) & (z > mean_val)),\n",
    "    '- - +': np.sum((x < mean_val) & (y < mean_val) & (z > mean_val)),\n",
    "    '+ + -': np.sum((x > mean_val) & (y > mean_val) & (z < mean_val)),\n",
    "    '- + -': np.sum((x < mean_val) & (y > mean_val) & (z < mean_val)),\n",
    "    '+ - -': np.sum((x > mean_val) & (y < mean_val) & (z < mean_val)),\n",
    "    '- - -': np.sum((x < mean_val) & (y < mean_val) & (z < mean_val)),\n",
    "}\n",
    "xlim, ylim, zlim = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()\n",
    "coords = {\n",
    "    '+ + +': (xlim[1]*0.9, ylim[1]*0.9, zlim[1]*0.9),\n",
    "    '- + +': (xlim[0]*1.1, ylim[1]*0.9, zlim[1]*0.9),\n",
    "    '+ - +': (xlim[1]*0.9, ylim[0]*1.1, zlim[1]*0.9),\n",
    "    '- - +': (xlim[0]*1.1, ylim[0]*1.1, zlim[1]*0.9),\n",
    "    '+ + -': (xlim[1]*0.9, ylim[1]*0.9, zlim[0]*1.1),\n",
    "    '- + -': (xlim[0]*1.1, ylim[1]*0.9, zlim[0]*1.1),\n",
    "    '+ - -': (xlim[1]*0.9, ylim[0]*1.1, zlim[0]*1.1),\n",
    "    '- - -': (xlim[0]*1.1, ylim[0]*1.1, zlim[0]*1.1),\n",
    "}\n",
    "for key, count in octants.items():\n",
    "    ax.text(*coords[key], f\"{key}\\n{count}\", color='black', fontsize=8, ha='center', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N=back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_6back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(6, len(group)):\n",
    "        row1 = group.loc[i - 6, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_6back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_7back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(7, len(group)):\n",
    "        row1 = group.loc[i - 7, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_7back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_8back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(8, len(group)):\n",
    "        row1 = group.loc[i - 8, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_8back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_9back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(9, len(group)):\n",
    "        row1 = group.loc[i - 9, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_9back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_10back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(10, len(group)):\n",
    "        row1 = group.loc[i - 10, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_10back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_11back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(11, len(group)):\n",
    "        row1 = group.loc[i - 11, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_11back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_12back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(12, len(group)):\n",
    "        row1 = group.loc[i - 12, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_12back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    # np.mean(data2[\"num_features_same_12back\"]),\n",
    "    # np.mean(data2[\"num_features_same_11back\"]),\n",
    "    # np.mean(data2[\"num_features_same_10back\"]),\n",
    "    # np.mean(data2[\"num_features_same_9back\"]),\n",
    "    # np.mean(data2[\"num_features_same_8back\"]),\n",
    "    # np.mean(data2[\"num_features_same_7back\"]),\n",
    "    # np.mean(data2[\"num_features_same_6back\"]),\n",
    "    np.mean(data2[\"num_features_same_5back\"]),\n",
    "    np.mean(data2[\"num_features_same_4back\"]),\n",
    "    np.mean(data2[\"num_features_same_3back\"]),\n",
    "    np.mean(data2[\"num_features_same_2back\"]),\n",
    "    np.mean(data2[\"num_features_same\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    # np.std(data2[\"num_features_same_12back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_12back\"].dropna())),\n",
    "    # np.std(data2[\"num_features_same_11back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_11back\"].dropna())),\n",
    "    # np.std(data2[\"num_features_same_10back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_10back\"].dropna())),\n",
    "    # np.std(data2[\"num_features_same_9back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_9back\"].dropna())),\n",
    "    # np.std(data2[\"num_features_same_8back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_8back\"].dropna())),\n",
    "    # np.std(data2[\"num_features_same_7back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_7back\"].dropna())),\n",
    "    # np.std(data2[\"num_features_same_6back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_6back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_5back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_5back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_4back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_4back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_3back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_3back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_2back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_2back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same\"], ddof=1) / np.sqrt(len(data2[\"num_features_same\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [\n",
    "    # -12, -11, -10, -9, -8, -7, -6, \\\n",
    "    -5, -4, -3, -2, -1]\n",
    "\n",
    "# Plot bar chart with error bars\n",
    "# plt.figure(figsize=(7.5, 4.55))\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel(\"Pos. preceeding most recent resp.\")\n",
    "plt.ylabel(\"Mean num. features same\")\n",
    "plt.ylim(95, 107)\n",
    "plt.xticks(x_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HS Profiles in Ppt responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data2.groupby(\"pid\").agg(list)[\"num_features_same\"]\n",
    "for pid, values in grouped.items():\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(values[1:], marker='o', linestyle='-', color='steelblue')\n",
    "    plt.title(f'PID: {pid}  num_features_same')\n",
    "    plt.xlabel('Index within participant')\n",
    "    plt.ylabel('num_features_same')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dot_product(df, featurecols):\n",
    "    df['dot_product'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_dot_product, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_dot_product(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    n = len(group)\n",
    "    \n",
    "    if n < 3:\n",
    "        group['dot_product'] = [np.nan] * n\n",
    "        return group\n",
    "    \n",
    "    dot_product = [np.nan, np.nan]  # For first two rows\n",
    "\n",
    "    for i in range(2, n):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i - 1, featurecols]\n",
    "        row3 = group.loc[i, featurecols]\n",
    "\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            dot_product.append(np.nan)\n",
    "        else:\n",
    "            vec1 = (row1 == row2).astype(int)\n",
    "            vec2 = (row2 == row3).astype(int)\n",
    "            dot_product.append(np.dot(vec1, vec2))\n",
    "\n",
    "    group['dot_product'] = dot_product\n",
    "    return group\n",
    "\n",
    "data2 = get_dot_product(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_ones(df, feature_col):\n",
    "    # result = {}\n",
    "    counts = {}\n",
    "    for subject in df[\"pid\"].unique():\n",
    "        subject_data = df[df[\"pid\"] == subject][feature_col].values\n",
    "        # counts = {}\n",
    "        current_count = 0\n",
    "        for value in subject_data:\n",
    "            if value == 1:\n",
    "                current_count += 1\n",
    "            elif current_count > 0:\n",
    "                counts[current_count] = counts.get(current_count, 0) + 1\n",
    "                current_count = 0\n",
    "        # Add the last streak if it ends with a 1\n",
    "        if current_count > 0:\n",
    "            counts[current_count] = counts.get(current_count, 0) + 1 \n",
    "        # result[subject] = counts\n",
    "    # return result\n",
    "    return counts\n",
    "\n",
    "def make_persistance_plots(df, featuredf, featurecols):\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "        plt.plot(list(fco.keys()), np.array(list(fco.values())) / (np.sum(list(fco.values()))), label = \"Data\")\n",
    "        \n",
    "        x = np.arange(1, list(fco.keys())[-1] + 1)\n",
    "        geometric_pdf = (p ** x) * (1 - p) \n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def make_persistance_plots_conditional(df, featuredf, featurecols):\n",
    "    cols_to_remove = []\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "        print(fco)\n",
    "        x = np.array(list(fco.keys()))\n",
    "        \n",
    "        data_values = np.array(list(fco.values()))\n",
    "        observed = data_values / np.sum(data_values)\n",
    "        plt.plot(x, observed, label = \"Data\")\n",
    "        \n",
    "        geometric_pdf = (p ** (x - 1)) * (1 - p) \n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "\n",
    "        # expected = geometric_pdf * np.sum(data_values)\n",
    "        # expected *= np.sum(data_values) / np.sum(expected)\n",
    "\n",
    "        chi2_stat, p_value = chisquare(data_values, f_exp=geometric_pdf / np.sum(geometric_pdf) * np.sum(data_values))\n",
    "        print(f\"Chi-Square Statistic: {chi2_stat}, p-value: {p_value}\")\n",
    "        if p_value > 0.01:\n",
    "            cols_to_remove.append(col)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return cols_to_remove\n",
    "\n",
    "def make_persistance_plots_hazard(df, featuredf, featurecols):\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "\n",
    "        remaining_population = sum(fco.values())\n",
    "        hazard_function = []\n",
    "\n",
    "        for i, freq_i in fco.items():\n",
    "            hazard_function.append(freq_i / remaining_population)  # Hazard probability for i\n",
    "            remaining_population -= freq_i\n",
    "\n",
    "        plt.plot(list(fco.keys()), hazard_function, label = \"Data\")\n",
    "        \n",
    "        x = np.arange(1, list(fco.keys())[-1] + 1)\n",
    "        geometric_pdf = [(1 - p)] * len(x)\n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"sum_features\"] = data2[feature_cols].sum(axis=1)\n",
    "data2[\"logRT\"] = np.log(data2[\"RT\"] + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = {}\n",
    "for col in feature_cols:\n",
    "    new_col = f\"{col}_switch\"\n",
    "    tmp = (data2.groupby('pid')[col].transform(lambda x: (x != x.shift()).astype(int)))\n",
    "    tmp.loc[data2.groupby('pid').head(1).index] = np.nan\n",
    "    new_cols[new_col] = tmp\n",
    "data2 = pd.concat([data2, pd.DataFrame(new_cols, index=data2.index)], axis=1)\n",
    "\n",
    "diff_dict = {}\n",
    "for col in feature_cols:\n",
    "    bin_col = f\"{col}_switch\"\n",
    "    temp = data2.dropna(subset=[bin_col])\n",
    "    logrt_diff = temp.groupby('pid').apply(lambda g: g.loc[g[bin_col] == 1, 'logRT'].mean() - g.loc[g[bin_col] == 0, 'logRT'].mean(), include_groups=False)  # Compute logRT difference per pid\n",
    "    diff_dict[col] = logrt_diff\n",
    "diff_df = pd.DataFrame(diff_dict)\n",
    "# Optional: reset index if you want 'pid' as a column\n",
    "# diff_df = diff_df.reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(4,3.5))\n",
    "plt.hist(diff_df.sum(axis=1).values, color=\"indianred\", alpha=0.6);\n",
    "plt.ylabel(\"Num. participants\")\n",
    "plt.xlabel(\"log(RT) diff. (Switch - Stay)\");\n",
    "\n",
    "plt.figure(figsize=(4,3.5))\n",
    "plt.hist(diff_df.sum(axis=0).values, color=\"indianred\", alpha=0.6);\n",
    "plt.ylabel(\"Num. features\")\n",
    "plt.xlabel(\"log(RT) diff. (Switch - Stay)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdict = json.load(open(\"../files/freq_abs_log.json\", \"r\"))\n",
    "data2[\"freq\"] = data2[\"response\"].map(freqdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data2[\"sum_features\"], data2[\"RT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you already computed log_likelihood and logRT\n",
    "bins = np.linspace(data2[\"log_likelihood\"].min(), data2[\"log_likelihood\"].max(), 9)\n",
    "data2[\"ll_bin\"] = pd.cut(data2[\"log_likelihood\"], bins=bins)\n",
    "\n",
    "# Compute mean logRT per bin\n",
    "mean_logRT = data2.groupby(\"ll_bin\")[\"logRT\"].mean()\n",
    "bin_centers = [interval.mid for interval in mean_logRT.index]\n",
    "\n",
    "# Plot scatter + mean trend\n",
    "plt.scatter(data2[\"log_likelihood\"], data2[\"logRT\"], alpha=0.4, label=\"Individual points\")\n",
    "plt.plot(bin_centers, mean_logRT, color=\"red\", marker=\"o\", label=\"Mean logRT per bin\")\n",
    "plt.xlabel(\"Log Likelihood\")\n",
    "plt.ylabel(\"log(RT)\")\n",
    "plt.title(\"Log Likelihood vs log(RT)\")\n",
    "plt.ylim(-2, 6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data2[\"RT\"].quantile(0.25)\n",
    "Q3 = data2[\"RT\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for non-outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame\n",
    "data2_no_outliers = data2[(data2[\"RT\"] >= lower_bound) & (data2[\"RT\"] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data2 = data2.copy()\n",
    "data2[\"logRT\"] = np.log(data2[\"RT\"])\n",
    "\n",
    "Q1 = data2[\"logRT\"].quantile(0.25)\n",
    "Q3 = data2[\"logRT\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "data2_no_outliers = data2[\n",
    "    (data2[\"logRT\"] >= lower) & (data2[\"logRT\"] <= upper)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_no_outliers = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_no_outliers[\"RT\"] = data2_no_outliers[\"RT\"] + 0.001\n",
    "data2_no_outliers[\"logRT\"] = np.log(data2_no_outliers[\"RT\"])\n",
    "bin_edges = np.arange(\n",
    "    data2_no_outliers[\"num_features_same\"].min() + 10, \n",
    "    data2_no_outliers[\"num_features_same\"].max(), \n",
    "    10\n",
    ")\n",
    "data2_no_outliers[\"bin\"] = pd.cut(data2_no_outliers[\"num_features_same\"], bins=bin_edges)\n",
    "group_stats = data2_no_outliers.groupby(\"bin\")[\"logRT\"].agg(['mean', sem]).reset_index()\n",
    "bin_centers = [interval.mid for interval in group_stats[\"bin\"]]\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.errorbar(\n",
    "    bin_centers,\n",
    "    group_stats[\"mean\"],\n",
    "    yerr=group_stats[\"sem\"],\n",
    "    fmt='o-',               # line with circle markers\n",
    "    color='red',\n",
    "    ecolor='black',         # color of error bars\n",
    "    elinewidth=1,\n",
    "    capsize=4\n",
    ")\n",
    "bin_labels = [f\"{int(interval.left)}{int(interval.right)}\" for interval in group_stats[\"bin\"]]\n",
    "plt.xticks(ticks=bin_centers, labels=bin_labels, rotation=45)\n",
    "plt.xlabel(\"Num. features same\")\n",
    "plt.ylabel(\"Mean log(RT)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_no_outliers[\"RT\"] = data2_no_outliers[\"RT\"] + 0.001\n",
    "data2_no_outliers[\"logRT\"] = np.log(data2_no_outliers[\"RT\"])\n",
    "data2_no_outliers[\"bin\"] = pd.qcut(data2_no_outliers[\"num_features_same\"], q=5)\n",
    "group_stats = data2_no_outliers.groupby(\"bin\")[\"logRT\"].agg(['mean', sem]).reset_index()\n",
    "x_positions = [ (interval.left + interval.right) / 2 for interval in group_stats[\"bin\"] ]\n",
    "bin_labels = [f\"{int(interval.left)}{int(interval.right)}\" for interval in group_stats[\"bin\"]]\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.errorbar(\n",
    "    x_positions,\n",
    "    group_stats[\"mean\"],\n",
    "    yerr=group_stats[\"sem\"],\n",
    "    fmt='o-',\n",
    "    color='red',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=4\n",
    ")\n",
    "plt.xticks(ticks=x_positions, labels=bin_labels, rotation=45)\n",
    "plt.xlabel(\"Num. features same\")\n",
    "plt.ylabel(\"Mean log(RT)\")\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data2[data2[\"num_features_same\"] > 122][\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data2[(data2[\"num_features_same\"] > 117) & (data2[\"num_features_same\"] < 122)][\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(data2[(data2[\"num_features_same\"] < 122) & (data2[\"num_features_same\"] > 117)][\"response\"], data2[(data2[\"num_features_same\"] < 122) & (data2[\"num_features_same\"] > 117)][\"previous_response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(data2[data2[\"num_features_same\"] > 122][\"response\"], data2[data2[\"num_features_same\"] > 122][\"previous_response\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_1back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_1back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_0'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group)):\n",
    "        row1 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = 0\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_0'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_1ahead'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group) - 1):\n",
    "        row1 = group.loc[i + 1, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_1ahead'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_2ahead'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group) - 2):\n",
    "        row1 = group.loc[i + 2, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_2ahead'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data2 = get_num_features_same(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    np.mean(data2[\"RT_diff_5back\"]),\n",
    "    np.mean(data2[\"RT_diff_4back\"]),\n",
    "    np.mean(data2[\"RT_diff_3back\"]),\n",
    "    np.mean(data2[\"RT_diff_2back\"]),\n",
    "    np.mean(data2[\"RT_diff_1back\"]),\n",
    "    np.mean(data2[\"RT_diff_0\"]),\n",
    "    np.mean(data2[\"RT_diff_1ahead\"]),\n",
    "    np.mean(data2[\"RT_diff_2ahead\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    np.std(data2[\"RT_diff_5back\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_5back\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_4back\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_4back\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_3back\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_3back\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_2back\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_2back\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_1back\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_1back\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_0\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_0\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_1ahead\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_1ahead\"].dropna())),\n",
    "    np.std(data2[\"RT_diff_2ahead\"], ddof=1) / np.sqrt(len(data2[\"RT_diff_2ahead\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [-5, -4, -3, -2, -1, 0, 1, 2]\n",
    "\n",
    "# Plot bar chart with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.7, color='mediumpurple')\n",
    "plt.xlabel(\"Pos. wrt to most recent resp.\")\n",
    "plt.ylabel(\"Mean RT Difference\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vf_featuredf.iloc[:, 0]            # First column = names\n",
    "features = vf_featuredf.iloc[:, 1:]        # Rest = binary features\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=42, metric='hamming')\n",
    "embedding = tsne.fit_transform(features)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=50, alpha=0.5, c=\"mediumpurple\")\n",
    "plt.title(\"t-SNE of Binary Vectors\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_chunks(lst):\n",
    "    count = 0  # Count of contiguous chunks of ones with length > 1\n",
    "    i = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == 1:\n",
    "            start = i\n",
    "            while i < len(lst) and lst[i] == 1:\n",
    "                i += 1\n",
    "            if i - start > 1:\n",
    "                count += 1\n",
    "                if count >= 2:\n",
    "                    return 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return 0\n",
    "\n",
    "# Apply function to each feature_* column, grouped by 'pid'\n",
    "returned_to_same_feature = {\n",
    "    col: data2.groupby(\"pid\")[col].apply(has_two_chunks) for col in vf_featurecols\n",
    "}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "returned_to_same_feature_df = pd.DataFrame(returned_to_same_feature)\n",
    "returned = (np.sum(returned_to_same_feature_df, axis=0)/len(data2[\"pid\"].unique())).to_dict()\n",
    "returned = dict(sorted(returned.items(), key=lambda item: item[1]))\n",
    "plt.figure(figsize=(3,30))\n",
    "plt.barh(list(returned.keys()), list(returned.values()))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchical modelling\n",
    "hierarchical_fit = pk.load(open(\"../fits/hierarchical_fits_freqweightedhsactivity.pk\", \"rb\"))\n",
    "individual_params = hierarchical_fit[\"individual_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hierarchical_fit[\"sigma2\"]);\n",
    "plt.xlabel(\"Feature Group Var\")\n",
    "plt.ylabel(\"Num features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,          # try 540 if you want to experiment\n",
    "    learning_rate='auto',\n",
    "    init='random',\n",
    "    n_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "X_tsne = tsne.fit_transform(individual_params)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "sc = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=30, alpha=0.8, c=data2.groupby(\"pid\").count()[\"response\"].tolist(), cmap=\"Greens\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "# plt.title(\"t-SNE Visualization (141277  2D)\")\n",
    "plt.tight_layout()\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"Num. responses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import statsmodels.api as sm\n",
    "probs = np.exp(individual_params)               # exponentiate to make positive\n",
    "probs = probs / probs.sum(axis=1, keepdims=True)  # normalize rows to sum to 1\n",
    "row_entropies = entropy(probs, axis=1)  # shape (141,)\n",
    "print(np.corrcoef(row_entropies, data2.groupby(\"pid\").count()[\"response\"].tolist()))\n",
    "x = np.array(row_entropies)\n",
    "y = np.array(data2.groupby(\"pid\").count()[\"response\"].tolist())\n",
    "x_with_const = sm.add_constant(x)\n",
    "model = sm.OLS(y, x_with_const).fit()\n",
    "print(model.summary())\n",
    "plt.hist(row_entropies);\n",
    "plt.xlabel(\"Entropy of weights\")\n",
    "plt.ylabel(\"Num. participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# individual_params: shape (141, 277)\n",
    "\n",
    "# Compute row-wise variance (or std)\n",
    "row_variances = np.mean(individual_params, axis=1)   # or np.std(..., axis=1) for std\n",
    "# row_stds = np.std(individual_params, axis=1)\n",
    "\n",
    "# Response counts per PID\n",
    "y = np.array(data2.groupby(\"pid\").count()[\"response\"].tolist())\n",
    "\n",
    "# Correlation\n",
    "print(np.corrcoef(row_variances, y))\n",
    "\n",
    "# OLS regression\n",
    "x = row_variances\n",
    "x_with_const = sm.add_constant(x)\n",
    "model = sm.OLS(y, x_with_const).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Optional: visualize\n",
    "plt.hist(row_variances, bins=20, color='steelblue', edgecolor='black')\n",
    "plt.xlabel(\"Row Variance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Row-wise Variances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(individual_params[3])\n",
    "print(row_entropies[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "log_text = \"\"\"\n",
    "Starting hierarchical MAP-EM fitting for 141 participants...\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 001 | =3.190698 | Time=97.12s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 002 | =0.721679 | Time=91.87s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 003 | =0.371508 | Time=90.42s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 004 | =0.245130 | Time=89.41s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 005 | =0.181792 | Time=90.03s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 006 | =0.144091 | Time=87.23s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 007 | =0.119237 | Time=86.67s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 008 | =0.101666 | Time=85.17s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 009 | =0.088722 | Time=86.54s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 010 | =0.078539 | Time=85.11s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 011 | =0.070305 | Time=83.08s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 012 | =0.063829 | Time=85.09s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 013 | =0.058258 | Time=86.29s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 014 | =0.053594 | Time=85.48s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 015 | =0.049716 | Time=83.68s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 016 | =0.046371 | Time=86.04s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 017 | =0.043390 | Time=84.20s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 018 | =0.040813 | Time=83.71s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 019 | =0.038580 | Time=83.62s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 020 | =0.036423 | Time=86.33s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 021 | =0.034517 | Time=84.29s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 022 | =0.032910 | Time=84.96s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 023 | =0.031488 | Time=84.62s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 024 | =0.030089 | Time=83.73s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 025 | =0.028700 | Time=82.34s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 026 | =0.027672 | Time=85.24s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 027 | =0.026705 | Time=83.74s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 028 | =0.025878 | Time=84.12s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 029 | =0.024755 | Time=86.20s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 030 | =0.024071 | Time=85.77s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 031 | =0.023323 | Time=84.26s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 032 | =0.022700 | Time=84.38s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 033 | =0.022190 | Time=82.28s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 034 | =0.021500 | Time=83.43s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 035 | =0.020997 | Time=83.25s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 036 | =0.020491 | Time=81.97s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 037 | =0.019983 | Time=83.18s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 038 | =0.019475 | Time=85.03s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 039 | =0.019016 | Time=83.65s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 040 | =0.018715 | Time=83.47s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 041 | =0.018322 | Time=85.35s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 042 | =0.017987 | Time=83.98s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 043 | =0.017787 | Time=83.26s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 044 | =0.017404 | Time=84.28s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 045 | =0.017183 | Time=83.57s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 046 | =0.016902 | Time=83.56s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 047 | =0.016660 | Time=83.79s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 048 | =0.016344 | Time=84.49s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 049 | =0.016198 | Time=85.90s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 050 | =0.015996 | Time=84.24s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 051 | =0.015812 | Time=85.99s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 052 | =0.015560 | Time=85.69s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 053 | =0.015504 | Time=84.36s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 054 | =0.015160 | Time=85.09s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 055 | =0.015176 | Time=84.97s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 056 | =0.014997 | Time=84.97s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 057 | =0.014831 | Time=84.24s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 058 | =0.014761 | Time=83.77s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 059 | =0.014497 | Time=86.50s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 060 | =0.014405 | Time=86.21s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 061 | =0.014338 | Time=84.77s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 062 | =0.014233 | Time=86.10s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 063 | =0.014107 | Time=85.43s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 064 | =0.013974 | Time=86.55s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 065 | =0.013859 | Time=84.73s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 066 | =0.013799 | Time=85.33s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 067 | =0.013705 | Time=85.67s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 068 | =0.013658 | Time=84.51s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 069 | =0.013602 | Time=84.34s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 070 | =0.013466 | Time=85.08s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 071 | =0.013387 | Time=85.66s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 072 | =0.013261 | Time=84.85s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 073 | =0.013217 | Time=85.39s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 074 | =0.013238 | Time=84.46s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 075 | =0.013038 | Time=85.22s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 076 | =0.012930 | Time=85.03s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 077 | =0.012921 | Time=85.80s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 078 | =0.012912 | Time=86.33s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 079 | =0.012811 | Time=87.81s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 080 | =0.012743 | Time=88.37s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 081 | =0.012688 | Time=85.01s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 082 | =0.012604 | Time=85.53s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 083 | =0.012583 | Time=86.26s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 084 | =0.012496 | Time=86.60s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 085 | =0.012475 | Time=86.22s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 086 | =0.012411 | Time=86.89s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 087 | =0.012310 | Time=85.89s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 088 | =0.012328 | Time=86.55s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 089 | =0.012259 | Time=85.53s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 090 | =0.012128 | Time=85.95s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 091 | =0.012071 | Time=85.61s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 092 | =0.011969 | Time=88.47s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 093 | =0.011973 | Time=86.36s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 094 | =0.011922 | Time=86.12s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 095 | =0.011852 | Time=85.00s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 096 | =0.011807 | Time=85.66s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 097 | =0.011796 | Time=84.63s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 098 | =0.011699 | Time=86.89s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 099 | =0.011526 | Time=84.86s\n",
    "Running parallel E-step on 64 workers...\n",
    "Iter 100 | =0.011509 | Time=84.95s\n",
    "\"\"\"\n",
    "\n",
    "iters = [int(i) for i in re.findall(r\"Iter\\s+(\\d+)\", log_text)]\n",
    "deltas = [float(x) for x in re.findall(r\"=([\\d\\.]+)\", log_text)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(iters, deltas, linewidth=2)\n",
    "plt.xlabel(\"Iteration\", fontsize=12)\n",
    "plt.ylabel(\"Change in \", fontsize=12)\n",
    "# plt.title(\"Change in  Across Hierarchical MAP-EM Iterations\", fontsize=13)\n",
    "# plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data2.groupby(\"pid\").count()[\"response\"].tolist())\n",
    "plt.xlabel(\"Num. responses\")\n",
    "plt.ylabel(\"Num. participants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(individual_params, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(im, label=\"Feature Mean Value\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"PID index\")\n",
    "plt.title(\"Heatmap of Participant Learned Weights\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pids, n_features = 141, 277\n",
    "\n",
    "# Base mean per feature (same for all PIDs)\n",
    "feature_means = np.random.normal(0, 0.5, size=n_features)\n",
    "\n",
    "# Feature-level standard deviations (how much each feature varies per PID)\n",
    "feature_sds = np.abs(np.random.normal(0.2, 0.1, size=n_features))\n",
    "\n",
    "# Each PID independently samples from the feature distribution\n",
    "individual_params = np.random.normal(\n",
    "    loc=feature_means,\n",
    "    scale=feature_sds,\n",
    "    size=(n_pids, n_features)\n",
    ")\n",
    "# Plot to verify variation\n",
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(individual_params, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(im, label=\"Feature Mean Value\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"PID index\")\n",
    "plt.title(\"Heatmap of Participant Learned Weights (No Personal Offset, With Variation)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk.dump(individual_params, open(\"../fits/random_individual_params_forsim.pk\", \"wb\"))\n",
    "individual_params_random = pk.load(open(\"../fits/random_individual_params_forsim.pk\", \"rb\"))\n",
    "individual_params_recovered = pk.load(open(\"../fits/hierarchical_fits_freqweightedhsactivity_hierarchicaltesting.pk\", \"rb\"))[\"individual_params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(individual_params_random, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(im, label=\"Feature Mean Value\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"PID index\")\n",
    "plt.title(\"Heatmap of Participant Learned Weights\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(individual_params_recovered, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(im, label=\"Feature Mean Value\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"PID index\")\n",
    "plt.title(\"Heatmap of Participant Learned Weights\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_features_heatmap(df, featurecols):\n",
    "    pids = df[\"pid\"].unique()\n",
    "    cnt = 1\n",
    "    for pid in pids:\n",
    "        responses = df[df[\"pid\"] == pid][\"response\"].values\n",
    "        pid_data = df[df[\"pid\"] == pid][featurecols].reset_index(drop=True)\n",
    "        \n",
    "        plt.figure(figsize=(25, len(pid_data) * 0.25))\n",
    "        sns.heatmap(\n",
    "            pid_data,\n",
    "            cmap=sns.color_palette([\"white\", \"mediumpurple\"]),\n",
    "            cbar=False,\n",
    "            linewidths=0.5,\n",
    "            linecolor='black'\n",
    "        )\n",
    "        plt.title(f\"Binary Features Heatmap for PID {pid}\")\n",
    "        plt.gca().xaxis.tick_top()\n",
    "        plt.xticks(ticks=np.arange(len(featurecols)) + 0.5, labels=featurecols, rotation=90)\n",
    "        plt.yticks(ticks=np.arange(len(responses)) + 0.5, labels=responses, rotation=0)\n",
    "        plt.show()\n",
    "        \n",
    "        if cnt == 10:\n",
    "            break\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_grouped = data2.groupby(\"pid\")[feature_cols].mean()\n",
    "pid_mean_feat = pid_grouped.values  # shape (141, 138) in your case\n",
    "pid_list = pid_grouped.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.groupby(\"pid\").count()[\"response\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = vf_featuredf[feature_cols].mean().values.astype(float)\n",
    "pid_mean_feat_unbiased = pid_mean_feat - mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(pid_mean_feat_unbiased, aspect='auto', cmap='viridis')\n",
    "plt.colorbar(im, label=\"Feature Mean Value\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"PID index\")\n",
    "plt.title(\"Heatmap of Participant Mean Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_probs = pid_mean_feat / pid_mean_feat.sum(axis=1, keepdims=True)\n",
    "row_entropies = entropy(row_probs, axis=1)\n",
    "print(np.corrcoef(row_entropies, data2.groupby(\"pid\").count()[\"response\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(row_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "y = data2.groupby(\"pid\").count()[\"response\"].values\n",
    "X = row_entropies\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_variances = np.var(pid_mean_feat_unbiased, axis=1)\n",
    "y = data2.groupby(\"pid\").count()[\"response\"].values\n",
    "X = row_variances\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Dependent variable\n",
    "y = data2.groupby(\"pid\").count()[\"response\"].values\n",
    "\n",
    "# Independent variables\n",
    "X = np.column_stack([row_entropies, row_variances])  # shape: (141, 2)\n",
    "\n",
    "# Add intercept term\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.clip(pid_mean_feat_unbiased, 1e-12, None)\n",
    "row_probs = arr / arr.sum(axis=1, keepdims=True)\n",
    "row_entropies = entropy(row_probs, axis=1)\n",
    "print(row_entropies.shape)  # (141,)\n",
    "print(row_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = vf_featuredf[feature_cols].mean().values.reshape(1, -1)\n",
    "combined = np.vstack([pid_mean_feat, mean_vec])\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=15,\n",
    "    learning_rate='auto',\n",
    "    init='pca',\n",
    "    random_state=42,\n",
    "    metric='hamming'\n",
    ")\n",
    "tsne_results = tsne.fit_transform(combined)\n",
    "tsne_participants = tsne_results[:-1, :]\n",
    "tsne_mean = tsne_results[-1, :]\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(tsne_participants[:, 0], tsne_participants[:, 1],\n",
    "            c='indianred', alpha=0.7, s=30, label='Participants')\n",
    "plt.scatter(tsne_mean[0], tsne_mean[1],\n",
    "            c='black', s=80, marker='*', label='Uniform Mean')\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(\n",
    "    tsne_participants[:, 0],\n",
    "    tsne_participants[:, 1],\n",
    "    c='indianred', alpha=0.7, s=30, label='Participants'\n",
    ")\n",
    "plt.scatter(\n",
    "    tsne_mean[0], tsne_mean[1],\n",
    "    c='black', s=80, marker='*', label='Uniform Mean'\n",
    ")\n",
    "\n",
    "# --- Annotate each participant ---\n",
    "for i, pid in enumerate(pid_list):\n",
    "    plt.text(\n",
    "        tsne_participants[i, 0],\n",
    "        tsne_participants[i, 1],\n",
    "        str(pid),\n",
    "        fontsize=7,\n",
    "        ha='center',\n",
    "        va='center'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2[\"pid\"].isin([592, 392, 578, 994, 692, 696, 398, 490, 790])].groupby(\"pid\").agg(list)[\"response\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2[\"pid\"].isin([394, 575, 797, 896, 480, 381])].groupby(\"pid\").agg(list)[\"response\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

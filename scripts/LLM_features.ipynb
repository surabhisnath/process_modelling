{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "# keys = json.load(open(\"keys.json\"))\n",
    "# os.environ[\"OPENAI_API_KEY\"]=keys[\"OPENAI_API_KEY\"]\n",
    "# openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# import together\n",
    "# os.environ[\"TOGETHER_API_KEY\"]=keys[\"TOGETHER_API_KEY\"]\n",
    "# together.api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "from scipy.stats import chisquare\n",
    "plt.rcParams.update({\n",
    "    \"axes.facecolor\": \"white\",                  # background stays white\n",
    "    \"axes.edgecolor\": \"black\",                  # keep axis edges\n",
    "    \"patch.facecolor\": \"lightcoral\",\n",
    "    \"text.usetex\": False,                     # render text with LaTeX\n",
    "    \"font.family\": \"sans-serif\",                  # use serif fonts\n",
    "    \"axes.spines.top\": False,                # remove top border\n",
    "    \"axes.spines.right\": False,              # remove right border\n",
    "    \"axes.labelsize\": 16,                    # bigger axis labels\n",
    "    \"xtick.labelsize\": 14,                   # bigger x-tick labels\n",
    "    \"ytick.labelsize\": 14,                   # bigger y-tick labels\n",
    "    \"axes.titlesize\": 18,                    # bigger title\n",
    "    \"figure.dpi\": 120,                       # higher resolution\n",
    "})\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"../csvs/hills.csv\")\n",
    "data3 = pd.read_csv(\"../csvs/noconstraints.csv\")\n",
    "data4 = pd.read_csv(\"../csvs/similar.csv\")\n",
    "data5 = pd.read_csv(\"../csvs/divergent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredict = pk.load(open(f\"../files/features_gpt41.pk\", \"rb\"))\n",
    "# feature_names = list(next(iter(featuredict.values())).keys())\n",
    "# response_to_featurevector = {k: [1 if values.get(f, \"\").lower()[:4] == \"true\" else 0 for f in feature_names] for k, values in featuredict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredict = pk.load(open(f\"../files/vf_features.pk\", \"rb\"))\n",
    "# featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "# featuredf = featuredf.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = featuredf.index[featuredf[\"feature_Is marsupial\"] == 1].tolist()\n",
    "# print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuredf():\n",
    "    featuredict = pk.load(open(f\"../files/features_gpt41.pk\", \"rb\"))\n",
    "    featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "    featuredf = featuredf.replace({True: 1, False: 0, 'True': 1, 'True.': 1, 'TRUE': 1, 'true': 1, 'False': 0, 'False.': 0, 'false': 0})\n",
    "    featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n",
    "    # featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n",
    "    # featuredf = featuredf[featuredf.apply(lambda row: row.map(lambda x: isinstance(x, int)).all(), axis=1)]\n",
    "    return featuredf, featuredf.columns.tolist()\n",
    "\n",
    "vf_featuredf, vf_featurecols = get_featuredf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    correlation_matrix = df.corr()\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape), k=1)  # Upper triangle mask\n",
    "    corr = correlation_matrix.where(mask == 0)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(corr, annot=False, cmap='RdBu', fmt=\".1f\", mask=np.triu(np.ones_like(corr, dtype=bool)), vmin=-1, vmax=1)\n",
    "    plt.show()\n",
    "    return correlation_matrix\n",
    "\n",
    "vf_featuredf_corr = correlation_matrix(vf_featuredf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of correlated features (greater than 0.5 corr)\n",
    "print(np.mean(vf_featuredf_corr > 0.5))\n",
    "\n",
    "# % of correlated features (less than -0.5 corr)\n",
    "print(np.mean(vf_featuredf_corr < -0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_columns(corr, threshold=0.75):\n",
    "    # List to store highly correlated columns\n",
    "    highly_correlated = {}\n",
    "    \n",
    "    # can change the logic to only keep correlated columns that as least correlated with others\n",
    "    # if (np.sum(vf_featuredf_corr.loc[\"feature_Is mammal\"].values) - 1)/(len(vf_featuredf_corr.loc[\"feature_Is mammal\"].values) - 1)\n",
    "\n",
    "    for i, col in enumerate(corr.columns):\n",
    "        for prev_col in corr.columns[:i]:  # Check only previous columns\n",
    "            if abs(corr.loc[col, prev_col]) >= threshold:  # Check correlation\n",
    "                highly_correlated[col] = prev_col\n",
    "                print(col, prev_col, corr.loc[col, prev_col])\n",
    "                break\n",
    "    \n",
    "    return highly_correlated\n",
    "\n",
    "high_corr_columns_vf = get_highly_correlated_columns(vf_featuredf_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns_vf = list(high_corr_columns_vf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_responsedf(df):\n",
    "    featuredict = vf_featuredf.to_dict(orient='index')\n",
    "    mapped_features = df['response'].map(featuredict)\n",
    "    mapped_features = mapped_features.apply(lambda x: x if isinstance(x, dict) else {})\n",
    "    fc = pd.DataFrame(mapped_features.tolist())\n",
    "    df = pd.concat([df, fc], axis=1)\n",
    "    df = df.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})\n",
    "    dropped_rows = df[df[vf_featurecols].isna().any(axis=1)]\n",
    "    df = df.dropna(subset=vf_featurecols)\n",
    "    for col in vf_featurecols:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df, dropped_rows\n",
    "\n",
    "data2, dropped_rows2 = add_features_to_responsedf(data2)\n",
    "# data3, dropped_rows3 = add_features_to_responsedf(data3)\n",
    "# data4, dropped_rows4 = add_features_to_responsedf(data4)\n",
    "# data5, dropped_rows5 = add_features_to_responsedf(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in data2.columns if col.startswith('feature_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.abs(vf_featuredf[feature_cols].mean() - data2[feature_cols].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_activity_diff1 = []\n",
    "num_simulations = 100\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "unique_responses = data2[\"response\"].unique()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff1.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff1, alpha=0.5, label=\"Uniform-weighted\");\n",
    "\n",
    "\n",
    "# with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "#     freq_abs = json.load(f)  # dict: response → log_freq\n",
    "# freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "# with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "#     corrections = json.load(f)\n",
    "# probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "# # probs = np.exp(probs)          # convert from log frequencies\n",
    "# probs /= probs.sum()           # normalize to sum to 1\n",
    "# sum_activity_diff = []\n",
    "# for i in range(num_simulations):\n",
    "#     data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "#     for pid, count in pid_to_count.items():\n",
    "#         sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "#         temp_df = pd.DataFrame({\n",
    "#             \"pid\": [pid] * count,\n",
    "#             \"response\": sampled_responses\n",
    "#         })\n",
    "#         data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "#     data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "#     sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "#     sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "# plt.hist(sum_activity_diff, alpha=0.5, label=\"LogFreq-weighted\");\n",
    "\n",
    "\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)  # dict: response → log_freq\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff2 = []\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff2.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff2, alpha=0.5, label=\"Freq-weighted\")\n",
    "plt.legend();\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sum_activity_diff1, alpha=0.5, label='Uniform Sampling')\n",
    "plt.hist(sum_activity_diff2, alpha=0.5, label='Frequency-based sampling')\n",
    "vline_value = np.sum(np.abs(vf_featuredf[feature_cols].mean() - data2[feature_cols].mean()))\n",
    "plt.axvline(vline_value, color='red', linestyle='--', linewidth=2, label=f'Data = {vline_value:.2f}')\n",
    "\n",
    "plt.xlabel(\"Total abs. feature activity diff.\")\n",
    "plt.ylabel(\"Num. simulations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example histograms\n",
    "plt.style.use('seaborn-v0_8-muted')\n",
    "\n",
    "# Create 3 subplots (one for each region)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(5,3),\n",
    "                                    gridspec_kw={'width_ratios': [0.5,0.5,0.3]})\n",
    "\n",
    "# 1️⃣ First histogram region (Uniform)\n",
    "ax1.hist(sum_activity_diff1, alpha=0.5, label='Uniform sampling', color='lightcoral')\n",
    "ax1.set_xlim(1.5, 2.25)\n",
    "# ax1.set_xlabel(\"Total abs. feature activity diff.\")\n",
    "ax1.set_ylabel(\"Num. simulations\")\n",
    "\n",
    "# 2️⃣ Second histogram region (Frequency-based)\n",
    "ax2.hist(sum_activity_diff2, alpha=0.7, label='Frequency-based sampling', color='indianred')\n",
    "ax2.set_xlim(4.5, 5.5)\n",
    "ax2.set_xlabel(\"Total abs. feature activity diff.\")\n",
    "\n",
    "# 3️⃣ Observed data vertical line\n",
    "vline_value = np.sum(np.abs(vf_featuredf[feature_cols].mean() - data2[feature_cols].mean()))\n",
    "ax3.axvline(vline_value, color='red', linestyle='--', linewidth=2, label=f'Data = {vline_value:.2f}')\n",
    "ax3.set_xlim(vline_value - 0.5, vline_value + 0.5)\n",
    "# ax3.set_xlabel(\"Total abs. feature activity diff.\")\n",
    "\n",
    "# Hide the spines for breaks\n",
    "for a, b in zip([ax1, ax2], [ax2, ax3]):\n",
    "    a.spines['right'].set_visible(False)\n",
    "    b.spines['left'].set_visible(False)\n",
    "\n",
    "# Add diagonal lines for breaks\n",
    "d = 0.015  # size of break slant\n",
    "# def add_break(ax_left, ax_right):\n",
    "#     kwargs = dict(color='k', clip_on=False)\n",
    "#     ax_left.plot((1 - d, 1 + d), (-d, +d), transform=ax_left.transAxes, **kwargs)\n",
    "#     ax_left.plot((1 - d, 1 + d), (1 - d, 1 + d), transform=ax_left.transAxes, **kwargs)\n",
    "#     ax_right.plot((-d, +d), (-d, +d), transform=ax_right.transAxes, **kwargs)\n",
    "#     ax_right.plot((-d, +d), (1 - d, 1 + d), transform=ax_right.transAxes, **kwargs)\n",
    "def add_break(ax_left, ax_right):\n",
    "    kwargs = dict(color='k', clip_on=False)\n",
    "    ax_left.plot((1 - d, 1 + d), (-d, +d), transform=ax_left.transAxes, **kwargs)\n",
    "    ax_right.plot((-d, +d), (-d, +d), transform=ax_right.transAxes, **kwargs)\n",
    "\n",
    "add_break(ax1, ax2)\n",
    "add_break(ax2, ax3)\n",
    "\n",
    "# Shared legend\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.58, 1.09), ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_activity_diff = []\n",
    "num_simulations = 50\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    unique_responses = data2[\"response\"].unique()\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Uniform-weighted\");\n",
    "\n",
    "\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)  # dict: response → log_freq\n",
    "unique_responses = data2[\"response\"].unique()\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "# probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff = []\n",
    "num_simulations = 50\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    unique_responses = data2[\"response\"].unique()\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"LogFreq-weighted\");\n",
    "\n",
    "\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)  # dict: response → log_freq\n",
    "unique_responses = data2[\"response\"].unique()\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff = []\n",
    "num_simulations = 50\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    unique_responses = data2[\"response\"].unique()\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Freq-weighted\")\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_activity_diff = []\n",
    "num_simulations = 50\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "unique_responses = data2[\"response\"].unique()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Uniform-weighted\");\n",
    "\n",
    "\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)  # dict: response → log_freq\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff = []\n",
    "num_simulations = 50\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Freq-weighted\")\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 100\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "unique_responses = data2[\"response\"].unique()\n",
    "plt.figure(figsize=(5.2, 5.2))\n",
    "\n",
    "sum_activity_diff = []\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Uniform-weighted\", color=\"cornflowerblue\");\n",
    "\n",
    "\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)  # dict: response → log_freq\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "# probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff = []\n",
    "pid_to_count = data2.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    unique_responses = data2[\"response\"].unique()\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"LogFreq-weighted\", color=\"mediumorchid\");\n",
    "\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff = []\n",
    "for i in range(num_simulations):\n",
    "    data2_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data2_simulatedrandom = pd.concat([data2_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data2_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff.append(sum_abs_mean_activity_diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Freq-weighted\", color=\"palevioletred\")\n",
    "\n",
    "\n",
    "sum_activity_diff = []\n",
    "response_probs = data2[\"response\"].value_counts(normalize=True).to_dict()\n",
    "probs = np.array(list(response_probs.values()))\n",
    "for _ in range(0):\n",
    "    simulated = []\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        simulated.append(pd.DataFrame({\"pid\": [pid]*count, \"response\": sampled}))\n",
    "    data2_simulatedrandom = pd.concat(simulated, ignore_index=True)\n",
    "    data2_simulatedrandom, _ = add_features_to_responsedf(data2_simulatedrandom)\n",
    "    diff = np.abs(data2[feature_cols].mean() - data2_simulatedrandom[feature_cols].mean()).sum()\n",
    "    sum_activity_diff.append(diff)\n",
    "plt.hist(sum_activity_diff, alpha=0.5, label=\"Proportion-weighted\", color=\"mediumaquamarine\")\n",
    "\n",
    "plt.xlabel(\"Total abs. activity diff. across features\")\n",
    "plt.ylabel(\"Num. simulations\")\n",
    "# plt.xlim(right=2)\n",
    "plt.xlim(7, 9.2)\n",
    "plt.ylim(0, 25)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in feature_cols:\n",
    "    new_col = f\"consecutive_{col}_bin\"\n",
    "    data2[new_col] = (\n",
    "        data2.groupby('pid')[col]\n",
    "        .transform(lambda x: (x != x.shift()).astype(int))\n",
    "    )\n",
    "    data2.loc[data2.groupby('pid').head(1).index, new_col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame()\n",
    "\n",
    "for col in feature_cols:\n",
    "    bin_col = f\"consecutive_{col}_bin\"\n",
    "    temp = data2.dropna(subset=[bin_col])\n",
    "    \n",
    "    # Compute RT difference for each pid\n",
    "    rt_diff = temp.groupby('pid').apply(\n",
    "        lambda g: g.loc[g[bin_col]==0, 'RT'].mean() - g.loc[g[bin_col]==1, 'RT'].mean()\n",
    "    )\n",
    "    \n",
    "    diff_df[col] = rt_diff\n",
    "\n",
    "# Optional: reset index if you want 'pid' as a column\n",
    "diff_df = diff_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diff_df[\"feature_Is mammal\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diff_df[\"feature_Is marsupial\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = data2[[col for col in data2.columns if col.startswith(\"feature_\")]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns_vf.extend([col for col in vf_featurecols if data2[col].mean() > 0.95 or data2[col].mean() < 0.05 and col != 'feature_Is insect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf = vf_featuredf.drop(columns = remove_columns_vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredict = vf_featuredf.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(featuredict, open(f\"../files/vf_features_updated.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means(df, featuredf, featurecols):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6,2))\n",
    "    ax[0].hist(featuredf.mean(axis=0).values);\n",
    "    ax[0].set_xlabel(\"P(feature) = 1\")\n",
    "    ax[0].set_ylabel(\"Number of features\")\n",
    "    ax[1].hist(df[featurecols].mean(axis=0).values);\n",
    "    ax[1].set_xlabel(\"P(feature) = 1 in responses\")\n",
    "\n",
    "plot_means(data_vf, vf_featuredf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_ones(df, feature_col):\n",
    "    # result = {}\n",
    "    counts = {}\n",
    "    for subject in df[\"pid\"].unique():\n",
    "        subject_data = df[df[\"pid\"] == subject][feature_col].values\n",
    "        # counts = {}\n",
    "        current_count = 0\n",
    "        for value in subject_data:\n",
    "            if value == 1:\n",
    "                current_count += 1\n",
    "            elif current_count > 0:\n",
    "                counts[current_count] = counts.get(current_count, 0) + 1\n",
    "                current_count = 0\n",
    "        # Add the last streak if it ends with a 1\n",
    "        if current_count > 0:\n",
    "            counts[current_count] = counts.get(current_count, 0) + 1 \n",
    "        # result[subject] = counts\n",
    "    # return result\n",
    "    return counts\n",
    "\n",
    "def make_persistance_plots(df, featuredf, featurecols):\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "        plt.plot(list(fco.keys()), np.array(list(fco.values())) / (np.sum(list(fco.values()))), label = \"Data\")\n",
    "        \n",
    "        x = np.arange(1, list(fco.keys())[-1] + 1)\n",
    "        geometric_pdf = (p ** x) * (1 - p) \n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def make_persistance_plots_conditional(df, featuredf, featurecols):\n",
    "    cols_to_remove = []\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "        print(fco)\n",
    "        x = np.array(list(fco.keys()))\n",
    "        \n",
    "        data_values = np.array(list(fco.values()))\n",
    "        observed = data_values / np.sum(data_values)\n",
    "        plt.plot(x, observed, label = \"Data\")\n",
    "        \n",
    "        geometric_pdf = (p ** (x - 1)) * (1 - p) \n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "\n",
    "        # expected = geometric_pdf * np.sum(data_values)\n",
    "        # expected *= np.sum(data_values) / np.sum(expected)\n",
    "\n",
    "        chi2_stat, p_value = chisquare(data_values, f_exp=geometric_pdf / np.sum(geometric_pdf) * np.sum(data_values))\n",
    "        print(f\"Chi-Square Statistic: {chi2_stat}, p-value: {p_value}\")\n",
    "        if p_value > 0.01:\n",
    "            cols_to_remove.append(col)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return cols_to_remove\n",
    "\n",
    "def make_persistance_plots_hazard(df, featuredf, featurecols):\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "\n",
    "        remaining_population = sum(fco.values())\n",
    "        hazard_function = []\n",
    "\n",
    "        for i, freq_i in fco.items():\n",
    "            hazard_function.append(freq_i / remaining_population)  # Hazard probability for i\n",
    "            remaining_population -= freq_i\n",
    "\n",
    "        plt.plot(list(fco.keys()), hazard_function, label = \"Data\")\n",
    "        \n",
    "        x = np.arange(1, list(fco.keys())[-1] + 1)\n",
    "        geometric_pdf = [(1 - p)] * len(x)\n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_vf.extend(make_persistance_plots_conditional(data_vf, vf_featuredf, vf_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_persistance_plots_hazard(data_vf, vf_featuredf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_autbrick.extend(make_persistance_plots_conditional(data_autbrick, autbrick_featuredf, autbrick_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_persistance_plots_hazard(data_autbrick, autbrick_featuredf, autbrick_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_autpaperclip.extend(make_persistance_plots_conditional(data_autpaperclip, autpaperclip_featuredf, autpaperclip_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_persistance_plots_hazard(data_autpaperclip, autpaperclip_featuredf, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf = vf_featuredf.drop(columns = remove_columns_vf)\n",
    "data_vf = data_vf.drop(columns = remove_columns_vf)\n",
    "vf_featurecols = [item for item in vf_featurecols if item not in remove_columns_vf]\n",
    "\n",
    "# autbrick_featuredf = autbrick_featuredf.drop(columns = remove_columns_autbrick)\n",
    "# # add data line\n",
    "# autbrick_featurecols = [item for item in autbrick_featurecols if item not in remove_columns_autbrick]\n",
    "\n",
    "# autpaperclip_featuredf = autpaperclip_featuredf.drop(columns = remove_columns_autpaperclip)\n",
    "# # add data line\n",
    "# autpaperclip_featurecols = [item for item in autpaperclip_featurecols if item not in remove_columns_autpaperclip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_active = vf_featuredf.mean() * 100\n",
    "top5 = percent_active.sort_values(ascending=False).head(10)\n",
    "bottom5 = percent_active.sort_values().head(10)\n",
    "\n",
    "# Concatenate for plotting\n",
    "combined = pd.concat([top5, bottom5])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 3.5))\n",
    "bars = plt.bar(combined.index, combined.values, color=\"#D8BFD8\")\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage Active\")\n",
    "plt.title(\"Top 5 and Bottom 5 Most Active Features -- UNIFORM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_active2 = data2[vf_featurecols].mean() * 100\n",
    "top5 = percent_active2.sort_values(ascending=False).head(10)\n",
    "bottom5 = percent_active2.sort_values().head(10)\n",
    "\n",
    "# Concatenate for plotting\n",
    "combined = pd.concat([top5, bottom5])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(combined.index, combined.values, color=\"#D8BFD8\")\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage Active (1)\")\n",
    "plt.title(\"Top 5 and Bottom 5 Most Active Features -- RESPONSES\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_diff = percent_active2.subtract(percent_active)\n",
    "\n",
    "percent_diff = percent_active2.subtract(percent_active)\n",
    "top5 = percent_diff.sort_values(ascending=False).head(10)\n",
    "bottom5 = percent_diff.sort_values().head(10)\n",
    "\n",
    "# Concatenate for plotting\n",
    "combined = pd.concat([top5, bottom5])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(combined.index, combined.values, color=\"#D8BFD8\")\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f\"{height:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Percentage Active (1)\")\n",
    "plt.title(\"Top 5 and Bottom 5 Most Active Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vf_featurecols)) #, len(autbrick_featurecols), len(autpaperclip_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = vf_featurecols[:3] + [\"feature_Is reptile\", \"feature_Is amphibian\"] + vf_featurecols[3:]\n",
    "pk.dump(final_features, open(\"../files/vf_final_features.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    num_features_same = [np.nan]  # Initialize with nan for the first row\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same.append(np.nan)\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            # consecutive_1s = ((row1 == 1) & (row2 == 1))\n",
    "            num_features_same.append(consecutive_1s.sum())\n",
    "    \n",
    "    group['num_features_same'] = num_features_same\n",
    "    return group\n",
    "\n",
    "# data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "data2 = get_num_features_same(data2, vf_featurecols)\n",
    "# data3 = get_num_features_same(data3, vf_featurecols)\n",
    "# data4 = get_num_features_same(data4, vf_featurecols)\n",
    "# data5 = get_num_features_same(data5, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = data2[\"RT\"].quantile(0.25)\n",
    "Q3 = data2[\"RT\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for non-outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame\n",
    "data2_no_outliers = data2[(data2[\"RT\"] >= lower_bound) & (data2[\"RT\"] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute log RT\n",
    "# data2_no_outliers[\"RT\"] = data2_no_outliers[\"RT\"] + 0.001\n",
    "# data2_no_outliers[\"logRT\"] = np.log(data2_no_outliers[\"RT\"])\n",
    "\n",
    "# # Bin `num_features_same` into strips of width 5\n",
    "# bin_edges = np.arange(data2_no_outliers[\"num_features_same\"].min() + 10, \n",
    "#                       data2_no_outliers[\"num_features_same\"].max(), \n",
    "#                       10)\n",
    "# data2_no_outliers[\"bin\"] = pd.cut(data2_no_outliers[\"num_features_same\"], bins=bin_edges)\n",
    "\n",
    "# # Compute mean logRT per bin\n",
    "# mean_logRT = data2_no_outliers.groupby(\"bin\")[\"logRT\"].mean()\n",
    "# bin_centers = [interval.mid for interval in mean_logRT.index]\n",
    "\n",
    "# # Overlay mean logRT per strip\n",
    "# plt.plot(bin_centers, mean_logRT.values, color='red', marker='o')\n",
    "\n",
    "# # Labels and formatting\n",
    "# plt.xlabel(\"num_features_same\")\n",
    "# plt.ylabel(\"Mean log(RT)\")\n",
    "# plt.title(\"Scatter of log(RT) vs. num_features_same with binned mean\");\n",
    "\n",
    "# bin_labels = [f\"{int(interval.left)}-{int(interval.right)}\" for interval in mean_logRT.index]\n",
    "# plt.xticks(ticks=bin_centers, labels=bin_labels, rotation=45)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem  # standard error of the mean\n",
    "\n",
    "# Step 1: Compute log RT\n",
    "data2_no_outliers[\"RT\"] = data2_no_outliers[\"RT\"] + 0.001\n",
    "data2_no_outliers[\"logRT\"] = np.log(data2_no_outliers[\"RT\"])\n",
    "\n",
    "# Step 2: Define bins of width 10 (starting from +10 offset)\n",
    "bin_edges = np.arange(\n",
    "    data2_no_outliers[\"num_features_same\"].min() + 10, \n",
    "    data2_no_outliers[\"num_features_same\"].max(), \n",
    "    10\n",
    ")\n",
    "data2_no_outliers[\"bin\"] = pd.cut(data2_no_outliers[\"num_features_same\"], bins=bin_edges)\n",
    "\n",
    "# Step 3: Compute mean + SEM of logRT per bin\n",
    "group_stats = data2_no_outliers.groupby(\"bin\")[\"logRT\"].agg(['mean', sem]).reset_index()\n",
    "\n",
    "# Step 4: Compute bin centers for plotting\n",
    "bin_centers = [interval.mid for interval in group_stats[\"bin\"]]\n",
    "plt.figure(figsize=(4,4))\n",
    "# Step 5: Plot with error bars\n",
    "plt.errorbar(\n",
    "    bin_centers,\n",
    "    group_stats[\"mean\"],\n",
    "    yerr=group_stats[\"sem\"],\n",
    "    fmt='o-',               # line with circle markers\n",
    "    color='red',\n",
    "    ecolor='black',         # color of error bars\n",
    "    elinewidth=1,\n",
    "    capsize=4\n",
    ")\n",
    "\n",
    "# Step 6: Labels and formatting\n",
    "bin_labels = [f\"{int(interval.left)}–{int(interval.right)}\" for interval in group_stats[\"bin\"]]\n",
    "plt.xticks(ticks=bin_centers, labels=bin_labels, rotation=45)\n",
    "plt.xlabel(\"Num. features same\")\n",
    "plt.ylabel(\"Mean log(RT)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2_no_outliers[\"RT\"] = data2_no_outliers[\"RT\"] + 0.001\n",
    "# data2_no_outliers[\"logRT\"] = np.log(data2_no_outliers[\"RT\"])\n",
    "\n",
    "# # Step 2: Quantile binning into 10 equal-sized bins\n",
    "# data2_no_outliers[\"bin\"] = pd.qcut(data2_no_outliers[\"num_features_same\"], q=15)\n",
    "\n",
    "# # Step 3: Group by bin and compute mean logRT\n",
    "# mean_logRT = data2_no_outliers.groupby(\"bin\")[\"logRT\"].mean()\n",
    "\n",
    "# # Step 4: Create integer x positions and corresponding labels\n",
    "# x_positions = np.arange(len(mean_logRT))\n",
    "# bin_labels = [f\"{int(interval.left)}–{int(interval.right)}\" for interval in mean_logRT.index]\n",
    "\n",
    "# # Step 5: Plot\n",
    "# plt.plot(x_positions, mean_logRT.values, color='red', marker='o')\n",
    "\n",
    "# # Step 6: Format x-axis to show bin ranges at equal spacing\n",
    "# plt.xticks(ticks=x_positions, labels=bin_labels, rotation=45)\n",
    "# plt.xlabel(\"num_features_same (quantile bins)\")\n",
    "# plt.ylabel(\"Mean log(RT)\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem  # standard error of the mean\n",
    "\n",
    "# Step 1: Adjust RT and compute logRT\n",
    "data2_no_outliers[\"RT\"] = data2_no_outliers[\"RT\"] + 0.001\n",
    "data2_no_outliers[\"logRT\"] = np.log(data2_no_outliers[\"RT\"])\n",
    "\n",
    "# Step 2: Quantile binning into 15 equal-sized bins\n",
    "data2_no_outliers[\"bin\"] = pd.qcut(data2_no_outliers[\"num_features_same\"], q=15)\n",
    "\n",
    "# Step 3: Group by bin and compute mean + SEM of logRT\n",
    "group_stats = data2_no_outliers.groupby(\"bin\")[\"logRT\"].agg(['mean', sem]).reset_index()\n",
    "\n",
    "# Step 4: Create x positions and bin labels\n",
    "x_positions = np.arange(len(group_stats))\n",
    "bin_labels = [f\"{int(interval.left)}–{int(interval.right)}\" for interval in group_stats[\"bin\"]]\n",
    "\n",
    "# Step 5: Plot with error bars\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.errorbar(\n",
    "    x_positions,\n",
    "    group_stats[\"mean\"],\n",
    "    yerr=group_stats[\"sem\"],\n",
    "    fmt='o-',                     # line + circle markers\n",
    "    color='red',\n",
    "    ecolor='black',               # color of error bars\n",
    "    elinewidth=1,\n",
    "    capsize=4                     # small caps on error bars\n",
    ")\n",
    "\n",
    "# Step 6: Labeling and formatting\n",
    "plt.xticks(ticks=x_positions, labels=bin_labels, rotation=45)\n",
    "plt.xlabel(\"Num. features same\")\n",
    "plt.ylabel(\"Mean log(RT)\")\n",
    "# plt.title(\"Mean log(RT) with Standard Error across Quantile Bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dot_product(df, featurecols):\n",
    "    df['dot_product'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_dot_product, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_dot_product(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    dot_product = [np.nan, np.nan]  # Initialize with nan for the first row\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i - 1, featurecols]\n",
    "        row3 = group.loc[i, featurecols]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            dot_product.append(np.nan)\n",
    "        else:\n",
    "            vec1 = (row1 == row2).astype(int)\n",
    "            vec2 = (row2 == row3).astype(int)\n",
    "            dot_product.append(np.dot(vec1, vec2))\n",
    "    \n",
    "    group['dot_product'] = dot_product  \n",
    "    return group\n",
    "\n",
    "# data_vf = get_dot_product(data_vf, vf_featurecols)\n",
    "data2 = get_dot_product(data2, vf_featurecols)\n",
    "# data3 = get_dot_product(data3, vf_featurecols)\n",
    "# data4 = get_dot_product(data4, vf_featurecols)\n",
    "# data5 = get_dot_product(data5, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dot_product(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    n = len(group)\n",
    "    \n",
    "    if n < 3:\n",
    "        group['dot_product'] = [np.nan] * n\n",
    "        return group\n",
    "    \n",
    "    dot_product = [np.nan, np.nan]  # For first two rows\n",
    "\n",
    "    for i in range(2, n):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i - 1, featurecols]\n",
    "        row3 = group.loc[i, featurecols]\n",
    "\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            dot_product.append(np.nan)\n",
    "        else:\n",
    "            vec1 = (row1 == row2).astype(int)\n",
    "            vec2 = (row2 == row3).astype(int)\n",
    "            dot_product.append(np.dot(vec1, vec2))\n",
    "\n",
    "    group['dot_product'] = dot_product\n",
    "    return group\n",
    "\n",
    "# data_vf = get_dot_product(data_vf, vf_featurecols)\n",
    "# data2 = get_dot_product(data2, vf_featurecols)\n",
    "data3 = get_dot_product(data3, vf_featurecols)\n",
    "data4 = get_dot_product(data4, vf_featurecols)\n",
    "data5 = get_dot_product(data5, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3[\"num_features_same\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"noconstraints\")\n",
    "plt.hist(data4[\"num_features_same\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"similar\")\n",
    "plt.hist(data5[\"num_features_same\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"divergent\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3[\"dot_product\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"noconstraints\")\n",
    "plt.hist(data4[\"dot_product\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"similar\")\n",
    "plt.hist(data5[\"dot_product\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"divergent\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3[\"RT\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"noconstraints\", bins=100)\n",
    "plt.hist(data4[\"RT\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"similar\", bins=100)\n",
    "plt.hist(data5[\"RT\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"divergent\", bins=100)\n",
    "plt.xlim(0, 20000)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3.groupby(\"pid\").count()[\"rid\"].tolist(), alpha=0.3, label=\"noconstraints\", bins=10)\n",
    "plt.hist(data4.groupby(\"pid\").count()[\"rid\"].tolist(), alpha=0.3, label=\"similar\", bins=10)\n",
    "plt.hist(data5.groupby(\"pid\").count()[\"rid\"].tolist(), alpha=0.3, label=\"divergent\", bins=10)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4[[\"RT\", \"num_features_same\", \"dot_product\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Data 2\n",
    "subset = data2[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data2 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data2 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n",
    "\n",
    "# Data 3\n",
    "subset = data3[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data3 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data3 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n",
    "\n",
    "# Data 4\n",
    "subset = data4[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data4 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data4 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n",
    "\n",
    "# Data 5\n",
    "subset = data5[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data5 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data5 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data2[[\"num_features_same\", \"dot_product\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['num_features_same'].corr(data2['dot_product'], method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"num_features_same\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean num features same\")\n",
    "plt.xlim(85, 110);\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"dot_product\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean dot product\")\n",
    "plt.xlim(65, 95);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle within each pid\n",
    "from tqdm import tqdm\n",
    "means = []\n",
    "for _ in tqdm(range(100)):\n",
    "    shuffled_data2 = data2.groupby(\"pid\", group_keys=False).apply(lambda x: x.sample(frac=1).reset_index(drop=True))\n",
    "    shuffled_data2 = get_num_features_same(shuffled_data2, vf_featurecols)\n",
    "    means.append(np.mean(shuffled_data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values))\n",
    "# shuffled_data2 = get_dot_product(shuffled_data2, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.hist(means, bins=6, label=\"Shuffled\")\n",
    "# plt.axvline(np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values), color='red')\n",
    "plt.ylabel(\"Number of simulations\")\n",
    "plt.xlabel(\"Mean num features same\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brokenaxes import brokenaxes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 3.5))\n",
    "\n",
    "# Example histogram data\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "# means = [98.2, 98.3, 98.4, 98.6, 98.7, 98.5] * 10  # your simulated data\n",
    "\n",
    "# Create a broken x-axis: main region (98.1–98.9) and jump to (105.5–106.5)\n",
    "bax = brokenaxes(xlims=((98.1, 98.9), (np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values) - 0.1, np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values) + 0.1)), hspace=.05)\n",
    "\n",
    "# Plot histogram in the first x-range\n",
    "bax.hist(means, bins=6, color='lightcoral', label=\"Shuffled\")\n",
    "\n",
    "# Add vertical line at 106 in the second x-range\n",
    "bax.axvline(np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values), color='red', linestyle='--', linewidth=2, label=f'  Data =\\n  {str(np.round(np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values),2))}')\n",
    "# bax.text(np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values), 8, f'  Data =\\n  {str(np.round(np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values),2))}', color='red', fontsize=8)\n",
    "\n",
    "# Label axes\n",
    "bax.set_xlabel(\"Mean num features same\", labelpad=25)\n",
    "bax.set_ylabel(\"Number of simulations\", labelpad=25)\n",
    "\n",
    "make plt.xticks([97, 97, 99 // 105 106])\n",
    "show plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brokenaxes import brokenaxes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.figure(figsize=(4, 3.5))\n",
    "\n",
    "# Calculate data mean\n",
    "data_mean = np.mean(data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values)\n",
    "\n",
    "# Create broken x-axis\n",
    "bax = brokenaxes(\n",
    "    xlims=((98, 99), (data_mean - 0.1, data_mean + 0.1)),\n",
    "    hspace=0.05\n",
    ")\n",
    "\n",
    "# Plot histogram\n",
    "bax.hist(means, bins=6, color='lightcoral', label=\"Shuffled\")\n",
    "\n",
    "# Add vertical line for observed data\n",
    "bax.axvline(data_mean, color='red', linestyle='--', linewidth=2, label=f'Data = {data_mean:.2f}')\n",
    "\n",
    "# Axis labels\n",
    "bax.set_xlabel(\"Mean num features same\", labelpad=25)\n",
    "bax.set_ylabel(\"Number of simulations\", labelpad=25)\n",
    "\n",
    "# Custom x-ticks\n",
    "bax.axs[0].set_xticks([98.2, 98.5, 98.8])\n",
    "bax.axs[1].set_xticks([105, 106])  # or [round(data_mean)] if dynamic\n",
    "\n",
    "# Add legend\n",
    "bax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(shuffled_data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"num_features_same\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean num features same\")\n",
    "plt.xlim(85, 110);\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(shuffled_data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"dot_product\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean dot product\")\n",
    "plt.xlim(65, 95);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"axes.facecolor\": \"white\",                  # background stays white\n",
    "    \"axes.edgecolor\": \"black\",                  # keep axis edges\n",
    "    \"patch.facecolor\": \"lightcoral\",\n",
    "    \"text.usetex\": False,                     # render text with LaTeX\n",
    "    \"font.family\": \"sans-serif\",                  # use serif fonts\n",
    "    \"axes.spines.top\": False,                # remove top border\n",
    "    \"axes.spines.right\": False,              # remove right border\n",
    "    \"axes.labelsize\": 16,                    # bigger axis labels\n",
    "    \"xtick.labelsize\": 14,                   # bigger x-tick labels\n",
    "    \"ytick.labelsize\": 14,                   # bigger y-tick labels\n",
    "    \"axes.titlesize\": 18,                    # bigger title\n",
    "    \"figure.dpi\": 120,                       # higher resolution\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.hist(data2[[\"pid\", \"num_features_same\", \"dot_product\"]][\"num_features_same\"].tolist(), alpha=0.5, color=\"mediumaquamarine\", label=\"Original data\")\n",
    "plt.ylabel(\"Num. response-pairs\")\n",
    "plt.xlabel(\"Num. features same\")\n",
    "\n",
    "plt.hist(shuffled_data2[[\"pid\", \"num_features_same\", \"dot_product\"]][\"num_features_same\"].tolist(), alpha=0.5, color=\"slateblue\", label=\"Shuffled data\")\n",
    "plt.ylabel(\"Num. response-pairs\")\n",
    "plt.xlabel(\"Num. features same\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.hist(data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"num_features_same\"].tolist(), alpha=0.5, color=\"mediumaquamarine\", label=\"Original data\")\n",
    "plt.ylabel(\"Num. participants\")\n",
    "plt.xlabel(\"Mean num. features same\")\n",
    "\n",
    "plt.hist(shuffled_data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"num_features_same\"].tolist(), alpha=0.5, color=\"slateblue\", label=\"Shuffled data\")\n",
    "plt.ylabel(\"Num. participants\")\n",
    "plt.xlabel(\"Mean num. features same\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "# data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "data2 = get_num_features_same(data2, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "# data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "data2 = get_num_features_same(data2, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "# data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "data2 = get_num_features_same(data2, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "# data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "data2 = get_num_features_same(data2, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    np.mean(data2[\"num_features_same_5back\"]),\n",
    "    np.mean(data2[\"num_features_same_4back\"]),\n",
    "    np.mean(data2[\"num_features_same_3back\"]),\n",
    "    np.mean(data2[\"num_features_same_2back\"]),\n",
    "    np.mean(data2[\"num_features_same\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    np.std(data2[\"num_features_same_5back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_5back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_4back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_4back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_3back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_3back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same_2back\"], ddof=1) / np.sqrt(len(data2[\"num_features_same_2back\"].dropna())),\n",
    "    np.std(data2[\"num_features_same\"], ddof=1) / np.sqrt(len(data2[\"num_features_same\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [-5, -4, -3, -2, -1]\n",
    "\n",
    "# Plot bar chart with error bars\n",
    "plt.figure(figsize=(4.55, 4.55))\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel(\"Pos. preceeding most recent resp.\")\n",
    "plt.ylabel(\"Mean num. features same\")\n",
    "plt.ylim(95, 107)\n",
    "plt.xticks(x_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_1back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_1back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_0'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group)):\n",
    "        row1 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = 0\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_0'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_1ahead'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group) - 1):\n",
    "        row1 = group.loc[i + 1, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_1ahead'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_2ahead'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group) - 2):\n",
    "        row1 = group.loc[i + 2, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_2ahead'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    np.mean(data_vf[\"RT_diff_5back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_4back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_3back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_2back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_1back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_0\"]),\n",
    "    np.mean(data_vf[\"RT_diff_1ahead\"]),\n",
    "    np.mean(data_vf[\"RT_diff_2ahead\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    np.std(data_vf[\"RT_diff_5back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_5back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_4back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_4back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_3back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_3back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_2back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_2back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_1back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_1back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_0\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_0\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_1ahead\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_1ahead\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_2ahead\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_2ahead\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [-5, -4, -3, -2, -1, 0, 1, 2]\n",
    "\n",
    "# Plot bar chart with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.7, color='mediumpurple')\n",
    "plt.xlabel(\"Back Steps\")\n",
    "plt.ylabel(\"Mean RT\")\n",
    "plt.title(\"Mean RT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_chunks(lst):\n",
    "    count = 0  # Count of contiguous chunks of ones with length > 1\n",
    "    i = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == 1:\n",
    "            start = i\n",
    "            while i < len(lst) and lst[i] == 1:\n",
    "                i += 1\n",
    "            if i - start > 1:\n",
    "                count += 1\n",
    "                if count >= 2:\n",
    "                    return 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return 0\n",
    "\n",
    "# Apply function to each feature_* column, grouped by 'pid'\n",
    "returned_to_same_feature = {\n",
    "    col: data_vf.groupby(\"pid\")[col].apply(has_two_chunks) for col in vf_featurecols\n",
    "}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "returned_to_same_feature_df = pd.DataFrame(returned_to_same_feature)\n",
    "returned = (np.sum(returned_to_same_feature_df, axis=0)/len(data_vf[\"pid\"].unique())).to_dict()\n",
    "returned = dict(sorted(returned.items(), key=lambda item: item[1]))\n",
    "plt.figure(figsize=(3,14))\n",
    "plt.barh(list(returned.keys()), list(returned.values()))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(data_vf[\"num_features_same\"], alpha=0.3, label=\"vf\", color=\"mediumpurple\");\n",
    "# plt.hist(data_autbrick[\"num_features_same\"], alpha=0.3, label=\"autbrick\");\n",
    "# plt.hist(data_autpaperclip[\"num_features_same\"], alpha=0.3, label=\"autpaperclip\");\n",
    "# plt.legend();\n",
    "plt.xlabel(\"Number of features same\")\n",
    "plt.ylabel(\"Num responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf = data_vf[(data_vf[\"order\"] > 0) & (data_vf[\"order\"] < 20)]\n",
    "# data_autbrick = data_autbrick[(data_autbrick[\"order\"] > 0) & (data_autbrick[\"order\"] < 20)]\n",
    "# data_autpaperclip = data_autpaperclip[(data_autpaperclip[\"order\"] > 0) & (data_autpaperclip[\"order\"] < 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_humans = pd.read_csv(\"../csvs/data_humans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf = pd.merge(data_vf, data_humans[data_humans[\"task\"] == 1].drop(\"Unnamed: 0\", axis=1), on=['pid', 'task', 'starttime', 'endtime', 'original_response_Dutch',\n",
    "       'original_response', 'original_response_cleaned', 'response', 'invalid', 'response_len', 'response_num_words', 'order', 'RT'], how='left')\n",
    "data_vf = data_vf[~data_vf[\"response\"].isin(vf_to_remove)]\n",
    "\n",
    "# data_autbrick = pd.merge(data_autbrick, data_humans[data_humans[\"task\"] == 2].drop(\"Unnamed: 0\", axis=1), on=['pid', 'task', 'starttime', 'endtime', 'original_response_Dutch',\n",
    "#        'original_response', 'original_response_cleaned', 'response', 'invalid', 'response_len', 'response_num_words', 'previous_original_response', 'previous_response', 'order', 'RT'], how='left')\n",
    "# data_autpaperclip = pd.merge(data_autpaperclip, data_humans[data_humans[\"task\"] == 3].drop(\"Unnamed: 0\", axis=1), on=['pid', 'task', 'starttime', 'endtime', 'original_response_Dutch',\n",
    "#        'original_response', 'original_response_cleaned', 'response', 'invalid', 'response_len', 'response_num_words', 'previous_original_response', 'previous_response', 'order', 'RT'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "valid_indices = ~np.isnan(data_vf[\"SS\"]) & ~np.isnan(data_vf[\"num_features_same\"])\n",
    "filtered_SS = data_vf[\"SS\"][valid_indices]\n",
    "filtered_num_features_same = data_vf[\"num_features_same\"][valid_indices]\n",
    "plt.scatter(filtered_SS, filtered_num_features_same, alpha=0.3, c=\"mediumpurple\")\n",
    "print(np.corrcoef(filtered_SS, filtered_num_features_same)[0,1])\n",
    "plt.xlabel(\"GTE Large SS\")\n",
    "plt.ylabel(\"Number of features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(data_autbrick[\"SS\"], data_autbrick[\"num_features_same\"])\n",
    "# plt.xlabel(\"SS\")\n",
    "# plt.ylabel(\"Number of features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(data_autpaperclip[\"SS\"], data_autpaperclip[\"num_features_same\"])\n",
    "# plt.xlabel(\"SS\")\n",
    "# plt.ylabel(\"Number of features same\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf['previous_response'] = data_vf.groupby('pid')['response'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least similar\n",
    "# print(data_vf[[\"response\", \"previous_response\", \"num_features_same\", \"SS\", \"jump\"]][data_vf[\"num_features_same\"] < 10][\"SS\"].mean())\n",
    "data_vf[[\"pid\", \"response\", \"previous_response\", \"num_features_same\"]][data_vf[\"num_features_same\"] < 44].drop_duplicates().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most similar\n",
    "# print(data_vf[[\"response\", \"previous_response\", \"num_features_same\", \"SS\", \"jump\"]][data_vf[\"num_features_same\"] > 80][\"SS\"].mean())\n",
    "data_vf[[\"response\", \"previous_response\", \"num_features_same\"]][data_vf[\"num_features_same\"] > 78].drop_duplicates().sort_values(by=[\"num_features_same\"], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Assume your dataframe is called df\n",
    "names = vf_featuredf.iloc[:, 0]            # First column = names\n",
    "features = vf_featuredf.iloc[:, 1:]        # Rest = binary features\n",
    "\n",
    "# Step 2: t-SNE with Hamming distance (good for binary vectors)\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=42, metric='hamming')\n",
    "embedding = tsne.fit_transform(features)\n",
    "\n",
    "# Step 3: Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=50, alpha=0.5, c=\"mediumpurple\")\n",
    "\n",
    "# Optionally annotate points with names\n",
    "for i, name in enumerate(names):\n",
    "    if i % 4 == 0:\n",
    "        plt.text(embedding[i, 0] + 0.005, embedding[i, 1] + 0.005, str(name), fontsize=10)\n",
    "\n",
    "plt.title(\"t-SNE of Binary Vectors\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf = vf_featuredf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(list(pd.merge(data_vf[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_vf[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"]))\n",
    "arr = temp[np.argsort(temp)][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data_vf[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index()[\"jump_profile\"].values, data_vf[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean()[\"num_features_same\"].values)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(np.arange(219), pd.merge(data_vf[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_vf[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"], c=\"mediumpurple\")\n",
    "print(np.corrcoef(np.arange(218), arr)[0,1])\n",
    "plt.xlabel(\"Max jump profile\")\n",
    "plt.ylabel(\"Mean features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# t = pd.merge(data_autbrick[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_autbrick[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"]\n",
    "# plt.scatter(np.arange(len(t)), t)\n",
    "# plt.xlabel(\"Max jump profile\")\n",
    "# plt.ylabel(\"Mean features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# t = pd.merge(data_autpaperclip[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_autpaperclip[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"]\n",
    "# plt.scatter(np.arange(len(t)), t)\n",
    "# plt.xlabel(\"Max jump profile\")\n",
    "# plt.ylabel(\"Mean features same\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_features_heatmap(df, featurecols):\n",
    "    pids = df[\"pid\"].unique()\n",
    "    cnt = 1\n",
    "    for pid in pids:\n",
    "        responses = df[df[\"pid\"] == pid][\"response\"].values\n",
    "        pid_data = df[df[\"pid\"] == pid][featurecols].reset_index(drop=True)\n",
    "        \n",
    "        plt.figure(figsize=(25, len(pid_data) * 0.25))\n",
    "        sns.heatmap(\n",
    "            pid_data,\n",
    "            cmap=sns.color_palette([\"white\", \"mediumpurple\"]),\n",
    "            cbar=False,\n",
    "            linewidths=0.5,\n",
    "            linecolor='black'\n",
    "        )\n",
    "        plt.title(f\"Binary Features Heatmap for PID {pid}\")\n",
    "        plt.gca().xaxis.tick_top()\n",
    "        plt.xticks(ticks=np.arange(len(featurecols)) + 0.5, labels=featurecols, rotation=90)\n",
    "        plt.yticks(ticks=np.arange(len(responses)) + 0.5, labels=responses, rotation=0)\n",
    "        plt.show()\n",
    "        \n",
    "        if cnt == 10:\n",
    "            break\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data_vf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data_autbrick, autbrick_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few vs many dimensions\n",
    "# How persistant are different dimensions? Compared to random\n",
    "# Transitions between different dimensions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

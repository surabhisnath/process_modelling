{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "keys = json.load(open(\"keys.json\"))\n",
    "os.environ[\"OPENAI_API_KEY\"]=keys[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "import together\n",
    "os.environ[\"TOGETHER_API_KEY\"]=keys[\"TOGETHER_API_KEY\"]\n",
    "together.api_key = os.environ.get(\"TOGETHER_API_KEY\")\n",
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"../csvs/hills.csv\")\n",
    "data3 = pd.read_csv(\"../csvs/noconstraints.csv\")\n",
    "data4 = pd.read_csv(\"../csvs/similar.csv\")\n",
    "data5 = pd.read_csv(\"../csvs/divergent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredict = pk.load(open(f\"../files/vf_features.pk\", \"rb\"))\n",
    "featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "featuredf = featuredf.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredf = featuredf[~featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuredf():\n",
    "    featuredict = pk.load(open(f\"../files/vf_features.pk\", \"rb\"))\n",
    "    featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "    featuredf = featuredf.replace({'True': 1, 'True.': 1, 'TRUE': 1, 'False': 0, 'False.': 0})\n",
    "    featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n",
    "    featuredf = featuredf[featuredf.apply(lambda row: row.map(lambda x: isinstance(x, int)).all(), axis=1)]\n",
    "    \n",
    "    # featuredf[\"feature_Is reptile\"] = (featuredf[\"feature_Is reptile\"] | featuredf[\"feature_Is amphibian\"]).astype(int)\n",
    "    # featuredf.rename(columns={\"feature_Is reptile\": \"feature_Is reptile or amphibian\"}, inplace=True)\n",
    "    # featuredf.drop(columns=[\"feature_Is amphibian\"], inplace=True)\n",
    "    print(len(featuredict), len(featuredf))\n",
    "    # assert len(featuredict) == len(featuredf)\n",
    "    return featuredf, featuredf.columns.tolist()\n",
    "\n",
    "vf_featuredf, vf_featurecols = get_featuredf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "    correlation_matrix = df.corr()\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape), k=1)  # Upper triangle mask\n",
    "    corr = correlation_matrix.where(mask == 0)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(corr, annot=False, cmap='RdBu', fmt=\".1f\", mask=np.triu(np.ones_like(corr, dtype=bool)), vmin=-1, vmax=1)\n",
    "    plt.show()\n",
    "    return correlation_matrix\n",
    "\n",
    "vf_featuredf_corr = correlation_matrix(vf_featuredf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of correlated features (greater than 0.5 corr)\n",
    "print(np.mean(vf_featuredf_corr > 0.5))\n",
    "\n",
    "# % of correlated features (less than -0.5 corr)\n",
    "print(np.mean(vf_featuredf_corr < -0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_columns(corr, threshold=0.75):\n",
    "    # List to store highly correlated columns\n",
    "    highly_correlated = {}\n",
    "    \n",
    "    # can change the logic to only keep correlated columns that as least correlated with others\n",
    "    # if (np.sum(vf_featuredf_corr.loc[\"feature_Is mammal\"].values) - 1)/(len(vf_featuredf_corr.loc[\"feature_Is mammal\"].values) - 1)\n",
    "\n",
    "    for i, col in enumerate(corr.columns):\n",
    "        for prev_col in corr.columns[:i]:  # Check only previous columns\n",
    "            if abs(corr.loc[col, prev_col]) >= threshold:  # Check correlation\n",
    "                highly_correlated[col] = prev_col\n",
    "                print(col, prev_col)\n",
    "                break\n",
    "    \n",
    "    return highly_correlated\n",
    "\n",
    "high_corr_columns_vf = get_highly_correlated_columns(vf_featuredf_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns_vf = list(high_corr_columns_vf.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_responsedf(df):\n",
    "    featuredict = vf_featuredf.to_dict(orient='index')\n",
    "    mapped_features = df['response'].map(featuredict)\n",
    "    mapped_features = mapped_features.apply(lambda x: x if isinstance(x, dict) else {})\n",
    "    fc = pd.DataFrame(mapped_features.tolist())\n",
    "    df = pd.concat([df, fc], axis=1)\n",
    "    df = df.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})\n",
    "    dropped_rows = df[df[vf_featurecols].isna().any(axis=1)]\n",
    "    df = df.dropna(subset=vf_featurecols)\n",
    "    for col in vf_featurecols:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df, dropped_rows\n",
    "\n",
    "data2, dropped_rows2 = add_features_to_responsedf(data2)\n",
    "data3, dropped_rows3 = add_features_to_responsedf(data3)\n",
    "data4, dropped_rows4 = add_features_to_responsedf(data4)\n",
    "data5, dropped_rows5 = add_features_to_responsedf(data5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = data2[[col for col in data2.columns if col.startswith(\"feature_\")]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns_vf.extend([col for col in vf_featurecols if data2[col].mean() > 0.95 or data2[col].mean() < 0.05 and col != 'feature_Is insect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf = vf_featuredf.drop(columns = remove_columns_vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredict = vf_featuredf.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk.dump(featuredict, open(f\"../files/vf_features_updated.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_means(df, featuredf, featurecols):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6,2))\n",
    "    ax[0].hist(featuredf.mean(axis=0).values);\n",
    "    ax[0].set_xlabel(\"P(feature) = 1\")\n",
    "    ax[0].set_ylabel(\"Number of features\")\n",
    "    ax[1].hist(df[featurecols].mean(axis=0).values);\n",
    "    ax[1].set_xlabel(\"P(feature) = 1 in responses\")\n",
    "\n",
    "plot_means(data_vf, vf_featuredf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_consecutive_ones(df, feature_col):\n",
    "    # result = {}\n",
    "    counts = {}\n",
    "    for subject in df[\"pid\"].unique():\n",
    "        subject_data = df[df[\"pid\"] == subject][feature_col].values\n",
    "        # counts = {}\n",
    "        current_count = 0\n",
    "        for value in subject_data:\n",
    "            if value == 1:\n",
    "                current_count += 1\n",
    "            elif current_count > 0:\n",
    "                counts[current_count] = counts.get(current_count, 0) + 1\n",
    "                current_count = 0\n",
    "        # Add the last streak if it ends with a 1\n",
    "        if current_count > 0:\n",
    "            counts[current_count] = counts.get(current_count, 0) + 1 \n",
    "        # result[subject] = counts\n",
    "    # return result\n",
    "    return counts\n",
    "\n",
    "def make_persistance_plots(df, featuredf, featurecols):\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "        plt.plot(list(fco.keys()), np.array(list(fco.values())) / (np.sum(list(fco.values()))), label = \"Data\")\n",
    "        \n",
    "        x = np.arange(1, list(fco.keys())[-1] + 1)\n",
    "        geometric_pdf = (p ** x) * (1 - p) \n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "def make_persistance_plots_conditional(df, featuredf, featurecols):\n",
    "    cols_to_remove = []\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "        print(fco)\n",
    "        x = np.array(list(fco.keys()))\n",
    "        \n",
    "        data_values = np.array(list(fco.values()))\n",
    "        observed = data_values / np.sum(data_values)\n",
    "        plt.plot(x, observed, label = \"Data\")\n",
    "        \n",
    "        geometric_pdf = (p ** (x - 1)) * (1 - p) \n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "\n",
    "        # expected = geometric_pdf * np.sum(data_values)\n",
    "        # expected *= np.sum(data_values) / np.sum(expected)\n",
    "\n",
    "        chi2_stat, p_value = chisquare(data_values, f_exp=geometric_pdf / np.sum(geometric_pdf) * np.sum(data_values))\n",
    "        print(f\"Chi-Square Statistic: {chi2_stat}, p-value: {p_value}\")\n",
    "        if p_value > 0.01:\n",
    "            cols_to_remove.append(col)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return cols_to_remove\n",
    "\n",
    "def make_persistance_plots_hazard(df, featuredf, featurecols):\n",
    "    for col in featurecols:\n",
    "        plt.figure(figsize = (2,2))\n",
    "        p = df[featurecols].mean(axis=0).loc[col]   # response df\n",
    "        p_ = featuredf.mean(axis=0).loc[col]        # feature df\n",
    "        \n",
    "        print(col, p, p_)\n",
    "\n",
    "        fco = dict(sorted(find_consecutive_ones(df, col).items()))\n",
    "\n",
    "        remaining_population = sum(fco.values())\n",
    "        hazard_function = []\n",
    "\n",
    "        for i, freq_i in fco.items():\n",
    "            hazard_function.append(freq_i / remaining_population)  # Hazard probability for i\n",
    "            remaining_population -= freq_i\n",
    "\n",
    "        plt.plot(list(fco.keys()), hazard_function, label = \"Data\")\n",
    "        \n",
    "        x = np.arange(1, list(fco.keys())[-1] + 1)\n",
    "        geometric_pdf = [(1 - p)] * len(x)\n",
    "        plt.plot(x, geometric_pdf, label = \"Random\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_vf.extend(make_persistance_plots_conditional(data_vf, vf_featuredf, vf_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_persistance_plots_hazard(data_vf, vf_featuredf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_autbrick.extend(make_persistance_plots_conditional(data_autbrick, autbrick_featuredf, autbrick_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_persistance_plots_hazard(data_autbrick, autbrick_featuredf, autbrick_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns_autpaperclip.extend(make_persistance_plots_conditional(data_autpaperclip, autpaperclip_featuredf, autpaperclip_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_persistance_plots_hazard(data_autpaperclip, autpaperclip_featuredf, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf = vf_featuredf.drop(columns = remove_columns_vf)\n",
    "data_vf = data_vf.drop(columns = remove_columns_vf)\n",
    "vf_featurecols = [item for item in vf_featurecols if item not in remove_columns_vf]\n",
    "\n",
    "# autbrick_featuredf = autbrick_featuredf.drop(columns = remove_columns_autbrick)\n",
    "# # add data line\n",
    "# autbrick_featurecols = [item for item in autbrick_featurecols if item not in remove_columns_autbrick]\n",
    "\n",
    "# autpaperclip_featuredf = autpaperclip_featuredf.drop(columns = remove_columns_autpaperclip)\n",
    "# # add data line\n",
    "# autpaperclip_featurecols = [item for item in autpaperclip_featurecols if item not in remove_columns_autpaperclip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vf_featurecols)) #, len(autbrick_featurecols), len(autpaperclip_featurecols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = vf_featurecols[:3] + [\"feature_Is reptile\", \"feature_Is amphibian\"] + vf_featurecols[3:]\n",
    "pk.dump(final_features, open(\"../files/vf_final_features.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    num_features_same = [np.nan]  # Initialize with nan for the first row\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same.append(np.nan)\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same.append(consecutive_1s.sum())\n",
    "    \n",
    "    group['num_features_same'] = num_features_same\n",
    "    return group\n",
    "\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "data2 = get_num_features_same(data2, vf_featurecols)\n",
    "data3 = get_num_features_same(data3, vf_featurecols)\n",
    "data4 = get_num_features_same(data4, vf_featurecols)\n",
    "data5 = get_num_features_same(data5, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dot_product(df, featurecols):\n",
    "    df['dot_product'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_dot_product, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_dot_product(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    dot_product = [np.nan, np.nan]  # Initialize with nan for the first row\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i - 1, featurecols]\n",
    "        row3 = group.loc[i, featurecols]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            dot_product.append(np.nan)\n",
    "        else:\n",
    "            vec1 = (row1 == row2).astype(int)\n",
    "            vec2 = (row2 == row3).astype(int)\n",
    "            dot_product.append(np.dot(vec1, vec2))\n",
    "    \n",
    "    group['dot_product'] = dot_product  \n",
    "    return group\n",
    "\n",
    "# data_vf = get_dot_product(data_vf, vf_featurecols)\n",
    "# data2 = get_dot_product(data2, vf_featurecols)\n",
    "# data3 = get_dot_product(data3, vf_featurecols)\n",
    "# data4 = get_dot_product(data4, vf_featurecols)\n",
    "data5 = get_dot_product(data5, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dot_product(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    n = len(group)\n",
    "    \n",
    "    if n < 3:\n",
    "        group['dot_product'] = [np.nan] * n\n",
    "        return group\n",
    "    \n",
    "    dot_product = [np.nan, np.nan]  # For first two rows\n",
    "\n",
    "    for i in range(2, n):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i - 1, featurecols]\n",
    "        row3 = group.loc[i, featurecols]\n",
    "\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            dot_product.append(np.nan)\n",
    "        else:\n",
    "            vec1 = (row1 == row2).astype(int)\n",
    "            vec2 = (row2 == row3).astype(int)\n",
    "            dot_product.append(np.dot(vec1, vec2))\n",
    "\n",
    "    group['dot_product'] = dot_product\n",
    "    return group\n",
    "\n",
    "# data_vf = get_dot_product(data_vf, vf_featurecols)\n",
    "# data2 = get_dot_product(data2, vf_featurecols)\n",
    "data3 = get_dot_product(data3, vf_featurecols)\n",
    "data4 = get_dot_product(data4, vf_featurecols)\n",
    "data5 = get_dot_product(data5, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3[\"num_features_same\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"noconstraints\")\n",
    "plt.hist(data4[\"num_features_same\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"similar\")\n",
    "plt.hist(data5[\"num_features_same\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"divergent\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3[\"dot_product\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"noconstraints\")\n",
    "plt.hist(data4[\"dot_product\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"similar\")\n",
    "plt.hist(data5[\"dot_product\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"divergent\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3[\"RT\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"noconstraints\", bins=100)\n",
    "plt.hist(data4[\"RT\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"similar\", bins=100)\n",
    "plt.hist(data5[\"RT\"].tolist()[:min(len(data3), len(data4), len(data5))], alpha=0.3, label=\"divergent\", bins=100)\n",
    "plt.xlim(0, 20000)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.hist(data3.groupby(\"pid\").count()[\"rid\"].tolist(), alpha=0.3, label=\"noconstraints\", bins=10)\n",
    "plt.hist(data4.groupby(\"pid\").count()[\"rid\"].tolist(), alpha=0.3, label=\"similar\", bins=10)\n",
    "plt.hist(data5.groupby(\"pid\").count()[\"rid\"].tolist(), alpha=0.3, label=\"divergent\", bins=10)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Data 2\n",
    "subset = data2[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data2 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data2 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n",
    "\n",
    "# Data 3\n",
    "subset = data3[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data3 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data3 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n",
    "\n",
    "# Data 4\n",
    "subset = data4[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data4 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data4 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n",
    "\n",
    "# Data 5\n",
    "subset = data5[[\"num_features_same\", \"dot_product\", \"RT\"]].dropna()\n",
    "# print(\"data5 - Pearson:\",\n",
    "#       np.corrcoef(subset[\"num_features_same\"], subset[\"RT\"])[0, 1],\n",
    "#       np.corrcoef(subset[\"dot_product\"], subset[\"RT\"])[0, 1])\n",
    "print(\"data5 - Spearman:\",\n",
    "      spearmanr(subset[\"num_features_same\"], subset[\"RT\"]).correlation,\n",
    "      spearmanr(subset[\"dot_product\"], subset[\"RT\"]).correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data2[[\"num_features_same\", \"dot_product\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['num_features_same'].corr(data2['dot_product'], method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"num_features_same\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean num features same\")\n",
    "plt.xlim(85, 110);\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"dot_product\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean dot product\")\n",
    "plt.xlim(65, 95);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle within each pid\n",
    "shuffled_data2 = data2.groupby(\"pid\", group_keys=False).apply(lambda x: x.sample(frac=1).reset_index(drop=True))\n",
    "shuffled_data2 = get_num_features_same(shuffled_data2, vf_featurecols)\n",
    "shuffled_data2 = get_dot_product(shuffled_data2, vf_featurecols)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(shuffled_data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"num_features_same\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean num features same\")\n",
    "plt.xlim(85, 110);\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.hist(shuffled_data2[[\"pid\", \"num_features_same\", \"dot_product\"]].groupby(\"pid\").mean()[\"dot_product\"].tolist())\n",
    "plt.ylabel(\"Number of Ppts\")\n",
    "plt.xlabel(\"Mean dot product\")\n",
    "plt.xlim(65, 95);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    np.mean(data_vf[\"num_features_same_5back\"]),\n",
    "    np.mean(data_vf[\"num_features_same_4back\"]),\n",
    "    np.mean(data_vf[\"num_features_same_3back\"]),\n",
    "    np.mean(data_vf[\"num_features_same_2back\"]),\n",
    "    np.mean(data_vf[\"num_features_same\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    np.std(data_vf[\"num_features_same_5back\"], ddof=1) / np.sqrt(len(data_vf[\"num_features_same_5back\"].dropna())),\n",
    "    np.std(data_vf[\"num_features_same_4back\"], ddof=1) / np.sqrt(len(data_vf[\"num_features_same_4back\"].dropna())),\n",
    "    np.std(data_vf[\"num_features_same_3back\"], ddof=1) / np.sqrt(len(data_vf[\"num_features_same_3back\"].dropna())),\n",
    "    np.std(data_vf[\"num_features_same_2back\"], ddof=1) / np.sqrt(len(data_vf[\"num_features_same_2back\"].dropna())),\n",
    "    np.std(data_vf[\"num_features_same\"], ddof=1) / np.sqrt(len(data_vf[\"num_features_same\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [-5, -4, -3, -2, -1]\n",
    "\n",
    "# Plot bar chart with error bars\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.7, color='mediumpurple')\n",
    "plt.xlabel(\"Back Steps\")\n",
    "plt.ylabel(\"Mean Number of Features Same\")\n",
    "plt.title(\"Mean Number of Features Same\")\n",
    "plt.ylim(50, 65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_1back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_1back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_0'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group)):\n",
    "        row1 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = 0\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_0'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_1ahead'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group) - 1):\n",
    "        row1 = group.loc[i + 1, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_1ahead'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['RT_diff_2ahead'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(0, len(group) - 2):\n",
    "        row1 = group.loc[i + 2, [\"RT\"]]\n",
    "        row2 = group.loc[i, [\"RT\"]]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            RTdiff = row2[\"RT\"] - row1[\"RT\"]\n",
    "            num_features_same[i] = RTdiff\n",
    "    \n",
    "    group['RT_diff_2ahead'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "# Apply the function to your datasets\n",
    "data_vf = get_num_features_same(data_vf, vf_featurecols)\n",
    "# data_autbrick = get_num_features_same(data_autbrick, autbrick_featurecols)\n",
    "# data_autpaperclip = get_num_features_same(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    np.mean(data_vf[\"RT_diff_5back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_4back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_3back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_2back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_1back\"]),\n",
    "    np.mean(data_vf[\"RT_diff_0\"]),\n",
    "    np.mean(data_vf[\"RT_diff_1ahead\"]),\n",
    "    np.mean(data_vf[\"RT_diff_2ahead\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    np.std(data_vf[\"RT_diff_5back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_5back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_4back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_4back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_3back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_3back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_2back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_2back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_1back\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_1back\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_0\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_0\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_1ahead\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_1ahead\"].dropna())),\n",
    "    np.std(data_vf[\"RT_diff_2ahead\"], ddof=1) / np.sqrt(len(data_vf[\"RT_diff_2ahead\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [-5, -4, -3, -2, -1, 0, 1, 2]\n",
    "\n",
    "# Plot bar chart with error bars\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.7, color='mediumpurple')\n",
    "plt.xlabel(\"Back Steps\")\n",
    "plt.ylabel(\"Mean RT\")\n",
    "plt.title(\"Mean RT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_chunks(lst):\n",
    "    count = 0  # Count of contiguous chunks of ones with length > 1\n",
    "    i = 0\n",
    "    while i < len(lst):\n",
    "        if lst[i] == 1:\n",
    "            start = i\n",
    "            while i < len(lst) and lst[i] == 1:\n",
    "                i += 1\n",
    "            if i - start > 1:\n",
    "                count += 1\n",
    "                if count >= 2:\n",
    "                    return 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return 0\n",
    "\n",
    "# Apply function to each feature_* column, grouped by 'pid'\n",
    "returned_to_same_feature = {\n",
    "    col: data_vf.groupby(\"pid\")[col].apply(has_two_chunks) for col in vf_featurecols\n",
    "}\n",
    "\n",
    "# Convert results to DataFrame\n",
    "returned_to_same_feature_df = pd.DataFrame(returned_to_same_feature)\n",
    "returned = (np.sum(returned_to_same_feature_df, axis=0)/len(data_vf[\"pid\"].unique())).to_dict()\n",
    "returned = dict(sorted(returned.items(), key=lambda item: item[1]))\n",
    "plt.figure(figsize=(3,14))\n",
    "plt.barh(list(returned.keys()), list(returned.values()))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.hist(data_vf[\"num_features_same\"], alpha=0.3, label=\"vf\", color=\"mediumpurple\");\n",
    "# plt.hist(data_autbrick[\"num_features_same\"], alpha=0.3, label=\"autbrick\");\n",
    "# plt.hist(data_autpaperclip[\"num_features_same\"], alpha=0.3, label=\"autpaperclip\");\n",
    "# plt.legend();\n",
    "plt.xlabel(\"Number of features same\")\n",
    "plt.ylabel(\"Num responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf = data_vf[(data_vf[\"order\"] > 0) & (data_vf[\"order\"] < 20)]\n",
    "# data_autbrick = data_autbrick[(data_autbrick[\"order\"] > 0) & (data_autbrick[\"order\"] < 20)]\n",
    "# data_autpaperclip = data_autpaperclip[(data_autpaperclip[\"order\"] > 0) & (data_autpaperclip[\"order\"] < 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_humans = pd.read_csv(\"../csvs/data_humans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf = pd.merge(data_vf, data_humans[data_humans[\"task\"] == 1].drop(\"Unnamed: 0\", axis=1), on=['pid', 'task', 'starttime', 'endtime', 'original_response_Dutch',\n",
    "       'original_response', 'original_response_cleaned', 'response', 'invalid', 'response_len', 'response_num_words', 'order', 'RT'], how='left')\n",
    "data_vf = data_vf[~data_vf[\"response\"].isin(vf_to_remove)]\n",
    "\n",
    "# data_autbrick = pd.merge(data_autbrick, data_humans[data_humans[\"task\"] == 2].drop(\"Unnamed: 0\", axis=1), on=['pid', 'task', 'starttime', 'endtime', 'original_response_Dutch',\n",
    "#        'original_response', 'original_response_cleaned', 'response', 'invalid', 'response_len', 'response_num_words', 'previous_original_response', 'previous_response', 'order', 'RT'], how='left')\n",
    "# data_autpaperclip = pd.merge(data_autpaperclip, data_humans[data_humans[\"task\"] == 3].drop(\"Unnamed: 0\", axis=1), on=['pid', 'task', 'starttime', 'endtime', 'original_response_Dutch',\n",
    "#        'original_response', 'original_response_cleaned', 'response', 'invalid', 'response_len', 'response_num_words', 'previous_original_response', 'previous_response', 'order', 'RT'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "valid_indices = ~np.isnan(data_vf[\"SS\"]) & ~np.isnan(data_vf[\"num_features_same\"])\n",
    "filtered_SS = data_vf[\"SS\"][valid_indices]\n",
    "filtered_num_features_same = data_vf[\"num_features_same\"][valid_indices]\n",
    "plt.scatter(filtered_SS, filtered_num_features_same, alpha=0.3, c=\"mediumpurple\")\n",
    "print(np.corrcoef(filtered_SS, filtered_num_features_same)[0,1])\n",
    "plt.xlabel(\"GTE Large SS\")\n",
    "plt.ylabel(\"Number of features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(data_autbrick[\"SS\"], data_autbrick[\"num_features_same\"])\n",
    "# plt.xlabel(\"SS\")\n",
    "# plt.ylabel(\"Number of features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(data_autpaperclip[\"SS\"], data_autpaperclip[\"num_features_same\"])\n",
    "# plt.xlabel(\"SS\")\n",
    "# plt.ylabel(\"Number of features same\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf['previous_response'] = data_vf.groupby('pid')['response'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least similar\n",
    "# print(data_vf[[\"response\", \"previous_response\", \"num_features_same\", \"SS\", \"jump\"]][data_vf[\"num_features_same\"] < 10][\"SS\"].mean())\n",
    "data_vf[[\"pid\", \"response\", \"previous_response\", \"num_features_same\"]][data_vf[\"num_features_same\"] < 44].drop_duplicates().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most similar\n",
    "# print(data_vf[[\"response\", \"previous_response\", \"num_features_same\", \"SS\", \"jump\"]][data_vf[\"num_features_same\"] > 80][\"SS\"].mean())\n",
    "data_vf[[\"response\", \"previous_response\", \"num_features_same\"]][data_vf[\"num_features_same\"] > 78].drop_duplicates().sort_values(by=[\"num_features_same\"], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Assume your dataframe is called df\n",
    "names = vf_featuredf.iloc[:, 0]            # First column = names\n",
    "features = vf_featuredf.iloc[:, 1:]        # Rest = binary features\n",
    "\n",
    "# Step 2: t-SNE with Hamming distance (good for binary vectors)\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=42, metric='hamming')\n",
    "embedding = tsne.fit_transform(features)\n",
    "\n",
    "# Step 3: Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=50, alpha=0.5, c=\"mediumpurple\")\n",
    "\n",
    "# Optionally annotate points with names\n",
    "for i, name in enumerate(names):\n",
    "    if i % 4 == 0:\n",
    "        plt.text(embedding[i, 0] + 0.005, embedding[i, 1] + 0.005, str(name), fontsize=10)\n",
    "\n",
    "plt.title(\"t-SNE of Binary Vectors\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf_featuredf = vf_featuredf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(list(pd.merge(data_vf[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_vf[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"]))\n",
    "arr = temp[np.argsort(temp)][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data_vf[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index()[\"jump_profile\"].values, data_vf[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean()[\"num_features_same\"].values)\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.scatter(np.arange(219), pd.merge(data_vf[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_vf[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"], c=\"mediumpurple\")\n",
    "print(np.corrcoef(np.arange(218), arr)[0,1])\n",
    "plt.xlabel(\"Max jump profile\")\n",
    "plt.ylabel(\"Mean features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# t = pd.merge(data_autbrick[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_autbrick[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"]\n",
    "# plt.scatter(np.arange(len(t)), t)\n",
    "# plt.xlabel(\"Max jump profile\")\n",
    "# plt.ylabel(\"Mean features same\");\n",
    "\n",
    "# plt.figure()\n",
    "# t = pd.merge(data_autpaperclip[[\"pid\", \"num_features_same\"]].groupby(\"pid\").mean().reset_index(), data_autpaperclip[[\"pid\", \"jump_profile\"]].groupby(\"pid\").max().reset_index(), on=[\"pid\"]).sort_values(\"jump_profile\")[\"num_features_same\"]\n",
    "# plt.scatter(np.arange(len(t)), t)\n",
    "# plt.xlabel(\"Max jump profile\")\n",
    "# plt.ylabel(\"Mean features same\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binary_features_heatmap(df, featurecols):\n",
    "    pids = df[\"pid\"].unique()\n",
    "    cnt = 1\n",
    "    for pid in pids:\n",
    "        responses = df[df[\"pid\"] == pid][\"response\"].values\n",
    "        pid_data = df[df[\"pid\"] == pid][featurecols].reset_index(drop=True)\n",
    "        \n",
    "        plt.figure(figsize=(25, len(pid_data) * 0.25))\n",
    "        sns.heatmap(\n",
    "            pid_data,\n",
    "            cmap=sns.color_palette([\"white\", \"mediumpurple\"]),\n",
    "            cbar=False,\n",
    "            linewidths=0.5,\n",
    "            linecolor='black'\n",
    "        )\n",
    "        plt.title(f\"Binary Features Heatmap for PID {pid}\")\n",
    "        plt.gca().xaxis.tick_top()\n",
    "        plt.xticks(ticks=np.arange(len(featurecols)) + 0.5, labels=featurecols, rotation=90)\n",
    "        plt.yticks(ticks=np.arange(len(responses)) + 0.5, labels=responses, rotation=0)\n",
    "        plt.show()\n",
    "        \n",
    "        if cnt == 10:\n",
    "            break\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data_vf, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data_autbrick, autbrick_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binary_features_heatmap(data_autpaperclip, autpaperclip_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few vs many dimensions\n",
    "# How persistant are different dimensions? Compared to random\n",
    "# Transitions between different dimensions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

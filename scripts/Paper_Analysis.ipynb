{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from brokenaxes import brokenaxes\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "plt.rcParams.update({\n",
    "    \"axes.facecolor\": \"white\",                      \n",
    "    \"axes.edgecolor\": \"black\",                      \n",
    "    \"patch.facecolor\": \"lightcoral\",\n",
    "    \"text.usetex\": False,                           \n",
    "    \"font.family\": \"sans-serif\",                    \n",
    "    \"axes.spines.top\": False,                       \n",
    "    \"axes.spines.right\": False,                     \n",
    "    \"axes.labelsize\": 16,                           \n",
    "    \"xtick.labelsize\": 14,                          \n",
    "    \"ytick.labelsize\": 14,                          \n",
    "    \"axes.titlesize\": 18,                           \n",
    "    \"figure.dpi\": 100,                              \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../csvs/hills.csv\")\n",
    "data = data[~data[\"response\"].isin([\"mammal\", \"bacterium\", \"unicorn\", \"woollymammoth\"])].reset_index(drop=True)\n",
    "data[\"previous_response\"] = data.groupby(\"pid\")[\"response\"].shift(1)\n",
    "data[\"order\"] = data.groupby(\"pid\").cumcount() + 1\n",
    "data = data.drop(columns=[\"fpatchnum\", \"fpatchitem\", \"fitemsfromend\", \"flastitem\",  \"meanirt\", \"catitem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996846/1064825101.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n"
     ]
    }
   ],
   "source": [
    "def get_featuredf():\n",
    "    featuredict = pk.load(open(f\"../files/features_gpt41.pk\", \"rb\"))\n",
    "    featuredf = pd.DataFrame.from_dict(featuredict, orient='index')\n",
    "    featuredf = featuredf.replace({True: 1, False: 0, 'True': 1, 'True.': 1, 'TRUE': 1, 'true': 1, 'False': 0, 'False.': 0, 'false': 0})\n",
    "    featuredf = featuredf[featuredf.applymap(lambda x: isinstance(x, int)).all(axis=1)]\n",
    "    return featuredf, featuredf.columns.tolist()\n",
    "\n",
    "vf_featuredf, vf_featurecols = get_featuredf()\n",
    "vf_featuredf = vf_featuredf.loc[vf_featuredf.index.isin(data[\"response\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add features to responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_to_responsedf(df):\n",
    "    featuredict = vf_featuredf.to_dict(orient='index')\n",
    "    mapped_features = df['response'].map(featuredict)\n",
    "    mapped_features = mapped_features.apply(lambda x: x if isinstance(x, dict) else {})\n",
    "    fc = pd.DataFrame(mapped_features.tolist())\n",
    "    df = pd.concat([df, fc], axis=1)\n",
    "    df = df.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})\n",
    "    dropped_rows = df[df[vf_featurecols].isna().any(axis=1)]\n",
    "    df = df.dropna(subset=vf_featurecols)\n",
    "    for col in vf_featurecols:\n",
    "        df[col] = df[col].astype(int)\n",
    "    return df, dropped_rows\n",
    "\n",
    "data, dropped_rows = add_features_to_responsedf(data)\n",
    "feature_cols = [col for col in data.columns if col.startswith('feature_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Datasets - Figure 2E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 100\n",
    "\n",
    "# Uniform\n",
    "sum_activity_diff1 = []\n",
    "pid_to_count = data.groupby(\"pid\")[\"response\"].count().to_dict()\n",
    "unique_responses = data[\"response\"].unique()\n",
    "for i in range(num_simulations):\n",
    "    data_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data_simulatedrandom = pd.concat([data_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff1.append(sum_abs_mean_activity_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency-based\n",
    "with open(\"../files/freq_abs_log.json\", \"r\") as f:\n",
    "    freq_abs = json.load(f)\n",
    "freq_abs = {k: v for k, v in freq_abs.items() if k.replace(\" \", \"\").replace(\"-\", \"\") in unique_responses}\n",
    "with open(\"../files/response_corrections.json\", 'r') as f:\n",
    "    corrections = json.load(f)\n",
    "probs = np.array([freq_abs[corrections.get(r, r)] for r in unique_responses])\n",
    "probs = np.exp(probs)          # convert from log frequencies\n",
    "probs /= probs.sum()           # normalize to sum to 1\n",
    "sum_activity_diff2 = []\n",
    "for i in range(num_simulations):\n",
    "    data_simulatedrandom = pd.DataFrame(columns=[\"pid\", \"response\"])\n",
    "    for pid, count in pid_to_count.items():\n",
    "        sampled_responses = np.random.choice(unique_responses, size=count, replace=False, p=probs)\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"pid\": [pid] * count,\n",
    "            \"response\": sampled_responses\n",
    "        })\n",
    "        data_simulatedrandom = pd.concat([data_simulatedrandom, temp_df], ignore_index=True)\n",
    "    data_simulatedrandom, dropped_rows2_simulatedrandom = add_features_to_responsedf(data_simulatedrandom)\n",
    "    sum_abs_mean_activity_diff = sum(np.abs(vf_featuredf[feature_cols].mean() - data_simulatedrandom[feature_cols].mean()))\n",
    "    sum_activity_diff2.append(sum_abs_mean_activity_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(5,3), gridspec_kw={'width_ratios': [0.5,0.5,0.3]})\n",
    "ax1.hist(sum_activity_diff1, alpha=0.5, label='Uniform sampling', color='cornflowerblue')\n",
    "ax1.set_xlim(2.35, 3.35)\n",
    "ax1.set_ylabel(\"Num. simulations\")\n",
    "ax2.hist(sum_activity_diff2, alpha=0.7, label='Frequency-based sampling', color='royalblue')\n",
    "ax2.set_xlim(5.8, 6.8)\n",
    "ax2.set_xlabel(\"Total abs. feature activity diff.\")\n",
    "vline_value = np.sum(np.abs(vf_featuredf[feature_cols].mean() - data[feature_cols].mean()))\n",
    "ax3.axvline(vline_value, color='darkblue', linestyle='--', linewidth=2, label=f'Data = {vline_value:.2f}')\n",
    "ax3.set_xlim(vline_value - 0.5, vline_value + 0.5)\n",
    "\n",
    "for a, b in zip([ax1, ax2], [ax2, ax3]):\n",
    "    a.spines['right'].set_visible(False)\n",
    "    b.spines['left'].set_visible(False)\n",
    "\n",
    "d = 0.015  # size of break slant\n",
    "def add_break(ax_left, ax_right):\n",
    "    kwargs = dict(color='k', clip_on=False)\n",
    "    ax_left.plot((1 - d, 1 + d), (-d, +d), transform=ax_left.transAxes, **kwargs)\n",
    "    ax_right.plot((-d, +d), (-d, +d), transform=ax_right.transAxes, **kwargs)\n",
    "\n",
    "add_break(ax1, ax2)\n",
    "add_break(ax2, ax3)\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.56, 1), ncol=1)\n",
    "plt.savefig(\"../plots/2E.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same'] = None\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols = featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    num_features_same = [np.nan]  # Initialize with nan for the first row\n",
    "    \n",
    "    for i in range(1, len(group)):\n",
    "        row1 = group.loc[i - 1, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same.append(np.nan)\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            # consecutive_1s = ((row1 == 1) & (row2 == 1))\n",
    "            num_features_same.append(consecutive_1s.sum())\n",
    "    \n",
    "    group['num_features_same'] = num_features_same\n",
    "    return group\n",
    "\n",
    "data = get_num_features_same(data, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle within each pid - TAKES SUPER LONG, SERIALIZE\n",
    "try:\n",
    "    means = pk.load(open(\"../files/shuffled_data_meanHS.pk\", \"rb\"))\n",
    "except:\n",
    "    means = []\n",
    "    for _ in tqdm(range(100)):\n",
    "        shuffled_data2 = data.groupby(\"pid\", group_keys=False).apply(lambda x: x.sample(frac=1).reset_index(drop=True))\n",
    "        shuffled_data2 = get_num_features_same(shuffled_data2, vf_featurecols)\n",
    "        means.append(np.mean(shuffled_data2[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values))\n",
    "    pk.dump(means, open(\"../files/shuffled_data_meanHS.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.figure(figsize=(4, 3.5))\n",
    "data_mean = np.mean(data[[\"num_features_same\", \"pid\"]].groupby(\"pid\").mean()[\"num_features_same\"].values)\n",
    "bax = brokenaxes(xlims=((97.9, 98.6), (data_mean - 0.1, data_mean + 0.1)),hspace=0.05)\n",
    "bax.hist(means, bins=6, color='lightcoral', label=\"Shuffled\", alpha=0.7)\n",
    "bax.axvline(data_mean, color='red', linestyle='--', linewidth=2, label=f'Data = {data_mean:.2f}')\n",
    "bax.set_xlabel(\"Mean num features same\", labelpad=25)\n",
    "bax.set_ylabel(\"Number of simulations\", labelpad=25)\n",
    "bax.axs[0].set_xticks([98.0, 98.5])\n",
    "bax.axs[1].set_xticks([105, 106])\n",
    "bax.legend(loc='upper right', bbox_to_anchor=(0.981, 1))\n",
    "plt.savefig(\"../plots/2A.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N=back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_2back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(2, len(group)):\n",
    "        row1 = group.loc[i - 2, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_2back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data = get_num_features_same(data, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_3back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(3, len(group)):\n",
    "        row1 = group.loc[i - 3, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_3back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data = get_num_features_same(data, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_4back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(4, len(group)):\n",
    "        row1 = group.loc[i - 4, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_4back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data = get_num_features_same(data, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_same(df, featurecols):\n",
    "    df['num_features_same_5back'] = np.nan  # Initialize the column with NaN\n",
    "    df = df.groupby('pid', group_keys=False).apply(calculate_num_features_same, featurecols=featurecols)\n",
    "    return df\n",
    "\n",
    "def calculate_num_features_same(group, featurecols):\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    num_features_same = np.full(len(group), np.nan)  # Initialize with NaN\n",
    "    \n",
    "    for i in range(5, len(group)):\n",
    "        row1 = group.loc[i - 5, featurecols]\n",
    "        row2 = group.loc[i, featurecols]\n",
    "\n",
    "        # Check for NaN values\n",
    "        if row1.isna().any() or row2.isna().any():\n",
    "            num_features_same[i] = np.nan\n",
    "        else:\n",
    "            consecutive_1s = ((row1 == 1) & (row2 == 1)) | ((row1 == 0) & (row2 == 0))\n",
    "            num_features_same[i] = consecutive_1s.sum()\n",
    "    \n",
    "    group['num_features_same_5back'] = num_features_same  # Assign correctly\n",
    "    return group\n",
    "\n",
    "data = get_num_features_same(data, vf_featurecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means and standard errors\n",
    "means = [\n",
    "    np.mean(data[\"num_features_same_5back\"]),\n",
    "    np.mean(data[\"num_features_same_4back\"]),\n",
    "    np.mean(data[\"num_features_same_3back\"]),\n",
    "    np.mean(data[\"num_features_same_2back\"]),\n",
    "    np.mean(data[\"num_features_same\"])\n",
    "]\n",
    "\n",
    "std_errors = [\n",
    "    np.std(data[\"num_features_same_5back\"], ddof=1) / np.sqrt(len(data[\"num_features_same_5back\"].dropna())),\n",
    "    np.std(data[\"num_features_same_4back\"], ddof=1) / np.sqrt(len(data[\"num_features_same_4back\"].dropna())),\n",
    "    np.std(data[\"num_features_same_3back\"], ddof=1) / np.sqrt(len(data[\"num_features_same_3back\"].dropna())),\n",
    "    np.std(data[\"num_features_same_2back\"], ddof=1) / np.sqrt(len(data[\"num_features_same_2back\"].dropna())),\n",
    "    np.std(data[\"num_features_same\"], ddof=1) / np.sqrt(len(data[\"num_features_same\"].dropna()))\n",
    "]\n",
    "\n",
    "x_labels = [-5, -4, -3, -2, -1]\n",
    "\n",
    "plt.bar(x_labels, means, yerr=std_errors, capsize=5, alpha=0.6, color='lightcoral')\n",
    "plt.xlabel(\"Pos. preceeding most recent resp.\")\n",
    "plt.ylabel(\"Mean num. features same\")\n",
    "plt.ylim(95, 107)\n",
    "plt.xticks(x_labels, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.savefig(\"../plots/2B.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sum_features\"] = data[vf_featurecols].sum(axis=1)\n",
    "data[\"logRT\"] = np.log(data[\"RT\"] + 0.001)\n",
    "data = data[data[\"logRT\"] > -1.6]   # removes RT < 200 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_activity_influence = []\n",
    "for feat in vf_featurecols:\n",
    "    rt0 = data.loc[data[feat] == 0, \"logRT\"].mean()\n",
    "    rt1 = data.loc[data[feat] == 1, \"logRT\"].mean()\n",
    "    influence = rt0 - rt1\n",
    "    rt_activity_influence.append(influence)\n",
    "rt_activity_influence = np.array(rt_activity_influence)\n",
    "\n",
    "feature_activities = data[vf_featurecols].mean(axis=0).values\n",
    "import scipy.stats\n",
    "pearsr_corr, p_value = scipy.stats.pearsonr(feature_activities, rt_activity_influence)\n",
    "print(f\"Pearson Correlation: {pearsr_corr}, p-value: {p_value}\")\n",
    "sspear_corr, p_value = scipy.stats.spearmanr(feature_activities, rt_activity_influence)\n",
    "print(f\"Spearman Correlation: {sspear_corr}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(feature_activities)\n",
    "y = np.array(rt_activity_influence)\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X).fit()\n",
    "x_line = np.linspace(x.min(), x.max(), 200)\n",
    "X_line = sm.add_constant(x_line)\n",
    "y_line = model.predict(X_line)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(x, y, color='royalblue', alpha=0.4, s=70)\n",
    "plt.plot(x_line, y_line, color='blue', linewidth=4)\n",
    "\n",
    "plt.xlabel(\"Mean Activity of Feature\")\n",
    "plt.ylabel(\"Log(RT) Influence on Feature Activity\")\n",
    "plt.tight_layout()\n",
    "plt.xlim(-0.02,1.02)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks([-0.4, -0.2, 0, 0.2, 0.4], fontsize=20)\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.savefig(\"../plots/2F.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = {}\n",
    "for col in feature_cols:\n",
    "    new_col = f\"{col}_switch\"\n",
    "    tmp = (data.groupby('pid')[col].transform(lambda x: (x != x.shift()).astype(int)))\n",
    "    tmp.loc[data.groupby('pid').head(1).index] = np.nan\n",
    "    new_cols[new_col] = tmp\n",
    "data = pd.concat([data, pd.DataFrame(new_cols, index=data.index)], axis=1)\n",
    "\n",
    "diff_dict = {}\n",
    "for col in feature_cols:\n",
    "    bin_col = f\"{col}_switch\"\n",
    "    temp = data.dropna(subset=[bin_col])\n",
    "    logrt_diff = temp.groupby('pid').apply(lambda g: g.loc[g[bin_col] == 1, 'logRT'].mean() - g.loc[g[bin_col] == 0, 'logRT'].mean(), include_groups=False)  # Compute logRT difference per pid\n",
    "    diff_dict[col] = logrt_diff\n",
    "diff_df = pd.DataFrame(diff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3.5))\n",
    "plt.hist(diff_df.sum(axis=0).values, color=\"lightcoral\", alpha=0.6, bins=5);\n",
    "plt.ylabel(\"Num. features\")\n",
    "plt.xlabel(\"log(RT) diff. (Switch - Stay)\");\n",
    "plt.savefig(\"../plots/2D.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"RT\"] = data[\"RT\"] + 0.001\n",
    "data[\"logRT\"] = np.log(data[\"RT\"])\n",
    "data[\"bin\"] = pd.qcut(data[\"num_features_same\"], q=5)\n",
    "group_stats = data.groupby(\"bin\")[\"logRT\"].agg(['mean', 'sem']).reset_index()\n",
    "x_positions = [ (interval.left + interval.right) / 2 for interval in group_stats[\"bin\"] ]\n",
    "bin_labels = [f\"{int(interval.left)}â€“{int(interval.right)}\" for interval in group_stats[\"bin\"]]\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.errorbar(\n",
    "    x_positions,\n",
    "    group_stats[\"mean\"],\n",
    "    yerr=group_stats[\"sem\"],\n",
    "    fmt='o-',\n",
    "    color='lightcoral',\n",
    "    ecolor='black',\n",
    "    elinewidth=1,\n",
    "    capsize=4\n",
    ")\n",
    "plt.xticks(ticks=x_positions, labels=bin_labels, rotation=45)\n",
    "plt.xlabel(\"Num. features same\")\n",
    "plt.ylabel(\"Mean log(RT)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../plots/2C.pdf\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vf_featuredf.iloc[:, 0]            # First column = names\n",
    "features = vf_featuredf.iloc[:, 1:]        # Rest = binary features\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=42, metric='hamming')\n",
    "embedding = tsne.fit_transform(features)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], s=50, alpha=0.5, c=\"mediumpurple\")\n",
    "plt.title(\"t-SNE of Binary Vectors\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthings to check\\n\\n1. frequency of clusters\\n2. cluster transitions matrix\\n3. do people come back to previous clusters?\\n4. depths per cluster\\n5. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "things to check\n",
    "\n",
    "1. frequency of clusters\n",
    "2. cluster transitions matrix\n",
    "3. do people come back to previous clusters?\n",
    "4. depths per cluster\n",
    "5. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from IPython.display import HTML\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Circle\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_features = pk.load(open(\"animal_features.pk\", \"rb\"))\n",
    "df = pd.DataFrame.from_dict(animal_features, orient='index')\n",
    "df = df.replace({'True': 1, 'True.': 1, 'False': 0, 'False.': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Mask the upper triangle\n",
    "mask = np.triu(np.ones(correlation_matrix.shape), k=1)  # Upper triangle mask\n",
    "correlation_matrix.where(mask == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_hist(arr, xlabel, ylabel, ax):\n",
    "#     ax.hist(arr, color=\"#d3c6f5\")\n",
    "#     ax.set_xlabel(xlabel)\n",
    "#     ax.set_ylabel(ylabel)\n",
    "\n",
    "def get_sentence_transformer_embeddings(texts):\n",
    "    \"\"\"Extracts Text Embeddings using SentenceTransformer (model: gte-large)\n",
    "    Args:\n",
    "        texts (list): List of texts\n",
    "    Returns:\n",
    "        dict: Text and corresponding embedding\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('thenlper/gte-large')\n",
    "    embeddings = model.encode(texts)\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)         # normalise embeddings\n",
    "    return dict(zip(texts, embeddings))\n",
    "\n",
    "# def get_clusters(embeddings, texts, cluster_assignment, printclusters):\n",
    "#     \"\"\"Helper function for perform_hierarchicalClustering()\n",
    "#     Args:\n",
    "#         embeddings (list): List of embeddings\n",
    "#         texts (list): List of texts\n",
    "#         cluster_assignment (list): cluster assignment for each text/embedding\n",
    "#         printcluster (bool): True will print all texts in each cluster\n",
    "#     Returns:\n",
    "#         response_to_cluster dict: text to cluster number\n",
    "#         cluster_to_response dict: cluster number to list of texts in that cluster\n",
    "#         num_clusters int: number of clusters\n",
    "#         min_similarities dict: cluster to min text similiarity\n",
    "#     \"\"\"\n",
    "\n",
    "#     cluster_to_response = defaultdict(list)                                 # make cluster_to_response\n",
    "#     cluster_to_embeddings = defaultdict(list)                               # make cluster_to_embeddings (used for min sim)\n",
    "#     for ind, cluster_num in enumerate(cluster_assignment):\n",
    "#         cluster_to_response[cluster_num].append(texts[ind])\n",
    "#         cluster_to_embeddings[cluster_num].append(embeddings[ind])\n",
    "    \n",
    "#     response_to_cluster = dict(zip(texts, cluster_assignment))              # make response_to_cluster \n",
    "    \n",
    "#     num_clusters = len(np.unique(cluster_assignment))                       # find num clusters\n",
    "\n",
    "#     min_similarities = {}                                                   # find min paiwise text similarity in each cluster\n",
    "#     for cl in cluster_to_embeddings:\n",
    "#         stacked = np.array(cluster_to_embeddings[cl])\n",
    "#         sim = stacked @ stacked.T\n",
    "#         np.fill_diagonal(sim, np.inf)\n",
    "#         min_sim = np.min(sim)\n",
    "#         min_similarities[cl] = min_sim\n",
    "    \n",
    "#     if printclusters:                                                       # print clusters if True\n",
    "#         for cluster_num, responses in cluster_to_response.items():\n",
    "#             print(f\"Cluster {cluster_num}\")\n",
    "#             print(responses, end=\"\\n\\n\")\n",
    "\n",
    "#     return response_to_cluster, cluster_to_response, num_clusters, min_similarities\n",
    "\n",
    "# def perform_hierarchicalClustering(embeddings, texts, ax, cut_off_distance, printclusters):\n",
    "#     \"\"\"Performs hierarchical clustering\n",
    "#     Args:\n",
    "#         embeddings (list): List of embeddings\n",
    "#         texts (list): List of texts\n",
    "#         ax: axes for plotting\n",
    "#         cut_off_distance (float): distance threshold for hierarchical clustering\n",
    "#         printcluster (bool): True will print all texts in each cluster\n",
    "#     Calls:\n",
    "#         get_clusters()\n",
    "#     \"\"\"\n",
    "#     linked = linkage(embeddings, 'ward')                                                                                                    # 'ward' distance for measuring distance between clusters\n",
    "#     dendrogram(linked, orientation='top', labels=texts, distance_sort='descending', show_leaf_counts=False, no_labels=True, ax=ax[0])       # Make dendogram\n",
    "    \n",
    "#     # Elbow plot -- Plot mean minsim/number of clusters as a fn of cut-off distance\n",
    "#     mean_minsemsim = []\n",
    "#     num_clusters = []\n",
    "#     for cod in np.linspace(1, 8, 30):\n",
    "#         cluster_assignment = fcluster(linked, t=cod, criterion='distance')\n",
    "#         _, _, _, minsims = get_clusters(embeddings, texts, cluster_assignment, False)\n",
    "#         mean_minsemsim.append(np.mean(list(minsims.values())))\n",
    "#         num_clusters.append(len(np.unique(cluster_assignment)))\n",
    "#     ax[1].plot(np.linspace(1, 8, 30), mean_minsemsim)\n",
    "#     ax[1].set_xlabel(\"Cut-off distance\"); ax[1].set_ylabel(\"Mean cluster semantic similarity\")\n",
    "#     ax[2].plot(np.linspace(1, 8, 30), num_clusters)\n",
    "#     ax[2].set_xlabel(\"Cut-off distance\"); ax[2].set_ylabel(\"Number of clusters\")\n",
    "\n",
    "#     cluster_assignment = fcluster(linked, t=cut_off_distance, criterion='distance')         # assign clusters using the decided cut_off_distance\n",
    "#     return get_clusters(embeddings, texts, cluster_assignment, printclusters)\n",
    "\n",
    "def calculate_cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"Calculates cosine similarity between two embeddings\n",
    "    Args:\n",
    "        embedding1: first embedding\n",
    "        embedding2: second embedding\n",
    "    Returns:\n",
    "        cosine similarity\n",
    "    \"\"\"\n",
    "    if np.any(embedding1) and np.any(embedding2):\n",
    "        return np.dot(embedding1, embedding2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def plot_movement(cluster_order, pid, ax, num_clusters, marker_sizes):\n",
    "    # Calculate the positions of the equidistant points on the circle\n",
    "    theta = np.linspace(0, 2 * np.pi, num_clusters, endpoint=False)\n",
    "    x = 1.5 * np.cos(theta)\n",
    "    y = 1.5 * np.sin(theta)\n",
    "\n",
    "    # Adjust the cluster_order to be zero-indexed\n",
    "    cluster_order = [p-1 for p in cluster_order]\n",
    "\n",
    "    # Create the figure and axis\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Annotate the points with their numbers\n",
    "    for i in range(num_clusters):\n",
    "        ax.text(x[i], y[i], str(i + 1), fontsize=12, ha='center', va='center', c=\"r\")\n",
    "\n",
    "    # Define a colormap\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "\n",
    "    # Draw lines and arrows\n",
    "    for i in range(len(cluster_order) - 1):\n",
    "        start = (x[cluster_order[i]], y[cluster_order[i]])\n",
    "        end = (x[cluster_order[i + 1]], y[cluster_order[i + 1]])\n",
    "        \n",
    "        # Get the color from the colormap\n",
    "        color = cmap(i / (len(cluster_order) - 1))\n",
    "        if start[0] == end[0] and start[1] == end[1]:\n",
    "\n",
    "            start_tuple = (start[0], start[1])\n",
    "            if start_tuple not in marker_sizes:\n",
    "                marker_sizes[start_tuple] = 10  # Initialize the marker size for this point\n",
    "\n",
    "            ax.plot(start[0], start[1], 'o', c=\"black\", markersize=marker_sizes[start_tuple], clip_on=False)\n",
    "            marker_sizes[start_tuple] += 1\n",
    "            continue\n",
    "        \n",
    "        # Draw the line with the assigned color\n",
    "        ax.plot([start[0], end[0]], [start[1], end[1]], color=color)\n",
    "        \n",
    "        # Calculate the midpoint for the arrow\n",
    "        mid = ((start[0] + end[0]) / 2, (start[1] + end[1]) / 2)\n",
    "        direction = (end[0] - start[0], end[1] - start[1])\n",
    "        length = np.hypot(direction[0], direction[1])\n",
    "        direction = (direction[0] / length, direction[1] / length)\n",
    "        \n",
    "        # Draw the arrow at the midpoint\n",
    "        arrow = FancyArrowPatch((mid[0] - direction[0] * 0.15, mid[1] - direction[1] * 0.15),\n",
    "                                (mid[0] + direction[0] * 0.15, mid[1] + direction[1] * 0.15),\n",
    "                                color=color, arrowstyle='-|>', mutation_scale=15)\n",
    "        ax.add_patch(arrow)\n",
    "\n",
    "    # Show the final static image\n",
    "    ax.set_title(f\"\\nParticipant {pid}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_to_animals = {\n",
    "#     1: [\"aardvark\", \"ankole\", \"antelope\", \"aoudad\", \"blesbok\", \"bontebok\", \"buffalo\", \"bushbaby\", \"bushbuck\", \"bushpig\", \"camel\", \"caracal\", \"chameleon\", \"cheetah\", \"chimpanzee\", \"civet\", \"cliffchat\", \"cobra\", \"colobus\", \"cusimanse\", \"dassie\", \"dikdik\",\"duiker\", \"eland\", \"elephant\", \"fennecfox\", \"gazelle\", \"gemsbok\", \"genet\", \"gerenuk\", \"giraffe\", \"gnu\", \"gorilla\", \"hartebeest\", \"hippopotamus\", \"hyena\", \"hyrax\", \"impala\", \"jackal\", \"jerboa\", \"klipspringer\", \"kob\", \"kongoni\", \"koodoo\", \"kudu\", \"lechwe\", \"lemur\", \"leopard\", \"lion\", \"lioness\", \"lourie\", \"lovebird\", \"manatee\", \"mandrill\", \"meerkat\", \"mongoose\", \"monkey\", \"mousebird\", \"nyala\", \"okapi\", \"oribi\", \"oryx\", \"ostrich\", \"panther\", \"puku\", \"quagga\", \"reedbuck\", \"rhino\", \"rhinoceros\", \"serval\", \"sifaka\", \"springbok\", \"springhare\", \"steenbok\", \"steenbuck\", \"sunbird\", \"sungazer\", \"suricate\", \"tiger\", \"topi\", \"tsessebe\", \"warthog\", \"waterbuck\", \"wildebeest\", \"zebra\", \"zorilla\"],\n",
    "#     2: [\"alpaca\", \"beaver\", \"chamois\", \"chinchilla\", \"ermine\", \"fox\", \"llama\", \"mink\", \"rabbit\", \"sable\", \"vicuna\"],\n",
    "#     3: [\"arcticfox\", \"auk\", \"caribou\", \"muskox\", \"penguin\", \"polarbear\", \"reindeer\", \"seal\", \"sheathbill\", \"walrus\", \"woollymammoth\"],\n",
    "#     4: [\"bandicoot\", \"corella\", \"devil\", \"dingo\", \"duckbill\", \"dunnart\", \"emu\", \"galah\", \"kangaroo\", \"kiwi\", \"koala\", \"macropod\", \"malleefowl\", \"numbat\", \"pademelon\", \"platypus\", \"opossum\", \"possum\", \"potoroo\", \"quoll\", \"sugar\", \"sugarglider\", \"tasmaniandevil\", \"wallaby\", \"wallaroo\", \"wambenger\", \"wombat\", \"woylie\"],\n",
    "#     5: [\"ass\", \"burro\", \"camel\", \"colt\", \"donkey\", \"dromedary\", \"horse\", \"llama\", \"mare\", \"mule\", \"mustang\", \"ox\", \"pony\", \"trotter\", \"yak\"],\n",
    "#     6: [\"albatross\", \"avian\", \"avocet\", \"bird\", \"bittern\", \"blackbird\", \"bluebird\", \"bluefootedbooby\", \"bluegill\", \"bluejay\", \"bobolink\", \"booby\", \"boubou\", \"bulbul\", \"bullfinch\", \"bunting\", \"Bustard\", \"buzzard\", \"canary\", \"cardinal\", \"cassowary\", \"chickadee\", \"chicken\", \"cliffchat\", \"cock\", \"cockatiel\", \"cockatoo\", \"conure\", \"corella\", \"cormorant\", \"crake\", \"crow\", \"cuckoo\", \"curassow\", \"dabchick\", \"darter\", \"dikkop\", \"dodo\", \"dove\", \"drake\", \"duck\", \"duckling\", \"eagle\", \"eaglet\", \"egret\", \"emu\", \"ewe\", \"falcon\", \"finch\", \"flamingo\", \"fowl\", \"francolin\", \"frogmouth\", \"galah\", \"gallinule\", \"gander\", \"goldfinch\", \"gonolek\", \"goshawk\", \"gosling\", \"grebe\", \"greenfinch\", \"grosbeak\", \"grouse\", \"guineafowl\", \"gull\", \"gyrfalcon\", \"hammerkop\", \"harrier\", \"hawk\", \"heron\", \"honeyeater\", \"hornbill\", \"hummingbird\", \"ibis\", \"jackdaw\", \"jay\", \"kestrel\", \"kingfisher\", \"kite\", \"kiwi\", \"kookaburra\", \"lark\", \"laughingthrush\", \"loon\", \"lorikeet\", \"lory\", \"lourie\", \"lovebird\", \"macaw\", \"magpie\", \"mallard\", \"malleefowl\", \"marshbird\", \"meadowlark\", \"merganser\", \"merlin\", \"mockingbird\", \"moorhen\", \"motmot\", \"mouse\", \"mousebird\", \"myna\", \"nightingale\", \"openbill\", \"oriole\", \"osprey\", \"ostrich\", \"owl\", \"parakeet\", \"parrot\", \"partridge\", \"peacock\", \"peafowl\", \"pelican\", \"penguin\", \"peregrine\", \"pheasant\", \"pigeon\", \"pintail\", \"pitta\", \"plover\", \"puffin\", \"quail\", \"quetzal\", \"rail\", \"raven\", \"rhea\", \"roadrunner\", \"robin\", \"rook\", \"sandgrouse\", \"sandpiper\", \"seagull\", \"seriema\", \"sheathbill\", \"shelduck\", \"shoveler\", \"shrike\", \"siskin\", \"skylark\", \"snipe\", \"songbird\", \"sparrow\", \"spoonbill\", \"spurfowl\", \"starling\", \"stilt\", \"stork\", \"sunbird\", \"sunbittern\", \"swallow\", \"swallowtail\", \"swan\", \"swift\", \"tanager\", \"teal\", \"thrush\", \"tinamou\", \"titmouse\", \"toucan\", \"tragopan\", \"trumpeter\", \"turaco\", \"turkey\", \"umbrellabird\", \"vulture\", \"warbler\", \"waxbill\", \"weaver\", \"wigeon\", \"wildfowl\", \"woodpecker\", \"wren\"],\n",
    "#     7: [\"ankole\", \"antilope\", \"aoudad\", \"bison\", \"blesbok\", \"bontebok\", \"buffalo\", \"bullock\", \"bushbock\", \"calf\", \"cattle\", \"cow\", \"dik dik\", \"gemsbok\", \"gerenuk\", \"hartebeest\", \"heifer\", \"ibex\", \"klipspringer\", \"kob\", \"monitor\", \"muskox\", \"nilgai\", \"nyala\", \"oribi\", \"pronghorn\", \"puku\", \"reedbuck\", \"springbok\", \"steenbuck\", \"steer\", \"topi\", \"tsessebe\", \"water buffalo\", \"waterbuck\", \"yak\", \"zebu\"],\n",
    "#     8: [\"akita\", \"barbet\", \"blacklab\", \"bloodhound\", \"bulldog\", \"canine\", \"chihuahua\", \"coati\", \"coatimundi\", \"coyote\", \"dachshund\", \"dalmatian\", \"dog\", \"fox\", \"golden retriever\", \"great dane\", \"greyhound\", \"grison\", \"harrier\", \"husky\", \"hyena\", \"jackal\", \"labrador retriever\", \"malamute\", \"pembroke welsh corgi\", \"poodle\", \"pug\", \"puggle\", \"pup\", \"shihtzu\", \"siberian husky\", \"terrier\", \"timber wolf\", \"wild dog\", \"wolf\"],\n",
    "#     9: [\"blacktailed deer\", \"brocket\", \"buck\", \"caribou\", \"chital\", \"deer\", \"doe\", \"eland\", \"elk\", \"fawn\", \"gazelle\", \"gnu\", \"impala\", \"moose\", \"muledeer\", \"muntjac\", \"reindeer\", \"roe\", \"sambar\", \"stag\", \"wapiti\", \"whitetailed deer\", \"wildebeest\"],\n",
    "#     10: [\"ass\", \"billygoat\", \"bronco\", \"bullock\", \"calf\", \"chick\", \"chicken\", \"cock\", \"colt\", \"cow\", \"donkey\", \"ferret\", \"foal\", \"goat\", \"heifer\", \"hen\", \"hog\", \"horse\", \"kid\", \"lamb\", \"mare\", \"miniature pony\", \"mule\", \"pig\", \"piglet\", \"pony\", \"potbellied pig\", \"ram\", \"rabbit\", \"rooster\", \"sheep\", \"snake\", \"sow\", \"spider\", \"stallion\", \"turkey\"],\n",
    "#     11: [\"bengal tiger\", \"bobcat\", \"bull\", \"cat\", \"caracal\", \"cheetah\", \"civet\", \"cougar\", \"crane\", \"jaguar\", \"jaguarundi\", \"leopard\", \"leopardess\", \"liger\", \"lion\", \"lynx\", \"margay\", \"mountainlion\", \"ocelot\", \"panther\", \"puma\", \"ring tailed cat\", \"serval\", \"siamese cat\", \"snow leopard\", \"snow lion\", \"tiger\", \"tigress\", \"tomcat\", \"whitetiger\", \"wildcat\"],\n",
    "#     12: [\"fisher\", \"angelfish\", \"arrowheadshark\", \"balloonfish\", \"barracuda\", \"bass\", \"betta\", \"blacktipreefshark\", \"blowfish\", \"carp\", \"catfish\", \"cavefish\", \"cichlids\", \"clownfish\", \"cuttlefish\", \"dragonet\", \"filefish\", \"fish\", \"flounder\", \"freshwater fish\", \"goldfish\", \"great white shark\", \"grenadier\", \"grouper\", \"grunt\", \"guppy\", \"herring\", \"jack\", \"knifefish\", \"koi\", \"lamprey\", \"lionfish\", \"lookdown\", \"mackerel\", \"mako shark\", \"minnow\", \"moray\", \"pacu\", \"parrotfish\", \"pike\", \"pink salmon\", \"piranha\", \"pleco\", \"porkfish\", \"pufferfish\", \"rainbowfish\", \"sailfish\", \"salmon\", \"saltwater fish\", \"scorpionfish\", \"seabass\", \"shark\", \"shrimp\", \"smelt\", \"stickleback\", \"stonefish\", \"sturgeon\", \"swordfish\", \"tang\", \"tetra\", \"tilapia\", \"triplefin\", \"trout\", \"tuna\", \"whale shark\"],\n",
    "#     13: [\"aardvark\", \"anteater\", \"armadillo\", \"bat\", \"bittern\", \"echidna\", \"hedgehog\", \"jumpingspider\", \"mole\", \"shrew\", \"spiderling\", \"sugar\"],    \n",
    "#     14: [\"ant\", \"antlion\", \"aphid\", \"bee\", \"beetle\", \"blackwidow\", \"bug\", \"butterfly\", \"caterpillar\", \"centipede\", \"cicada\", \"cockroach\", \"cricket\", \"daddy long legs\", \"dragonfly\", \"earthworm\", \"flea\", \"fly\", \"gnat\", \"grasshopper\", \"grub\", \"honeybee\", \"hornet\", \"insect\", \"June beetle\", \"ladybug\", \"larva\", \"leafy\", \"louse\", \"lubber\", \"maggot\", \"mealworm\", \"mite\", \"monarch butterfly\", \"mosquito\", \"moth\", \"pill bug\", \"praying mantis\", \"scorpion\", \"silkworm\", \"stick insect\", \"tarantula\", \"termite\", \"tick\", \"wasp\", \"worm\", \"yellow jacket\"],\n",
    "#     15: [\"antelope\", \"badger\", \"bear\", \"beaver\", \"bighorn\", \"bighorn\", \"bison\", \"blackbear\", \"boar\", \"bobcat\", \"brownbear\", \"caribou\", \"chipmunk\", \"chuckwalla\", \"cornsnake\", \"cottonmouth\", \"cougar\", \"coyote\", \"cub\", \"deer\", \"drongo\", \"elk\", \"fox\", \"grizzly bear\", \"kodiak bear\", \"moose\", \"mountaingoat\", \"mountainlion\", \"pronghorn\", \"puma\", \"rabbit\", \"raccoon\", \"skunk\", \"squirrel\", \"titmouse\", \"vaquita\", \"weasel\", \"wolf\"],\n",
    "#     16: [\"budgie\", \"canary\", \"cat\", \"cockatiel\", \"cockatoo\", \"dog\", \"gerbil\", \"goldenretriever\", \"goldfish\", \"guineapig\", \"guppy\", \"hamster\", \"kitten\", \"labradorretriever\", \"malamute\", \"parakeet\", \"parrot\", \"poodle\", \"puppy\", \"rabbit\"],\n",
    "#     17: [\"ape\", \"baboon\", \"bonobo\", \"capuchin\", \"chimpanzee\", \"colobus\", \"gibbon\", \"gorilla\", \"howler monkey\", \"human\", \"langur\", \"lemur\", \"loris\", \"macaque\", \"mandrill\", \"marmoset\", \"monkey\", \"orangutan\", \"primates\", \"saki monkey\", \"shrew\", \"siamang\", \"sifaka\", \"snow monkey\", \"spider monkey\", \"tamarin\", \"titi\"],\n",
    "#     18: [\"bunny\", \"coney\", \"hare\", \"jackrabbit\", \"rabbit\"],\n",
    "#     19: [\"adder\", \"alligator\", \"amphibian\", \"anaconda\", \"anole\", \"asp\", \"basilisk\", \"black mamba\", \"boaconstrictor\", \"bullfrog\", \"caiman\", \"chameleon\", \"chuckwalla\", \"cobra\", \"constrictor\", \"cornsnake\", \"cottonmouth\", \"crocodile\", \"diamondback\", \"dinosaur\", \"dragon\", \"frog\", \"garden snake\", \"gecko\", \"godzilla\", \"iguana\", \"kingsnake\", \"komododragon\", \"lizard\", \"milksnake\", \"moccasin\", \"newt\", \"pit viper\", \"python\", \"ratsnake\", \"rattlesnake\", \"reptile\", \"salamander\", \"serpent\", \"sidewinder\", \"skink\", \"snake\", \"sungazer\", \"toad\", \"tomistoma\", \"tortoise\", \"tree frog\", \"turtle\", \"velociraptor\", \"viper\", \"watersnake\"],\n",
    "#     20: [\"agouti\", \"beaver\", \"blacksquirrel\", \"capybara\", \"cavy\", \"chinchilla\", \"chipmunk\", \"degu\", \"dormouse\", \"flyingsquirrel\", \"gerbil\", \"goldenmarmot\", \"gopher\", \"groundhog\", \"guineapig\", \"hamster\", \"hedgehog\", \"lemming\", \"marmot\", \"mole\", \"mouse\", \"muskrat\", \"naked mole rat\", \"phascogale\", \"porcupine\", \"potoroo\", \"prairie dog\", \"rat\", \"rodent\", \"shrew\", \"springhare\", \"squirrel\", \"vole\", \"woodchuck\", \"woylie\"],\n",
    "#     21: [\"alga\", \"alligator\", \"anemone\", \"axolotl\", \"baiji\", \"balloonfish\", \"beaver\", \"beluga\", \"blacktipreefshark\", \"bluewhale\", \"boto\", \"brineshrimp\", \"cavefish\", \"cichlids\", \"clam\", \"conch\", \"coral\", \"cowry\", \"crab\", \"crawfish\", \"crayfish\", \"dogfish\", \"dolphin\", \"dragonet\", \"dugong\", \"eel\", \"elephant seal\", \"filefish\", \"fish\", \"frog\", \"goose\", \"hammerhead shark\", \"humpback whale\", \"jellyfish\", \"killer whale\", \"knifefish\", \"leech\", \"limpet\", \"lionfish\", \"lobster\", \"lookdown\", \"manatee\", \"mantaray\", \"moray\", \"murex\", \"muskrat\", \"mussel\", \"narwhal\", \"nautilus\", \"newt\", \"octopus\", \"orca\", \"otter\", \"oyster\", \"pacu\", \"penguin\", \"platypus\", \"pleco\", \"porkfish\", \"porpoise\", \"prawn\", \"pufferfish\", \"ray\", \"sailfish\", \"salamander\", \"sanddollar\", \"sawfish\", \"scallop\", \"scorpionfish\", \"sealion\", \"seamonkey\", \"seahorse\", \"seal\", \"shark\", \"slug\", \"snail\", \"sponge\", \"squid\", \"starfish\", \"stingray\", \"stonefish\", \"tadpole\", \"tang\", \"tetra\", \"toad\", \"triplefin\", \"turtle\", \"urchin\", \"vaquita\", \"whale\"],\n",
    "#     22: [\"badger\", \"ferret\", \"groundhog\", \"marten\", \"mink\", \"mongoose\", \"otter\", \"polecat\", \"seaotter\", \"skunk\", \"stoat\", \"weasel\", \"wolverine\"]\n",
    "# }\n",
    "\n",
    "# animal_to_clusters = {}\n",
    "# for key, value_list in cluster_to_animals.items():\n",
    "#     for value in value_list:\n",
    "#         if value not in animal_to_clusters:\n",
    "#             animal_to_clusters[value] = [key]\n",
    "#         else:\n",
    "#             animal_to_clusters[value].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../csvs/data.csv\")\n",
    "# df[\"pid\"] = pd.factorize(df['sid'])[0]\n",
    "# df[\"clusters\"] = df[\"entry\"].map(animal_to_clusters)\n",
    "# df['previous_entry'] = df['entry'].shift(1)\n",
    "# df[\"order\"] = np.concatenate([np.arange(i) for i in df.groupby(\"pid\").count()[\"entry\"].tolist()])    # order variable per ppt x task. Starts from 0\n",
    "# df.loc[df[\"order\"] == 0, \"previous_entry\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animals = df[\"entry\"].unique().tolist()\n",
    "# filename = \"sentence_transformer_embeddings_animals_gtelarge\"\n",
    "# try:\n",
    "#     sentence_transformer_embeddings = pk.load(open(f\"../embeddings/{filename}.pk\", \"rb\"))\n",
    "# except:\n",
    "#     sentence_transformer_embeddings = get_sentence_transformer_embeddings(animals)\n",
    "#     pk.dump(sentence_transformer_embeddings, open(f\"../embeddings/{filename}.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_categories = 22\n",
    "# num_participants = len(df[\"sid\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,3, figsize=(10,3))\n",
    "# plot_hist(df.groupby(\"pid\").count()[\"entry\"], \"Number of Responses\", \"Number of Participants\", ax[0])\n",
    "# plot_hist(df.groupby(\"pid\").sum()[\"flastitem\"].to_numpy()/df.groupby(\"pid\").count()[\"entry\"].to_numpy(), \"Percentage Jumps\", \"Number of Participants\",  ax[1])\n",
    "\n",
    "# def mean_depth(group):\n",
    "#     reset_points = list(group[group['fpatchitem'] == 1].index) + [int(group.index[-1]) + 1]\n",
    "#     segment_lengths = np.array(pd.Series(reset_points).diff().tolist()[1:])\n",
    "#     return segment_lengths.mean()\n",
    "# plot_hist(df.groupby('pid').apply(mean_depth).tolist(), \"Mean Depth\", \"Number of Participants\",  ax[2]);\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df[\"clusters\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 3, figsize=(25, 5))\n",
    "# response_to_cluster, cluster_to_response, num_clusters, minsim = perform_hierarchicalClustering([sentence_transformer_embeddings[x] for x in animals], animals, ax, 1, True)\n",
    "# print(\"Num clusters =\", num_clusters)\n",
    "# print(np.min(list(minsim.values())), np.mean(list(minsim.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_category_SS_jump_filterorder0(data):\n",
    "#     # write category using response_to_cluster\n",
    "#     data[\"category\"] = data[\"entry\"].apply(lambda x: response_to_cluster.get(x))\n",
    "\n",
    "#     data['SS'] = data.apply(lambda row: calculate_cosine_similarity(sentence_transformer_embeddings.get(row['entry']), sentence_transformer_embeddings.get(row['previous_entry'])), axis=1)\n",
    "    \n",
    "#     data[\"category\"] = data[\"category\"].astype(int)\n",
    "#     data[\"jump_cat\"] = (~(data[\"category\"].diff() == 0)).astype(int)\n",
    "#     data[\"jump_SS\"] = (data[\"SS\"] < 0.81).astype(int)\n",
    "#     # data['jump'] = data[\"jump_cat\"] & data[\"jump_SS\"]\n",
    "#     data['jump'] = data[\"jump_cat\"]\n",
    "\n",
    "\n",
    "#     data = data[data[\"order\"] > 0]      # remove the first response\n",
    "\n",
    "#     return data\n",
    "\n",
    "# df = write_category_SS_jump_filterorder0(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(10, 3, figsize=(13,40))\n",
    "# for pid in range(30):\n",
    "#     plot_movement(df[df[\"pid\"] == pid][\"category\"].tolist(), pid, ax[pid//3, pid%3])\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_humans = pd.read_csv(\"../csvs/data_humans_allresponses.csv\")\n",
    "data_humans = data_humans.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "cluster_to_response_autbrick = pk.load(open(\"../pickle/cluster_to_response_autbrick.pk\", 'rb'))\n",
    "num_clusters_autbrick = len(cluster_to_response_autbrick)\n",
    "sentence_transformer_embeddings_autbrick = pk.load(open(\"../embeddings/sentence_transformer_embeddings_autbrick_gtelarge.pk\", 'rb'))\n",
    "cluster_to_response_autpaperclip = pk.load(open(\"../pickle/cluster_to_response_autpaperclip.pk\", 'rb'))\n",
    "num_clusters_autpaperclip = len(cluster_to_response_autpaperclip)\n",
    "sentence_transformer_embeddings_autpaperclip = pk.load(open(\"../embeddings/sentence_transformer_embeddings_autpaperclip_gtelarge.pk\", 'rb'))\n",
    "cluster_to_response_vf = pk.load(open(\"../pickle/cluster_to_response_vf.pk\", 'rb'))\n",
    "num_clusters_vf = len(cluster_to_response_vf)\n",
    "sentence_transformer_embeddings_vf = pk.load(open(\"../embeddings/sentence_transformer_embeddings_vf_gtelarge.pk\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate think times\n",
    "\n",
    "data_humans[\"response_length\"] = data_humans[\"original_response_Dutch\"].str.len()\n",
    "\n",
    "grouped = data_humans.groupby('pid')\n",
    "\n",
    "def calculate_slope(group):\n",
    "    RT = group['RT'].values\n",
    "    length = group['response_length'].values\n",
    "    slope, intercept = np.polyfit(RT, length, 1)\n",
    "    return slope\n",
    "\n",
    "data_humans['slope'] = data_humans['pid'].map(grouped.apply(calculate_slope))\n",
    "data_humans[\"RT_think\"] = data_humans[\"RT\"] - data_humans[\"response_len\"] * data_humans['slope']\n",
    "data_humans[\"RT_think_transformed\"] = data_humans.groupby([\"pid\", \"task\"])[\"RT_think\"].transform(lambda x: x - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_humans[\"RT_think_transformed\"], bins=100)\n",
    "plt.xlim(0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever sim caused stay, take previous clister as the cluster\n",
    "\n",
    "data_humans[\"category_simaccounted\"] = data_humans[\"category\"]\n",
    "\n",
    "grouped = data_humans.groupby([\"task\", \"pid\"])\n",
    "\n",
    "def apply_ffill(group):\n",
    "    group = group.reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        if group['jump'].iloc[i] == 0:\n",
    "            group['category_simaccounted'].iloc[i] = group['category_simaccounted'].iloc[i-1]\n",
    "    return group\n",
    "\n",
    "data_humans = grouped.apply(apply_ffill).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_humans.loc[data_humans[\"task\"] == 2, 'SS_ref'] = data_humans[data_humans[\"task\"] == 2].apply(lambda row: calculate_cosine_similarity(sentence_transformer_embeddings_autbrick.get(row['response']), embeddings_ref_aut.get(\"brick\")), axis=1)\n",
    "# data_humans.loc[data_humans[\"task\"] == 3, 'SS_ref'] = data_humans[data_humans[\"task\"] == 3].apply(lambda row: calculate_cosine_similarity(sentence_transformer_embeddings_autpaperclip.get(row['response']), embeddings_ref_aut.get(\"paperclip\")), axis=1)\n",
    "# data_humans.loc[data_humans[\"task\"] == 1, 'SS_ref'] = data_humans[data_humans[\"task\"] == 1].apply(lambda row: calculate_cosine_similarity(sentence_transformer_embeddings_vf.get(row['response']), embeddings_ref_vf.get(\"animal\")), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent unique clusters\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12,3))\n",
    "ax[0].hist(data_humans[data_humans[\"task\"] == 2][[\"pid\", \"category_simaccounted\", \"jump\"]].groupby(\"pid\").nunique()[\"category_simaccounted\"].values / (data_humans[data_humans[\"task\"] == 2][[\"pid\", \"category_simaccounted\", \"jump\"]].groupby(\"pid\").sum()[\"jump\"].values + 1) * 100, color=\"teal\")\n",
    "ax[0].set_xlabel(\"Percentage Unique Clusters\")\n",
    "ax[0].set_ylabel(\"Number of Participants\");\n",
    "\n",
    "ax[1].hist(data_humans[data_humans[\"task\"] == 3][[\"pid\", \"category_simaccounted\", \"jump\"]].groupby(\"pid\").nunique()[\"category_simaccounted\"] / (data_humans[data_humans[\"task\"] == 3][[\"pid\", \"category_simaccounted\", \"jump\"]].groupby(\"pid\").sum()[\"jump\"] + 1) * 100, color=\"teal\")\n",
    "ax[1].set_xlabel(\"Percentage Unique Clusters\")\n",
    "ax[1].set_ylabel(\"Number of Participants\");\n",
    "\n",
    "ax[2].hist(data_humans[data_humans[\"task\"] == 1][[\"pid\", \"category_simaccounted\", \"jump\"]].groupby(\"pid\").nunique()[\"category_simaccounted\"] / (data_humans[data_humans[\"task\"] == 1][[\"pid\", \"category_simaccounted\", \"jump\"]].groupby(\"pid\").sum()[\"jump\"] + 1) * 100, color=\"teal\")\n",
    "ax[2].set_xlabel(\"Percentage Unique Clusters\")\n",
    "ax[2].set_ylabel(\"Number of Participants\");\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the clusters\n",
    "\n",
    "# k = 32\n",
    "# print(len(cluster_to_response_autbrick[k]))\n",
    "# cluster_to_response_autbrick[k]\n",
    "\n",
    "# AUT brick -\n",
    "# 1: \"47: oven, pizza, heating - good\"\n",
    "# 2: \"52: heating, cooling - good\"\n",
    "# 3: \"36: fireplace, chimney, hearth - good\"\n",
    "# 4: \"76: barbeque, grill, campfire - good\"\n",
    "# 5: \"119: house, roof, building, apartment - good\"\n",
    "# 6: \"54: brickyard, block, brickandmortar - good but confounded with toy\"\n",
    "# 7: \"63: rain, dam , pond, water - good\"\n",
    "# 8: \"69: animal shelter, weapon against animals - good all about animals but may mean different things\"\n",
    "# 9: \"87: fences, demarcation, barrier, shielding, wall - ok, some other random ones, confounded with cover\"\n",
    "# 10: \"116: household furniture - doorstopper, shelf, bottle, bookend, bookshelf, closet - not too good\"\n",
    "# 11: \"74: plants, flowers, vase, pot - good\"\n",
    "# 12: \"46: street, sidewalk, pathway, road - good\"\n",
    "# 13: \"120: path, outdoor, patio, garden - not so good\"\n",
    "# 14: \"54: exercise weight, lifting, training weight - good\"\n",
    "# 15: \"121: \"weight other, anchor - okay\"\n",
    "# 16: \"81: \"throw, ball, game - confounded with game\"\n",
    "# 17: \"31: everything paper - good\"\n",
    "# 18: \"93: game, toy, puzzle, personify - good\"\n",
    "# 19: \"45: break, smash, throw, window - okay\"\n",
    "# 20: \"98: fight, kill, injury, murder, weapon, throw in anger - goodish\"\n",
    "# 21: \"164: \"smash, grind, crack, press, crush - quite noisy, not so good\"\n",
    "# 22: \"89: decoration, ornaments, art - some noise, mostly okay\"\n",
    "# 23: \"68: art, painting, colour - good\"\n",
    "# 24: \"104: \"stool, elevation, support, stairs - good\"\n",
    "# 25: \"110: furniture, raise leg, table, bench, chair - okayish\" \n",
    "# 26: \"89: very noisy\"\n",
    "# 27: \"244: very noisy\"\n",
    "# 28: \"85: engrave, mark, draw, scratch, write - noisy\"\n",
    "# 29: \"55: stones, pebbles, statue, scultpure\"\n",
    "# 30: \"390: very noisy\" \n",
    "# 31: \"81: base, holder, stand - noisy\"\n",
    "# 32: \"120: noisy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "print(len(cluster_to_response_autpaperclip[k]))\n",
    "cluster_to_response_autpaperclip[k]\n",
    "\n",
    "# 1: \"124: marker, write - noisy\"\n",
    "# 2: \"171: binder, holder - okayish\"\n",
    "# 3: \"82: has word clip or paper clip - bad cluster\"\n",
    "# 4: \"99: together, bind, connect - good\"\n",
    "# 5: \"71: scratch, itch - good\"\n",
    "# 6: \"51: nail of both meanings - good\"\n",
    "# 7: \"48: glasses, eye glass - good\"\n",
    "# 8: \"57: device reset, button - confouned with button of reset button and shirt button\"\n",
    "# 9: \"231: very noisy\"\n",
    "# 10: \"47: thread and needle - good\"\n",
    "# 11: \"107: poke, prick, puncture, pop, make holes - good\"\n",
    "# 12: \"179: art, scultpure, shapes, paint, carve, write - good\"\n",
    "# 13: \"32: magnetic property - good\"\n",
    "# 14: \"52: fishing, hook - good\"\n",
    "# 15: \"45: guitar, instrument, music - good\"\n",
    "# 16: \"138: food, kitchen - okqyish\"\n",
    "# 17: \"716: very noisy\"\n",
    "# 18: \"95: unlock, lock pick - good\"\n",
    "# 19: \"58: key, keyboard, piano keys - noisy\"\n",
    "# 20: \"80: zip - good\"\n",
    "# 21: \"145: seal, close, opener - confounded close and open\"\n",
    "# 22: \"85: jewelry - good\"\n",
    "# 23: \"77: ear related: earring, headphones, earwax, piercing - good\"\n",
    "# 24: \"92: photo hanger, frame - good\"\n",
    "# 25: \"90: ornament, christmas, decoration - okayish\"\n",
    "# 26: \"55: plant, flowers - good\"\n",
    "# 27: \"76: hair related, head - good\"\n",
    "# 28: \"132: clothes, fabric - good\"\n",
    "# 29: \"343: very noisy\"\n",
    "# 30: \"108: chain, string, rope, wire - noisy\"\n",
    "# 31: \"135: electronics related - phone, car, telephone - noisy\"\n",
    "# 32: \"64: cables, cords - good\"\n",
    "# 33: \"117: wire, electricity, conduction, soldering - okayish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 10\n",
    "# print(len(cluster_to_response_vf[k]))\n",
    "# cluster_to_response_vf[k]\n",
    "\n",
    "# 1: \"\"\n",
    "# 2: \"cats all kinds - good\"\n",
    "# 3: \"rhinoceros and beetle? - not good\"\n",
    "# 4: \"hippopotamus\"\n",
    "#\n",
    "# 6: \"horse?\"\n",
    "\n",
    "# 8: \"23: primates\"\n",
    "# 9: \"18: dogs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster sizes\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12,3))\n",
    "ax[0].hist([len(cluster_to_response_autbrick[l]) for l in cluster_to_response_autbrick], color=\"teal\")\n",
    "ax[0].set_xlabel(\"Size of cluster\")\n",
    "ax[0].set_ylabel(\"Number of clusters\")\n",
    "\n",
    "ax[1].hist([len(cluster_to_response_autpaperclip[l]) for l in cluster_to_response_autpaperclip], color=\"teal\")\n",
    "ax[1].set_xlabel(\"Size of cluster\")\n",
    "ax[1].set_ylabel(\"Number of clusters\")\n",
    "\n",
    "ax[2].hist([len(cluster_to_response_vf[l]) for l in cluster_to_response_vf], color=\"teal\")\n",
    "ax[2].set_xlabel(\"Size of cluster\")\n",
    "ax[2].set_ylabel(\"Number of clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_transition_matrix(data, num_clusters):\n",
    "    cluster_transition_matrix = np.zeros((num_clusters, num_clusters))\n",
    "    for [cl1, cl2] in list(zip(data[\"previous_category\"], data[\"category\"])):\n",
    "        try:\n",
    "            cluster_transition_matrix[int(cl1) - 1, int(cl2) - 1] += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    row_sums = cluster_transition_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    # Avoid division by zero by adding a small number (epsilon) to row_sums\n",
    "    epsilon = 1e-10\n",
    "    return cluster_transition_matrix / (row_sums + epsilon)\n",
    "    # return cluster_transition_matrix/cluster_transition_matrix.sum(axis = 1)\n",
    "\n",
    "data_humans['previous_category'] = data_humans.groupby(['pid', 'task'])['category'].shift(1)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 4))\n",
    "sns.heatmap(get_cluster_transition_matrix(data_humans[data_humans[\"task\"] == 2], 32), ax=ax[0])\n",
    "ax[0].set_xlabel(\"Next cluster\")\n",
    "ax[0].set_ylabel(\"Current cluster\")\n",
    "ax[0].set_title(\"AUT Brick\", fontsize=13)\n",
    "sns.heatmap(get_cluster_transition_matrix(data_humans[data_humans[\"task\"] == 3], 33), ax=ax[1])\n",
    "ax[1].set_xlabel(\"Next cluster\")\n",
    "ax[1].set_ylabel(\"Current cluster\")\n",
    "ax[1].set_title(\"AUT Paperclip\", fontsize=13)\n",
    "sns.heatmap(get_cluster_transition_matrix(data_humans[data_humans[\"task\"] == 1], 26), ax=ax[2])\n",
    "ax[2].set_xlabel(\"Next cluster\")\n",
    "ax[2].set_ylabel(\"Current cluster\")\n",
    "ax[2].set_title(\"Verbal Fluency\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_transition_matrix(data, num_clusters):\n",
    "    cluster_transition_matrix = np.zeros((num_clusters, num_clusters))\n",
    "    for [cl1, cl2] in list(zip(data[\"previous_category_simaccounted\"], data[\"category_simaccounted\"])):\n",
    "        try:\n",
    "            cluster_transition_matrix[int(cl1) - 1, int(cl2) - 1] += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    row_sums = cluster_transition_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    # Avoid division by zero by adding a small number (epsilon) to row_sums\n",
    "    epsilon = 1e-10\n",
    "    return cluster_transition_matrix / (row_sums + epsilon)\n",
    "    # return cluster_transition_matrix/cluster_transition_matrix.sum(axis = 1)\n",
    "\n",
    "data_humans['previous_category_simaccounted'] = data_humans.groupby(['pid', 'task'])['category_simaccounted'].shift(1)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 4))\n",
    "sns.heatmap(get_cluster_transition_matrix(data_humans[data_humans[\"task\"] == 2], 32), ax=ax[0])\n",
    "ax[0].set_xlabel(\"Next cluster\")\n",
    "ax[0].set_ylabel(\"Current cluster\")\n",
    "ax[0].set_title(\"AUT Brick\", fontsize=13)\n",
    "sns.heatmap(get_cluster_transition_matrix(data_humans[data_humans[\"task\"] == 3], 33), ax=ax[1])\n",
    "ax[1].set_xlabel(\"Next cluster\")\n",
    "ax[1].set_ylabel(\"Current cluster\")\n",
    "ax[1].set_title(\"AUT Paperclip\", fontsize=13)\n",
    "sns.heatmap(get_cluster_transition_matrix(data_humans[data_humans[\"task\"] == 1], 26), ax=ax[2])\n",
    "ax[2].set_xlabel(\"Next cluster\")\n",
    "ax[2].set_ylabel(\"Current cluster\")\n",
    "ax[2].set_title(\"Verbal Fluency\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster similarity matrix:\n",
    "\n",
    "# method 1 - median pairwise similarity between 2 clusters\n",
    "\n",
    "cluster_sim_mat_autbrick = np.zeros((num_clusters_autbrick, num_clusters_autbrick))\n",
    "for cl1 in cluster_to_response_autbrick:\n",
    "    for cl2 in cluster_to_response_autbrick:\n",
    "        r1_embeddings = np.array([sentence_transformer_embeddings_autbrick[r1] for r1 in cluster_to_response_autbrick[cl1]])\n",
    "        r2_embeddings = np.array([sentence_transformer_embeddings_autbrick[r2] for r2 in cluster_to_response_autbrick[cl2]])\n",
    "        similarity_matrix = np.dot(r1_embeddings, r2_embeddings.T)\n",
    "        cluster_sim_mat_autbrick[cl1 - 1, cl2 - 1] = np.median(similarity_matrix)\n",
    "\n",
    "cluster_sim_mat_autpaperclip = np.zeros((num_clusters_autpaperclip, num_clusters_autpaperclip))\n",
    "for cl1 in cluster_to_response_autpaperclip:\n",
    "    for cl2 in cluster_to_response_autpaperclip:\n",
    "        r1_embeddings = np.array([sentence_transformer_embeddings_autpaperclip[r1] for r1 in cluster_to_response_autpaperclip[cl1]])\n",
    "        r2_embeddings = np.array([sentence_transformer_embeddings_autpaperclip[r2] for r2 in cluster_to_response_autpaperclip[cl2]])\n",
    "        similarity_matrix = np.dot(r1_embeddings, r2_embeddings.T)\n",
    "        cluster_sim_mat_autpaperclip[cl1 - 1, cl2 - 1] = np.median(similarity_matrix)\n",
    "\n",
    "cluster_sim_mat_vf = np.zeros((num_clusters_vf, num_clusters_vf))\n",
    "for cl1 in cluster_to_response_vf:\n",
    "    for cl2 in cluster_to_response_vf:\n",
    "        r1_embeddings = np.array([sentence_transformer_embeddings_vf[r1] for r1 in cluster_to_response_vf[cl1]])\n",
    "        r2_embeddings = np.array([sentence_transformer_embeddings_vf[r2] for r2 in cluster_to_response_vf[cl2]])\n",
    "        similarity_matrix = np.dot(r1_embeddings, r2_embeddings.T)\n",
    "        cluster_sim_mat_vf[cl1 - 1, cl2 - 1] = np.median(similarity_matrix)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 4))\n",
    "sns.heatmap(cluster_sim_mat_autbrick, annot=False, ax=ax[0])\n",
    "ax[0].set_xlabel(\"Next cluster\")\n",
    "ax[0].set_ylabel(\"Current cluster\")\n",
    "sns.heatmap(cluster_sim_mat_autpaperclip, annot=False, ax=ax[1])\n",
    "ax[1].set_xlabel(\"Next cluster\")\n",
    "ax[1].set_ylabel(\"Current cluster\")\n",
    "sns.heatmap(cluster_sim_mat_vf, annot=False, ax=ax[2]);\n",
    "ax[2].set_xlabel(\"Next cluster\")\n",
    "ax[2].set_ylabel(\"Current cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_similarity_matrix(similarity_matrix, min_val=0, max_val=10):\n",
    "    # Normalize the matrix to a 0-1 range\n",
    "    min_sim = np.min(similarity_matrix)\n",
    "    max_sim = np.max(similarity_matrix)\n",
    "    scaled_matrix = (similarity_matrix - min_sim) / (max_sim - min_sim)\n",
    "    \n",
    "    # Scale the matrix to the specified range (0 to 10)\n",
    "    scaled_matrix = scaled_matrix * (max_val - min_val) + min_val\n",
    "    return scaled_matrix\n",
    "\n",
    "scaled_cluster_sim_mat_autbrick = scale_similarity_matrix(cluster_sim_mat_autbrick)\n",
    "scaled_cluster_sim_mat_autpaperclip = scale_similarity_matrix(cluster_sim_mat_autpaperclip)\n",
    "scaled_cluster_sim_mat_vf = scale_similarity_matrix(cluster_sim_mat_vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_sizes = {}\n",
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "i = 0\n",
    "for pid in data_humans[data_humans[\"task\"] == 2][\"pid\"].unique():\n",
    "    marker_sizes = {}\n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < 20:\n",
    "        continue\n",
    "    try:\n",
    "        plot_movement(categ_list[:20], i, ax[i//5, i%5], max(data_humans[data_humans[\"task\"] == 2][\"category_simaccounted\"].values), marker_sizes)\n",
    "        i = i + 1\n",
    "    except:\n",
    "        continue\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_sizes = {}\n",
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "i = 0\n",
    "for pid in data_humans[data_humans[\"task\"] == 3][\"pid\"].unique():\n",
    "    marker_sizes = {}\n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 3) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < 20:\n",
    "        continue\n",
    "    try:\n",
    "        plot_movement(categ_list[:20], i, ax[i//5, i%5], max(data_humans[data_humans[\"task\"] == 3][\"category_simaccounted\"].values), marker_sizes)\n",
    "        i = i + 1\n",
    "    except:\n",
    "        continue\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "i = 0\n",
    "for pid in data_humans[data_humans[\"task\"] == 1][\"pid\"].unique():\n",
    "    marker_sizes = {}\n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 1) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < 20:\n",
    "        continue\n",
    "    try:\n",
    "        plot_movement(categ_list[:20], i, ax[i//5, i%5], max(data_humans[data_humans[\"task\"] == 1][\"category_simaccounted\"].values), marker_sizes)\n",
    "        i = i + 1\n",
    "    except:\n",
    "        continue\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When people jump back, how far back do they jump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeated_cluster_distances_from_end(cluster_sequence):\n",
    "    last_pos = {}\n",
    "    distances = []\n",
    "    \n",
    "    # Iterate through the sequence starting from the end\n",
    "    for i in reversed(range(len(cluster_sequence))):\n",
    "        cluster = cluster_sequence[i]\n",
    "        \n",
    "        if cluster in last_pos and last_pos[cluster] != i + 1:\n",
    "            # Calculate distance if the cluster is not repeated directly\n",
    "            distance = last_pos[cluster] - i\n",
    "            distances.append((cluster, distance, i, last_pos[cluster]))\n",
    "        \n",
    "        # Update the last seen position of the cluster\n",
    "        last_pos[cluster] = i\n",
    "    \n",
    "    return distances\n",
    "\n",
    "rep_lengths_autbrick = []\n",
    "for pid in data_humans[data_humans[\"task\"] == 2][\"pid\"].unique(): \n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < seq_len:\n",
    "        continue\n",
    "    categ_list = categ_list[:seq_len]\n",
    "    # print(categ_list)\n",
    "    distances = find_repeated_cluster_distances_from_end(categ_list)\n",
    "    # print(pid, list(reversed(distances)))\n",
    "    for item in list(reversed(distances)):\n",
    "        rep_lengths_autbrick.append(item[1])\n",
    "\n",
    "rep_lengths_autpaperclip = []\n",
    "for pid in data_humans[data_humans[\"task\"] == 3][\"pid\"].unique(): \n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 3) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < seq_len:\n",
    "        continue\n",
    "    categ_list = categ_list[:seq_len]\n",
    "    # print(categ_list)\n",
    "    distances = find_repeated_cluster_distances_from_end(categ_list)\n",
    "    # print(pid, list(reversed(distances)))\n",
    "    for item in list(reversed(distances)):\n",
    "        rep_lengths_autpaperclip.append(item[1])\n",
    "\n",
    "rep_lengths_vf = []\n",
    "for pid in data_humans[data_humans[\"task\"] == 1][\"pid\"].unique(): \n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 1) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < seq_len:\n",
    "        continue\n",
    "    categ_list = categ_list[:seq_len]\n",
    "    # print(categ_list)\n",
    "    distances = find_repeated_cluster_distances_from_end(categ_list)\n",
    "    # print(pid, list(reversed(distances)))\n",
    "    for item in list(reversed(distances)):\n",
    "        rep_lengths_vf.append(item[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 4))\n",
    "ax[0].hist(rep_lengths_autbrick)\n",
    "ax[0].set_title(\"AUT Brick\")\n",
    "ax[0].set_xlabel(\"Distance jump back\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_xlim(2, seq_len)\n",
    "ax[0].set_ylim(0, 110)\n",
    "\n",
    "ax[1].hist(rep_lengths_autpaperclip)\n",
    "ax[1].set_title(\"AUT Paperclip\")\n",
    "ax[1].set_xlim(2, seq_len)\n",
    "ax[1].set_ylim(0, 110)\n",
    "ax[1].set_xlabel(\"Distance jump back\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "ax[2].hist(rep_lengths_vf)\n",
    "ax[2].set_title(\"Verbal Fluency\")\n",
    "ax[2].set_ylim(0, 110)\n",
    "ax[2].set_xlabel(\"Distance jump back\")\n",
    "ax[2].set_ylabel(\"Frequency\")\n",
    "ax[2].set_xlim(2, seq_len);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When people jump ahead, how much do they exploit it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stay_durations(cluster_sequence):\n",
    "    durations = []\n",
    "    n = len(cluster_sequence)\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        current_cluster = cluster_sequence[start]\n",
    "        end = start\n",
    "        \n",
    "        while end < n and cluster_sequence[end] == current_cluster:\n",
    "            end += 1\n",
    "        \n",
    "        duration = end - start\n",
    "        if duration > 1:\n",
    "            durations.append((current_cluster, duration, start, end - 1))\n",
    "        start = end    \n",
    "    return durations\n",
    "\n",
    "# Example cluster sequence\n",
    "stay_lengths_autbrick = []\n",
    "for pid in data_humans[data_humans[\"task\"] == 2][\"pid\"].unique(): \n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < seq_len:\n",
    "        continue\n",
    "    categ_list = categ_list[:seq_len]\n",
    "    stay_durations = find_stay_durations(categ_list)\n",
    "    # print(stay_durations)\n",
    "    for item in list(reversed(stay_durations)):\n",
    "        stay_lengths_autbrick.append(item[1])\n",
    "\n",
    "stay_lengths_autpaperclip = []\n",
    "for pid in data_humans[data_humans[\"task\"] == 3][\"pid\"].unique(): \n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 3) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < seq_len:\n",
    "        continue\n",
    "    categ_list = categ_list[:seq_len]\n",
    "    stay_durations = find_stay_durations(categ_list)\n",
    "    # print(stay_durations)\n",
    "    for item in list(reversed(stay_durations)):\n",
    "        stay_lengths_autpaperclip.append(item[1])\n",
    "    \n",
    "stay_lengths_vf = []\n",
    "for pid in data_humans[data_humans[\"task\"] == 1][\"pid\"].unique(): \n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 1) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    if len(categ_list) < seq_len:\n",
    "        continue\n",
    "    categ_list = categ_list[:seq_len]\n",
    "    stay_durations = find_stay_durations(categ_list)\n",
    "    # print(stay_durations)\n",
    "    for item in list(reversed(stay_durations)):\n",
    "        stay_lengths_vf.append(item[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(17, 4))\n",
    "ax[0].hist(stay_lengths_autbrick)\n",
    "ax[0].set_title(\"AUT Brick\")\n",
    "ax[0].set_xlim(2, seq_len)\n",
    "ax[0].set_xlabel(\"Length of stay\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].set_ylim(0, 360)\n",
    "ax[1].hist(stay_lengths_autpaperclip)\n",
    "ax[1].set_title(\"AUT Paperclip\")\n",
    "ax[1].set_xlim(2, seq_len)\n",
    "ax[1].set_xlabel(\"Length of stay\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "ax[1].set_ylim(0, 360)\n",
    "ax[2].hist(stay_lengths_vf)\n",
    "ax[2].set_title(\"Verbal Fluency\")\n",
    "ax[2].set_ylim(0, 360)\n",
    "ax[2].set_xlabel(\"Length of stay\")\n",
    "ax[2].set_ylabel(\"Frequency\")\n",
    "ax[2].set_xlim(2, seq_len);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jump vs stays\n",
    "\n",
    "data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0)][[\"jump\", \"RT\", \"RT_think\", \"RT_think_transformed\", \"SS\"]].groupby(\"jump\").mean()\n",
    "\n",
    "# All significant\n",
    "# t_stat, p_val = stats.ttest_ind(data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0) & ((data_humans[\"jump\"] == 0))][\"RT\"], data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0) & ((data_humans[\"jump\"] == 1))][\"RT\"])\n",
    "# print(p_val)\n",
    "# t_stat, p_val = stats.ttest_ind(data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0) & ((data_humans[\"jump\"] == 0))][\"RT_think\"], data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0) & ((data_humans[\"jump\"] == 1))][\"RT_think\"])\n",
    "# print(p_val)\n",
    "# t_stat, p_val = stats.ttest_ind(data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0) & ((data_humans[\"jump\"] == 0))][\"SS\"], data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"order\"] > 0) & ((data_humans[\"jump\"] == 1))][\"SS\"])\n",
    "# print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project this as random walk based on similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "\n",
    "def random_walk(cluster_visits, cluster_sim_mat):\n",
    "    positions = [(0, 0)]  # Start at the origin\n",
    "    current_position = np.array([0, 0])  # Start position\n",
    "    \n",
    "    for i in range(1, len(cluster_visits)):\n",
    "        current_cluster = cluster_visits[i-1]\n",
    "        next_cluster = cluster_visits[i]\n",
    "        \n",
    "        # Get similarity from the matrix\n",
    "        similarity = cluster_sim_mat[current_cluster - 1, next_cluster - 1]\n",
    "        if similarity == 0:  # Handle edge case where similarity might be zero\n",
    "            step_size = np.inf  # Handle division by zero\n",
    "        else:\n",
    "            step_size = 1 / similarity  # Larger similarity = smaller step size\n",
    "\n",
    "        # while True: \n",
    "        random_angle = np.random.uniform(0, 2 * np.pi)\n",
    "        step_vector = np.array([np.cos(random_angle), np.sin(random_angle)]) * step_size\n",
    "        current_position = current_position + step_vector\n",
    "            # if (current_position[0] > 0 and current_position[0] < boundary) and (current_position[1] > 0 and current_position[1] < boundary):\n",
    "            #     break\n",
    "\n",
    "        positions.append(tuple(current_position))\n",
    "    \n",
    "    return positions\n",
    "\n",
    "# Generate the random walk\n",
    "i = 0\n",
    "for pid in data_humans[data_humans[\"task\"] == 2][\"pid\"].unique():\n",
    "    marker_sizes = {}\n",
    "    categ_list = data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"pid\"] == pid)][\"category_simaccounted\"].tolist()\n",
    "    path = random_walk(categ_list, scaled_cluster_sim_mat_autbrick)\n",
    "\n",
    "    # Extract x and y coordinates from the positions\n",
    "    x_coords, y_coords = zip(*path)\n",
    "\n",
    "    # Plot the random walk\n",
    "    try:\n",
    "        ax[i//5, i%5].plot(x_coords, y_coords, marker='o', linestyle='-', markersize=5, color='blue')\n",
    "        ax[i//5, i%5].set_title(f\"Participant {i}\")\n",
    "        # ax[i//5, i%5].set_xlim(-5, 5)\n",
    "        # ax[i//5, i%5].set_ylim(-5, 5)\n",
    "        i += 1\n",
    "    except:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15, 3))\n",
    "\n",
    "ax[0].scatter(data_humans[data_humans[\"task\"] == 2][\"RT\"], data_humans[data_humans[\"task\"] == 2][\"SS\"], s=10, alpha=0.3, c=\"#d3c6f5\")\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(data_humans[data_humans[\"task\"] == 2][\"RT\"], data_humans[data_humans[\"task\"] == 2][\"SS\"])\n",
    "line_x = np.linspace(data_humans[data_humans[\"task\"] == 2][\"RT\"].min(), data_humans[data_humans[\"task\"] == 2][\"RT\"].max(), 100)\n",
    "line_y = slope * line_x + intercept\n",
    "\n",
    "# Plot the regression line\n",
    "ax[0].plot(line_x, line_y, color='orange', label=f'Fit Line: y={slope:.2f}x+{intercept:.2f}')\n",
    "\n",
    "# Add Pearson correlation coefficient to the plot\n",
    "spearman_corr, _ = stats.spearmanr(data_humans[data_humans[\"task\"] == 2][\"RT\"], data_humans[data_humans[\"task\"] == 2][\"SS\"])\n",
    "ax[0].text(np.max(data_humans[data_humans[\"task\"] == 2][\"RT\"])*0.7, np.max(data_humans[data_humans[\"task\"] == 2][\"SS\"])*0.9, f'Spearman r: {spearman_corr:.2f}', fontsize=12)\n",
    "ax[0].set_xlabel(\"RT\")\n",
    "ax[0].set_ylabel(\"SS\");\n",
    "\n",
    "\n",
    "ax[1].scatter(data_humans[data_humans[\"task\"] == 2][\"RT\"], data_humans[data_humans[\"task\"] == 2][\"SS_ref\"], s=10, alpha=0.3, c=\"#d3c6f5\")\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(data_humans[data_humans[\"task\"] == 2][\"RT\"], data_humans[data_humans[\"task\"] == 2][\"SS_ref\"])\n",
    "line_x = np.linspace(data_humans[data_humans[\"task\"] == 2][\"RT\"].min(), data_humans[data_humans[\"task\"] == 2][\"RT\"].max(), 100)\n",
    "line_y = slope * line_x + intercept\n",
    "\n",
    "# Plot the regression line\n",
    "ax[1].plot(line_x, line_y, color='orange', label=f'Fit Line: y={slope:.2f}x+{intercept:.2f}')\n",
    "\n",
    "# Add Pearson correlation coefficient to the plot\n",
    "spearman_corr, _ = stats.spearmanr(data_humans[data_humans[\"task\"] == 2][\"RT\"], data_humans[data_humans[\"task\"] == 2][\"SS_ref\"])\n",
    "ax[1].text(np.max(data_humans[data_humans[\"task\"] == 2][\"RT\"])*0.7, np.max(data_humans[data_humans[\"task\"] == 2][\"SS_ref\"])*0.9, f'Spearman r: {spearman_corr:.2f}', fontsize=12)\n",
    "ax[1].set_xlabel(\"RT\")\n",
    "ax[1].set_ylabel(\"SS_ref\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = data_humans.select_dtypes(include=[np.number]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(20, 20))\n",
    "corr = data_humans[data_humans[\"task\"] == 2][numeric_cols].corr(method='spearman').round(decimals=2)\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"Purples\", mask=matrix, ax = ax[0])\n",
    "ax[0].set_title('AUT Brick', fontsize=20)\n",
    "\n",
    "corr = data_humans[data_humans[\"task\"] == 3][numeric_cols].corr(method='spearman').round(decimals=2)\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"Purples\", mask=matrix, ax = ax[1])\n",
    "ax[1].set_title('AUT Paperclip', fontsize=20)\n",
    "\n",
    "corr = data_humans[data_humans[\"task\"] == 1][numeric_cols].corr(method='spearman').round(decimals=2)\n",
    "matrix = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"Purples\", mask=matrix, ax = ax[2])\n",
    "ax[2].set_title('Verbal Fluency', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for pid, (index, group) in enumerate(data_humans[data_humans[\"task\"] == 2].groupby(\"pid\").agg(list)[[\"RT_think\", \"SS\", \"jump\", \"response\"]].iterrows()):\n",
    "    group[\"RT_think\"] = group[\"RT_think\"][1:]\n",
    "    group[\"SS\"] = group[\"SS\"][1:]\n",
    "    group[\"jump\"] = group[\"jump\"][1:]\n",
    "    group[\"response\"] = group[\"response\"][1:]\n",
    "\n",
    "    if 0 in group[\"jump\"]:\n",
    "        # Plot RT values for the current group\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "        ax[0].plot(group[\"RT_think\"], label=f'PID {pid}', linestyle='-', marker='', markersize=10, c=\"mediumpurple\")\n",
    "        # Check corresponding 'jump' values and plot scatter points where 'jump' is 1\n",
    "        for i, (RT, jump) in enumerate(zip(group[\"RT_think\"], group[\"jump\"])):\n",
    "            if jump == 1:\n",
    "                ax[0].scatter(i, RT, color='black')  # Scatter plot on the markers\n",
    "        for i, word in enumerate(group[\"response\"]):\n",
    "            ax[0].text(i + 0.1, group[\"RT_think\"][i], word, ha='left', va='center')\n",
    "        ax[0].set_ylabel(\"RT_think\")\n",
    "\n",
    "        ax[1].plot(group[\"SS\"], label=f'PID {pid}', linestyle='-', marker='', markersize=10, c=\"mediumpurple\")\n",
    "        # Check corresponding 'jump' values and plot scatter points where 'jump' is 1\n",
    "        for i, (SS, jump) in enumerate(zip(group[\"SS\"], group[\"jump\"])):\n",
    "            if jump == 1:\n",
    "                ax[1].scatter(i, SS, color='black')  # Scatter plot on the markers\n",
    "        for i, word in enumerate(group[\"response\"]):\n",
    "            ax[1].text(i + 0.1, group[\"SS\"][i], word, ha='left', va='center')\n",
    "        ax[1].set_ylabel(\"SS\")\n",
    "        # ax[1].set_ylim(0,1)\n",
    "\n",
    "        plt.show()\n",
    "        cnt += 1\n",
    "        if cnt == 30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_humans[\"RT_jump\"] = 1\n",
    "\n",
    "def find_RT_cluster_indices(rts, tolerance=1):\n",
    "    clusters = []\n",
    "    current_cluster = [0]  # Start with the first RT\n",
    "    for i in range(1, len(rts)):\n",
    "        if abs(rts[i] - rts[i-1]) <= tolerance or rts[i] < rts[i-1]:\n",
    "            current_cluster.append(i)\n",
    "        else:\n",
    "            if len(current_cluster) > 1:\n",
    "                clusters.append(current_cluster)\n",
    "            current_cluster = [i]\n",
    "    if len(current_cluster) > 1:\n",
    "        clusters.append(current_cluster)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "for pid in data_humans[data_humans[\"task\"] == 2][\"pid\"].unique(): \n",
    "    RT_list = data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"pid\"] == pid)][\"RT_think\"].tolist()\n",
    "    RT_clusters = find_RT_cluster_indices(RT_list)\n",
    "    RT_jump = np.ones(len(data_humans[(data_humans[\"task\"] == 2) & (data_humans[\"pid\"] == pid)]))\n",
    "    for item in RT_clusters:\n",
    "        RT_jump[item[1:]] = 0\n",
    "    data_humans.loc[(data_humans[\"pid\"] == pid) & (data_humans[\"task\"] == 2), \"RT_jump\"] = RT_jump\n",
    "\n",
    "for pid in data_humans[data_humans[\"task\"] == 3][\"pid\"].unique(): \n",
    "    RT_list = data_humans[(data_humans[\"task\"] == 3) & (data_humans[\"pid\"] == pid)][\"RT_think\"].tolist()\n",
    "    RT_clusters = find_RT_cluster_indices(RT_list)\n",
    "    RT_jump = np.ones(len(data_humans[(data_humans[\"task\"] == 3) & (data_humans[\"pid\"] == pid)]))\n",
    "    for item in RT_clusters:\n",
    "        RT_jump[item[1:]] = 0\n",
    "    data_humans.loc[(data_humans[\"pid\"] == pid) & (data_humans[\"task\"] == 3), \"RT_jump\"] = RT_jump\n",
    "\n",
    "for pid in data_humans[data_humans[\"task\"] == 1][\"pid\"].unique(): \n",
    "    RT_list = data_humans[(data_humans[\"task\"] == 1) & (data_humans[\"pid\"] == pid)][\"RT_think\"].tolist()\n",
    "    RT_clusters = find_RT_cluster_indices(RT_list)\n",
    "    RT_jump = np.ones(len(data_humans[(data_humans[\"task\"] == 1) & (data_humans[\"pid\"] == pid)]))\n",
    "    for item in RT_clusters:\n",
    "        RT_jump[item[1:]] = 0\n",
    "    data_humans.loc[(data_humans[\"pid\"] == pid) & (data_humans[\"task\"] == 1), \"RT_jump\"] = RT_jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(data_humans[data_humans[\"task\"] == 2][\"jump\"], data_humans[data_humans[\"task\"] == 2][\"RT_jump\"])\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate TPR, FPR, TNR, FNR\n",
    "TPR = TP / (TP + FN)  # Sensitivity, Recall\n",
    "FPR = FP / (FP + TN)\n",
    "TNR = TN / (TN + FP)  # Specificity\n",
    "FNR = FN / (TP + FN)\n",
    "\n",
    "print(f\"True Positive Rate (TPR): {TPR:.2f}\")\n",
    "print(f\"True Negative Rate (TNR): {TNR:.2f}\")\n",
    "print(f\"False Positive Rate (FPR): {FPR:.2f}\")\n",
    "print(f\"False Negative Rate (FNR): {FNR:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2) & (data_humans[\"jump_cat\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2) & (data_humans[\"jump_SS\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2) & (data_humans[\"jump\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2) & (data_humans[\"RT_jump\"] == 0)][\"RT_think\"]),\n",
    ")\n",
    "\n",
    "print(np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 3) & (data_humans[\"jump_cat\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 3) & (data_humans[\"jump_SS\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 3) & (data_humans[\"jump\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 3) & (data_humans[\"RT_jump\"] == 0)][\"RT_think\"]),\n",
    ")\n",
    "\n",
    "print(np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 1) & (data_humans[\"jump_cat\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 1) & (data_humans[\"jump_SS\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 1) & (data_humans[\"jump\"] == 0)][\"RT_think\"]),\n",
    "np.mean(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 1) & (data_humans[\"RT_jump\"] == 0)][\"RT_think\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of switches across order\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,3))\n",
    "ax[0].plot(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2)][[\"order\", \"jump_cat\"]].groupby(\"order\").mean()[\"jump_cat\"].tolist()[0:20])\n",
    "ax[0].set_ylabel(\"Mean Number of Jumps\")\n",
    "ax[1].plot(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2)][[\"order\", \"jump_SS\"]].groupby(\"order\").mean()[\"jump_SS\"].tolist()[0:20])\n",
    "ax[1].set_xlabel(\"Order\")\n",
    "ax[2].plot(data_humans[(data_humans[\"order\"] > 0) & (data_humans[\"task\"] == 2)][[\"order\", \"jump\"]].groupby(\"order\").mean()[\"jump\"].tolist()[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example list of cluster visits with consecutive clusters (e.g., 6, 6, 6)\n",
    "cluster_visits = [3, 4, 5, 6, 6, 6, 3, 4, 1, 2]\n",
    "\n",
    "# Function to generate random walk with circles around consecutive same clusters\n",
    "def random_walk_with_circles(cluster_visits, cluster_sim_mat):\n",
    "    positions = [(0, 0)]  # Start at the origin\n",
    "    current_position = np.array([0, 0])  # Start position\n",
    "    same_cluster_positions = []  # To store positions for consecutive clusters\n",
    "    \n",
    "    for i in range(1, len(cluster_visits)):\n",
    "        current_cluster = cluster_visits[i-1]\n",
    "        next_cluster = cluster_visits[i]\n",
    "        \n",
    "        # Get similarity from the matrix\n",
    "        similarity = cluster_sim_mat[current_cluster, next_cluster]\n",
    "        if similarity == 0:  # Handle edge case where similarity might be zero\n",
    "            step_size = np.inf  # Handle division by zero\n",
    "        else:\n",
    "            step_size = 1 / similarity  # Larger similarity = smaller step size\n",
    "\n",
    "        # Generate a random angle to determine direction\n",
    "        random_angle = np.random.uniform(0, 2 * np.pi)\n",
    "        \n",
    "        # Determine the step direction in x and y\n",
    "        step_vector = np.array([np.cos(random_angle), np.sin(random_angle)]) * step_size\n",
    "        \n",
    "        # Update current position\n",
    "        current_position = current_position + step_vector\n",
    "        positions.append(tuple(current_position))\n",
    "        \n",
    "        # Track consecutive clusters\n",
    "        if current_cluster == next_cluster:\n",
    "            same_cluster_positions.append(tuple(current_position))\n",
    "\n",
    "    return positions, same_cluster_positions\n",
    "\n",
    "# Generate the random walk and identify consecutive clusters\n",
    "path, same_cluster_positions = random_walk_with_circles(cluster_visits, scaled_cluster_sim_mat_autbrick)\n",
    "\n",
    "# Extract x and y coordinates from the positions\n",
    "x_coords, y_coords = zip(*path)\n",
    "\n",
    "# Plot the random walk\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_coords, y_coords, marker='o', linestyle='-', markersize=5, color='blue')\n",
    "plt.title(\"Random Walk with Circles Around Consecutive Same Clusters\")\n",
    "plt.xlabel(\"X position\")\n",
    "plt.ylabel(\"Y position\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Draw circles around the points that belong to the same cluster consecutively\n",
    "for pos in same_cluster_positions:\n",
    "    circle = plt.Circle(pos, 0.5, color='red', fill=False, linestyle='--', linewidth=2)\n",
    "    plt.gca().add_artist(circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_means_autbrick = np.array([np.mean([sentence_transformer_embeddings_autbrick[resp] for resp in cluster_to_response_autbrick[i]], axis=0) for i in range(1, len(cluster_to_response_autbrick) + 1)])\n",
    "# cluster_stds_autbrick = np.array([np.std([sentence_transformer_embeddings_autbrick[resp] for resp in cluster_to_response_autbrick[i]], axis=0) for i in range(1, len(cluster_to_response_autbrick) + 1)])\n",
    "\n",
    "# cluster_means_autpaperclip = np.array([np.mean([sentence_transformer_embeddings_autpaperclip[resp] for resp in cluster_to_response_autpaperclip[i]], axis=0) for i in range(1, len(cluster_to_response_autpaperclip) + 1)])\n",
    "# cluster_stds_autpaperclip = np.array([np.std([sentence_transformer_embeddings_autpaperclip[resp] for resp in cluster_to_response_autpaperclip[i]], axis=0) for i in range(1, len(cluster_to_response_autpaperclip) + 1)])\n",
    "\n",
    "# cluster_means_vf = np.array([np.mean([sentence_transformer_embeddings_vf[resp] for resp in cluster_to_response_vf[i]], axis=0) for i in range(1, len(cluster_to_response_vf) + 1)])\n",
    "# cluster_stds_vf = np.array([np.std([sentence_transformer_embeddings_vf[resp] for resp in cluster_to_response_vf[i]], axis=0) for i in range(1, len(cluster_to_response_vf) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "k = 0\n",
    "\n",
    "for pid, group in data_humans[data_humans[\"task\"] == 2].groupby('pid'):\n",
    "    if len(group) < 20:\n",
    "        continue  \n",
    "    \n",
    "    try:\n",
    "\n",
    "        vectors = np.array([sentence_transformer_embeddings_autbrick[resp] for resp in group[\"response\"].values])\n",
    "        aggregated = group.groupby(\"category\").agg(list)\n",
    "        # Filter out categories where the number of responses is greater than 1\n",
    "        filtered_aggregated = aggregated[aggregated[\"response\"].apply(len) > 1]\n",
    "        single_points = np.array([np.mean([sentence_transformer_embeddings_autbrick[resp] for resp in l], axis=0) for l in filtered_aggregated[\"response\"].values])\n",
    "        radius = np.array([np.mean([sentence_transformer_embeddings_autbrick[resp] for resp in l], axis=0) for l in filtered_aggregated[\"response\"].values])\n",
    "\n",
    "        allvec = np.concatenate((vectors, single_points), axis = 0)\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(allvec)\n",
    "        allvec = pca.transform(allvec)\n",
    "        reduced_vectors = pca.transform(vectors)\n",
    "        reduced_vectors_dict = dict(zip(group[\"response\"].values, reduced_vectors))\n",
    "        reduced_single_points = pca.transform(single_points)\n",
    "\n",
    "        radius = np.array([np.max([np.sqrt(np.sum((reduced_vectors_dict[resp] - reduced_single_points[i]) ** 2)) for resp in l], axis=0) for i, l in enumerate(filtered_aggregated[\"response\"].values)])\n",
    "\n",
    "        # Normalize the reduced vectors and single points to fit within the range 0-boundary\n",
    "        # x = boundary * (reduced_vectors[:, 0] - allvec[:, 0].min()) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "        # y = boundary * (reduced_vectors[:, 1] - allvec[:, 1].min()) / (allvec[:, 1].max() - allvec[:, 1].min())\n",
    "        x = reduced_vectors[:, 0]\n",
    "        y = reduced_vectors[:, 1]\n",
    "        # single_x = boundary * (reduced_single_points[:, 0] - allvec[:, 0].min()) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "        # single_y = boundary * (reduced_single_points[:, 1] - allvec[:, 1].min()) / (allvec[:, 1].max() - allvec[:, 1].min())\n",
    "        single_x = reduced_single_points[:, 0]\n",
    "        single_y = reduced_single_points[:, 1]\n",
    "        # radius = boundary * (radius) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "\n",
    "        # Choose a colormap\n",
    "        cmap = get_cmap('viridis')\n",
    "        colors = np.linspace(0, 1, len(x) - 1)\n",
    "        for i in range(len(x) - 1):\n",
    "            ax[k//5, k%5].plot([x[i], x[i+1]], [y[i], y[i+1]], color=cmap(colors[i]), marker='o')\n",
    "\n",
    "        for i in range(len(single_x)):\n",
    "            ax[k//5, k%5].plot(single_x[i], single_y[i], 'ro')  # Red point\n",
    "            circle = Circle((single_x[i], single_y[i]), radius=radius[i], color='red', fill=False, linestyle='--')\n",
    "            ax[k//5, k%5].add_patch(circle)\n",
    "            \n",
    "        # ax[k//5, k%5].set_xlabel('Component 1')\n",
    "        # ax[k//5, k%5].set_ylabel('Component 2')\n",
    "        # ax[k//5, k%5].set_xlim(0, boundary)\n",
    "        # ax[k//5, k%5].set_ylim(0, boundary)\n",
    "        k += 1\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "k = 0\n",
    "for pid, group in data_humans[data_humans[\"task\"] == 3].groupby('pid'):\n",
    "    if len(group) < 20:\n",
    "        continue  \n",
    "    \n",
    "    try:\n",
    "\n",
    "        vectors = np.array([sentence_transformer_embeddings_autpaperclip[resp] for resp in group[\"response\"].values])\n",
    "        aggregated = group.groupby(\"category\").agg(list)\n",
    "        # Filter out categories where the number of responses is greater than 1\n",
    "        filtered_aggregated = aggregated[aggregated[\"response\"].apply(len) > 1]\n",
    "        single_points = np.array([np.mean([sentence_transformer_embeddings_autpaperclip[resp] for resp in l], axis=0) for l in filtered_aggregated[\"response\"].values])\n",
    "\n",
    "        allvec = np.concatenate((vectors, single_points), axis = 0)\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(allvec)\n",
    "        allvec = pca.transform(allvec)\n",
    "        reduced_vectors = pca.transform(vectors)\n",
    "        reduced_vectors_dict = dict(zip(group[\"response\"].values, reduced_vectors))\n",
    "        reduced_single_points = pca.transform(single_points)\n",
    "\n",
    "        radius = np.array([np.max([np.sqrt(np.sum((reduced_vectors_dict[resp] - reduced_single_points[i]) ** 2)) for resp in l], axis=0) for i, l in enumerate(filtered_aggregated[\"response\"].values)])\n",
    "\n",
    "        # Normalize the reduced vectors and single points to fit within the range 0-boundary\n",
    "        # x = boundary * (reduced_vectors[:, 0] - allvec[:, 0].min()) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "        # y = boundary * (reduced_vectors[:, 1] - allvec[:, 1].min()) / (allvec[:, 1].max() - allvec[:, 1].min())\n",
    "        x = reduced_vectors[:, 0]\n",
    "        y = reduced_vectors[:, 1]\n",
    "        # single_x = boundary * (reduced_single_points[:, 0] - allvec[:, 0].min()) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "        # single_y = boundary * (reduced_single_points[:, 1] - allvec[:, 1].min()) / (allvec[:, 1].max() - allvec[:, 1].min())\n",
    "        single_x = reduced_single_points[:, 0]\n",
    "        single_y = reduced_single_points[:, 1]\n",
    "        # radius = boundary * (radius) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "\n",
    "\n",
    "        # Choose a colormap\n",
    "        cmap = get_cmap('viridis')\n",
    "        colors = np.linspace(0, 1, len(x) - 1)\n",
    "        for i in range(len(x) - 1):\n",
    "            ax[k//5, k%5].plot([x[i], x[i+1]], [y[i], y[i+1]], color=cmap(colors[i]), marker='o')\n",
    "\n",
    "        for i in range(len(single_x)):\n",
    "            ax[k//5, k%5].plot(single_x[i], single_y[i], 'ro')  # Red point\n",
    "            circle = Circle((single_x[i], single_y[i]), radius=radius[i], color='red', fill=False, linestyle='--')\n",
    "            ax[k//5, k%5].add_patch(circle)\n",
    "\n",
    "        # ax[k//5, k%5].set_xlabel('Component 1')\n",
    "        # ax[k//5, k%5].set_ylabel('Component 2')\n",
    "        # ax[k//5, k%5].set_xlim(0, boundary)\n",
    "        # ax[k//5, k%5].set_ylim(0, boundary)\n",
    "        k += 1\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(18,18))\n",
    "k = 0\n",
    "for pid, group in data_humans[data_humans[\"task\"] == 1].groupby('pid'):\n",
    "    if len(group) < 20:\n",
    "        continue  \n",
    "    \n",
    "    try:\n",
    "\n",
    "        vectors = np.array([sentence_transformer_embeddings_vf[resp] for resp in group[\"response\"].values])\n",
    "        aggregated = group.groupby(\"category\").agg(list)\n",
    "        # Filter out categories where the number of responses is greater than 1\n",
    "        filtered_aggregated = aggregated[aggregated[\"response\"].apply(len) > 1]\n",
    "        single_points = np.array([np.mean([sentence_transformer_embeddings_vf[resp] for resp in l], axis=0) for l in filtered_aggregated[\"response\"].values])\n",
    "\n",
    "        allvec = np.concatenate((vectors, single_points), axis = 0)\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(allvec)\n",
    "        allvec = pca.transform(allvec)\n",
    "        reduced_vectors = pca.transform(vectors)\n",
    "        reduced_vectors_dict = dict(zip(group[\"response\"].values, reduced_vectors))\n",
    "        reduced_single_points = pca.transform(single_points)\n",
    "\n",
    "        radius = np.array([np.max([np.sqrt(np.sum((reduced_vectors_dict[resp] - reduced_single_points[i]) ** 2)) for resp in l], axis=0) for i, l in enumerate(filtered_aggregated[\"response\"].values)])\n",
    "\n",
    "        # Normalize the reduced vectors and single points to fit within the range 0-boundary\n",
    "        # x = boundary * (reduced_vectors[:, 0] - allvec[:, 0].min()) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "        # y = boundary * (reduced_vectors[:, 1] - allvec[:, 1].min()) / (allvec[:, 1].max() - allvec[:, 1].min())\n",
    "        x = reduced_vectors[:, 0]\n",
    "        y = reduced_vectors[:, 1]\n",
    "        # single_x = boundary * (reduced_single_points[:, 0] - allvec[:, 0].min()) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "        # single_y = boundary * (reduced_single_points[:, 1] - allvec[:, 1].min()) / (allvec[:, 1].max() - allvec[:, 1].min())\n",
    "        single_x = reduced_single_points[:, 0]\n",
    "        single_y = reduced_single_points[:, 1]\n",
    "        # radius = boundary * (radius) / (allvec[:, 0].max() - allvec[:, 0].min())\n",
    "\n",
    "        # Choose a colormap\n",
    "        cmap = get_cmap('viridis')\n",
    "        colors = np.linspace(0, 1, len(x) - 1)\n",
    "        for i in range(len(x) - 1):\n",
    "            ax[k//5, k%5].plot([x[i], x[i+1]], [y[i], y[i+1]], color=cmap(colors[i]), marker='o')\n",
    "\n",
    "        for i in range(len(single_x)):\n",
    "            ax[k//5, k%5].plot(single_x[i], single_y[i], 'ro')  # Red point\n",
    "            circle = Circle((single_x[i], single_y[i]), radius=radius[i], color='red', fill=False, linestyle='--')\n",
    "            ax[k//5, k%5].add_patch(circle)\n",
    "\n",
    "        # ax[k//5, k%5].set_xlabel('Component 1')\n",
    "        # ax[k//5, k%5].set_ylabel('Component 2')\n",
    "        # ax[k//5, k%5].set_xlim(0, boundary)\n",
    "        # ax[k//5, k%5].set_ylim(0, boundary)\n",
    "        k += 1\n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure in Going Back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
